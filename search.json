[{"path":"https://jpcompartir.github.io/LimpiaR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021 Jack Penzer Permission hereby granted, free charge, person obtaining copy software associated documentation files (â€œSoftwareâ€), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED â€œâ€, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar-overview","dir":"Articles","previous_headings":"","what":"LimpiaR Overview","title":"Introduction to LimpiaR","text":"LimpiaR package built expedite pre-processing cleaning text data, handy functions Spanish R. get started, â€™ll load helpful libraries.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"walkthrough","dir":"Articles","previous_headings":"","what":"Walkthrough","title":"Introduction to LimpiaR","text":"LimpiaRâ€™s functions begin limpiar_. library loaded, typing limpiar_ Rstudio script Rmarkdown code block, produce drop menu LimpiaR functions, help find name function â€™re looking - can use tab autocomplete function. inside function, RStudio give popover shows argument function expects. can also type â€˜control + spaceâ€™ cursor inside functionâ€™s brackets force extra help.","code":"library(magrittr) library(dplyr) library(stringr) library(LimpiaR) data #> # A tibble: 10 Ã— 2 #>    Mention.Content                                                   Mention.Url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa! cÃ³Ã³mo    estÃ¡s @magdalena   ?!\"                       www.twitteâ€¦ #>  2 \"  han visto este articulo!? Que horror! https://guardian.com/emâ€¦ www.twitteâ€¦ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.faceboâ€¦ #>  4 \"jajajajaja eres un wn!\"                                          www.faceboâ€¦ #>  5 \"RT dale un click a ver una mujer baila con su perro\"             www.twitteâ€¦ #>  6 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  7 \"yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£ \"                    www.instagâ€¦ #>  8 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  9 \"grax ntonces q?\"                                                 www.youtubâ€¦ #> 10 \"grax ntonces q?\"                                                 www.youtubâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"column-names","dir":"Articles","previous_headings":"Walkthrough","what":"Column Names","title":"Introduction to LimpiaR","text":"created data frame posts URLs. loading libraries data, first part workflow clean column names, makes using tab completion, accessing column names much faster (long run = big productivity gains). , â€™ll use janitor package. can uncomment code install janitor already installed machine.","code":"# ifelse(!\"janitor\" %in% installed.packages(), #    install.packages(\"janitor\"), library(janitor))  (data <- data %>%     janitor::clean_names()) #> # A tibble: 10 Ã— 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa! cÃ³Ã³mo    estÃ¡s @magdalena   ?!\"                       www.twitteâ€¦ #>  2 \"  han visto este articulo!? Que horror! https://guardian.com/emâ€¦ www.twitteâ€¦ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.faceboâ€¦ #>  4 \"jajajajaja eres un wn!\"                                          www.faceboâ€¦ #>  5 \"RT dale un click a ver una mujer baila con su perro\"             www.twitteâ€¦ #>  6 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  7 \"yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£ \"                    www.instagâ€¦ #>  8 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  9 \"grax ntonces q?\"                                                 www.youtubâ€¦ #> 10 \"grax ntonces q?\"                                                 www.youtubâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"lower-case-text-variable","dir":"Articles","previous_headings":"Walkthrough","what":"Lower Case Text Variable","title":"Introduction to LimpiaR","text":"workflows, next step make text variable lower case. make tokens like â€˜AMAZINGâ€™ â€˜Amazingâ€™ -> â€˜amazingâ€™. need LimpiaR function , base R function tolower() works just fine.","code":"(data <- data %>%   mutate(mention_content = tolower(mention_content))) #> # A tibble: 10 Ã— 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa! cÃ³Ã³mo    estÃ¡s @magdalena   ?!\"                       www.twitteâ€¦ #>  2 \"  han visto este articulo!? que horror! https://guardian.com/emâ€¦ www.twitteâ€¦ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.faceboâ€¦ #>  4 \"jajajajaja eres un wn!\"                                          www.faceboâ€¦ #>  5 \"rt dale un click a ver una mujer baila con su perro\"             www.twitteâ€¦ #>  6 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  7 \"yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£ \"                    www.instagâ€¦ #>  8 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  9 \"grax ntonces q?\"                                                 www.youtubâ€¦ #> 10 \"grax ntonces q?\"                                                 www.youtubâ€¦"},{"path":[]},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_accents","dir":"Articles","previous_headings":"Walkthrough","what":"limpiar_accents","title":"Introduction to LimpiaR","text":"Now â€™re going look LimpiaRâ€™s functions individually. first function limpiar_accents, replace accents common Spanish words text variable, Latin-alphabet equivalents e.g.Â â€˜Ã© -> eâ€™ . use assignment operator make sure changes saved. Tip: can type ?limpiar_accents access documentation, see arguments need fill .","code":"(data <- data %>%   limpiar_accents(text_var = mention_content)) #> # A tibble: 10 Ã— 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa! coomo    estas @magdalena   ?!\"                       www.twitteâ€¦ #>  2 \"  han visto este articulo!? que horror! https://guardian.com/emâ€¦ www.twitteâ€¦ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.faceboâ€¦ #>  4 \"jajajajaja eres un wn!\"                                          www.faceboâ€¦ #>  5 \"rt dale un click a ver una mujer baila con su perro\"             www.twitteâ€¦ #>  6 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  7 \"yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£ \"                    www.instagâ€¦ #>  8 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  9 \"grax ntonces q?\"                                                 www.youtubâ€¦ #> 10 \"grax ntonces q?\"                                                 www.youtubâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_duplicates","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents","what":"limpiar_duplicates","title":"Introduction to LimpiaR","text":"Now â€™ll remove duplicate posts, notice donâ€™t actually need type text_var = mention_content, default argument text_var already mention_content. Note: text column data frame called â€˜textâ€™ specify text_var = text, call:","code":"(data <- data %>%   limpiar_duplicates()) #> # A tibble: 7 Ã— 2 #>   mention_content                                                    mention_url #>   <chr>                                                              <chr>       #> 1 \"holaaaaaa! coomo    estas @magdalena   ?!\"                        www.twitteâ€¦ #> 2 \"  han visto este articulo!? que horror! https://guardian.com/emoâ€¦ www.twitteâ€¦ #> 3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\"  www.faceboâ€¦ #> 4 \"jajajajaja eres un wn!\"                                           www.faceboâ€¦ #> 5 \"rt dale un click a ver una mujer baila con su perro\"              www.twitteâ€¦ #> 6 \"grax ntonces q?\"                                                  www.youtubâ€¦ #> 7 \"yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£ \"                     www.instagâ€¦ data %>% rename(text = mention_content) #> # A tibble: 7 Ã— 2 #>   text                                                               mention_url #>   <chr>                                                              <chr>       #> 1 \"holaaaaaa! coomo    estas @magdalena   ?!\"                        www.twitteâ€¦ #> 2 \"  han visto este articulo!? que horror! https://guardian.com/emoâ€¦ www.twitteâ€¦ #> 3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\"  www.faceboâ€¦ #> 4 \"jajajajaja eres un wn!\"                                           www.faceboâ€¦ #> 5 \"rt dale un click a ver una mujer baila con su perro\"              www.twitteâ€¦ #> 6 \"grax ntonces q?\"                                                  www.youtubâ€¦ #> 7 \"yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£ \"                     www.instagâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_retweets","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents","what":"limpiar_retweets","title":"Introduction to LimpiaR","text":"need remove retweets, example create bigram network, limpiar function just .","code":"(data <- data %>%     limpiar_retweets()) #> # A tibble: 6 Ã— 2 #>   mention_content                                                    mention_url #>   <chr>                                                              <chr>       #> 1 \"holaaaaaa! coomo    estas @magdalena   ?!\"                        www.twitteâ€¦ #> 2 \"  han visto este articulo!? que horror! https://guardian.com/emoâ€¦ www.twitteâ€¦ #> 3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\"  www.faceboâ€¦ #> 4 \"jajajajaja eres un wn!\"                                           www.faceboâ€¦ #> 5 \"grax ntonces q?\"                                                  www.youtubâ€¦ #> 6 \"yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£ \"                     www.instagâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_url","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents","what":"limpiar_url","title":"Introduction to LimpiaR","text":"generally donâ€™t want URLs appearing charts analyses, can remove limpiar_url function.","code":"(data <- data %>%    limpiar_url()) #> # A tibble: 6 Ã— 2 #>   mention_content                                                   mention_url  #>   <chr>                                                             <chr>        #> 1 \"holaaaaaa! coomo    estas @magdalena   ?!\"                       www.twitterâ€¦ #> 2 \"  han visto este articulo!? que horror!  no se puede!!\"          www.twitterâ€¦ #> 3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.facebooâ€¦ #> 4 \"jajajajaja eres un wn!\"                                          www.facebooâ€¦ #> 5 \"grax ntonces q?\"                                                 www.youtubeâ€¦ #> 6 \"yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£ \"                    www.instagrâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_spaces","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents","what":"limpiar_spaces","title":"Introduction to LimpiaR","text":"Next â€™ll look use LimpiaR remove annoying white spaces, like beginning sentence, punctuation, multiple white spaces reason; common messy data often encounter.","code":"(data <- data %>%   limpiar_spaces()) #> # A tibble: 6 Ã— 2 #>   mention_content                                               mention_url      #>   <chr>                                                         <chr>            #> 1 holaaaaaa! coomo estas @magdalena?!                           www.twitter.comâ€¦ #> 2 han visto este articulo!? que horror! no se puede!!           www.twitter.comâ€¦ #> 3 ayyyyyy a mi me gustaria ir a londres yaaa #llevame #porfavor www.facebook.coâ€¦ #> 4 jajajajaja eres un wn!                                        www.facebook.coâ€¦ #> 5 grax ntonces q?                                               www.youtube.comâ€¦ #> 6 yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£                   www.instagram.câ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_tags","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents","what":"limpiar_tags","title":"Introduction to LimpiaR","text":"can also remove user handles (e.g. @magdalena) hashtags limpiar_tags function. Remember, can type ?limpiar_tags access documentation. Replace hashtags: Replace user tags: Replace hashtags user handles:","code":"data %>%   limpiar_tags(user = FALSE, hashtag = TRUE) #> # A tibble: 6 Ã— 2 #>   mention_content                                            mention_url         #>   <chr>                                                      <chr>               #> 1 holaaaaaa! coomo estas @magdalena?!                        www.twitter.com/poâ€¦ #> 2 han visto este articulo!? que horror! no se puede!!        www.twitter.com/poâ€¦ #> 3 ayyyyyy a mi me gustaria ir a londres yaaa hashtag hashtag www.facebook.com/pâ€¦ #> 4 jajajajaja eres un wn!                                     www.facebook.com/pâ€¦ #> 5 grax ntonces q?                                            www.youtube.com/poâ€¦ #> 6 yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£                www.instagram.com/â€¦ data %>%   limpiar_tags(user = TRUE, hashtag = FALSE) #> # A tibble: 6 Ã— 2 #>   mention_content                                               mention_url      #>   <chr>                                                         <chr>            #> 1 holaaaaaa! coomo estas @user?!                                www.twitter.comâ€¦ #> 2 han visto este articulo!? que horror! no se puede!!           www.twitter.comâ€¦ #> 3 ayyyyyy a mi me gustaria ir a londres yaaa #llevame #porfavor www.facebook.coâ€¦ #> 4 jajajajaja eres un wn!                                        www.facebook.coâ€¦ #> 5 grax ntonces q?                                               www.youtube.comâ€¦ #> 6 yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£                   www.instagram.câ€¦ data %>%   limpiar_tags() #> # A tibble: 6 Ã— 2 #>   mention_content                                            mention_url         #>   <chr>                                                      <chr>               #> 1 holaaaaaa! coomo estas @user?!                             www.twitter.com/poâ€¦ #> 2 han visto este articulo!? que horror! no se puede!!        www.twitter.com/poâ€¦ #> 3 ayyyyyy a mi me gustaria ir a londres yaaa hashtag hashtag www.facebook.com/pâ€¦ #> 4 jajajajaja eres un wn!                                     www.facebook.com/pâ€¦ #> 5 grax ntonces q?                                            www.youtube.com/poâ€¦ #> 6 yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£                www.instagram.com/â€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"quick-recap---weve-looked-at","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents","what":"Quick recap - weâ€™ve looked at:","title":"Introduction to LimpiaR","text":"cleaning column names janitor::clean_names() making text variable lower case mutate() & tolower() cleaning accents limpiar_accents() cleaning duplicate posts limpiar_duplicates() cleaning retweets limpiar_retweets() cleaning urls limpiar_url() cleaning spaces limpiar_spaces() cleaning user handles hashtags limpiar_tags()","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_shorthands","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents","what":"limpiar_shorthands","title":"Introduction to LimpiaR","text":"One biggest problems messy data encounter, shorthands. Generally, algorithms trained clean, standard language, encounter shorthands abbreviations. Shorthands also change time, making impractical continuously train algorithms new shorthands arise. function attempts bridge gap, normalising common shorthands.","code":"(data <- data %>%    limpiar_shorthands()) #> # A tibble: 6 Ã— 2 #>   mention_content                                               mention_url      #>   <chr>                                                         <chr>            #> 1 holaaaaaa! coomo estas @magdalena?!                           www.twitter.comâ€¦ #> 2 han visto este articulo!? que horror! no se puede!!           www.twitter.comâ€¦ #> 3 ayyyyyy a mi me gustaria ir a londres yaaa #llevame #porfavor www.facebook.coâ€¦ #> 4 jajajajaja eres un wuevon!                                    www.facebook.coâ€¦ #> 5 gracias entonces que?                                         www.youtube.comâ€¦ #> 6 yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£                   www.instagram.câ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_repeated_chars","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents","what":"limpiar_repeated_chars","title":"Introduction to LimpiaR","text":"donâ€™t want algorithm learn difference â€˜ajajajâ€™ â€˜jajaâ€™ â€˜ayâ€™ â€˜ayyyyâ€™ practically speaking, none. also donâ€™t want introduce unnecessary tokens, normalise common occurrences repeated additional characters. Generally, steps â€™ve taken far used every analysis/project help clean data. now look circumstantial functions, .e.Â used every analysis.","code":"(data <- data %>%    limpiar_repeat_chars()) #> # A tibble: 6 Ã— 2 #>   mention_content                                        mention_url             #>   <chr>                                                  <chr>                   #> 1 hola! coomo estas @magdalena?!                         www.twitter.com/post1   #> 2 han visto este articulo!? que horror! no se puede!!    www.twitter.com/post2   #> 3 ay a mi me gustaria ir a londres ya #llevame #porfavor www.facebook.com/post1  #> 4 jaja eres un wuevon!                                   www.facebook.com/post2  #> 5 gracias entonces que?                                  www.youtube.com/post1   #> 6 yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£            www.instagram.com/post1"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"emojis","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents","what":"Emojis","title":"Introduction to LimpiaR","text":"Emojis type non-ASCII unicode character, means removing non-ASCII characters remove emojis force. also means functions designed target certain unicode characters patterns, may inadvertently remove special characters - well emojis, instead emojis!","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_recode_emojis","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents > Emojis","what":"limpiar_recode_emojis()","title":"Introduction to LimpiaR","text":"donâ€™t need use limpiar_recode_emojis every analysis, many ParseR & SegmentR functions ignore implicitly. However, know need replace text descriptios, can can use limpiar__recode_emojis(). One problem , reason special cases , emojiâ€™s encodings English. may, point, translate Spanish, seems unlikely. set with_emoji_tag TRUE, emojis now pasted together â€™_â€™ â€™_emojiâ€™ label. Warning limpiar_recode_emojis() function quite slow scales poorly size inputs. using large dataset many long documents expect functions take run.","code":"data %>%   limpiar_recode_emojis(text_var = mention_content, with_emoji_tag = FALSE) #> # A tibble: 6 Ã— 2 #>   mention_content                                                    mention_url #>   <chr>                                                              <chr>       #> 1 hola! coomo estas @magdalena?!                                     www.twitteâ€¦ #> 2 han visto este articulo!? que horror! no se puede!!                www.twitteâ€¦ #> 3 ay a mi me gustaria ir a londres ya #llevame #porfavor             www.faceboâ€¦ #> 4 jaja eres un wuevon!                                               www.faceboâ€¦ #> 5 gracias entonces que?                                              www.youtubâ€¦ #> 6 yo soy el mejor face with tears of joy face with tears of joy facâ€¦ www.instagâ€¦ data %>%   limpiar_recode_emojis(mention_content, with_emoji_tag = TRUE) #> # A tibble: 6 Ã— 2 #>   mention_content                                                    mention_url #>   <chr>                                                              <chr>       #> 1 hola! coomo estas @magdalena?!                                     www.twitteâ€¦ #> 2 han visto este articulo!? que horror! no se puede!!                www.twitteâ€¦ #> 3 ay a mi me gustaria ir a londres ya #llevame #porfavor             www.faceboâ€¦ #> 4 jaja eres un wuevon!                                               www.faceboâ€¦ #> 5 gracias entonces que?                                              www.youtubâ€¦ #> 6 yo soy el mejor face_with_tears_of_joy_emoji face_with_tears_of_jâ€¦ www.instagâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_remove_emojis","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents > Emojis","what":"limpiar_remove_emojis()","title":"Introduction to LimpiaR","text":"situations donâ€™t want replace emojis text inputs, donâ€™t mind risking loss non-ASCII characters, want something runs fast? Instead limpiar_recode_emojis can use limpiar_remove_emojis! function operates fairly simple RegEx pattern, meaning runs lot efficiently recode counterpart.","code":"data %>%   limpiar_remove_emojis(mention_content) #> # A tibble: 6 Ã— 2 #>   mention_content                                          mention_url           #>   <chr>                                                    <chr>                 #> 1 \"hola! coomo estas @magdalena?!\"                         www.twitter.com/post1 #> 2 \"han visto este articulo!? que horror! no se puede!!\"    www.twitter.com/post2 #> 3 \"ay a mi me gustaria ir a londres ya #llevame #porfavor\" www.facebook.com/posâ€¦ #> 4 \"jaja eres un wuevon!\"                                   www.facebook.com/posâ€¦ #> 5 \"gracias entonces que?\"                                  www.youtube.com/post1 #> 6 \"yo soy el mejor , no eres nada!! \"                      www.instagram.com/poâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"non-ascii-characters","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents","what":"non-ASCII characters","title":"Introduction to LimpiaR","text":"ASCII (American Standard Code Information Interchange) character-encoding standard representing numbers text. 128 ASCII characters, including letters -z upper lowercase, numbers 0-9, common punctuation marks, additional characters specific uses computers. Everything else non-ASCII. purposes extended ASCII characters include Latin accents (Ã©, Ã­, etc.), letâ€™s get original data frame back demonstrate. remove things like emojis, keep punctuation accented characters:","code":"#> # A tibble: 10 Ã— 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa! cÃ³Ã³mo    estÃ¡s @magdalena   ?!\"                       www.twitteâ€¦ #>  2 \"  han visto este articulo!? Que horror! https://guardian.com/emâ€¦ www.twitteâ€¦ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.faceboâ€¦ #>  4 \"jajajajaja eres un wn!\"                                          www.faceboâ€¦ #>  5 \"RT dale un click a ver una mujer baila con su perro\"             www.twitteâ€¦ #>  6 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  7 \"yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£ \"                    www.instagâ€¦ #>  8 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  9 \"grax ntonces q?\"                                                 www.youtubâ€¦ #> 10 \"grax ntonces q?\"                                                 www.youtubâ€¦ data %>%   limpiar_non_ascii(mention_content) #> # A tibble: 10 Ã— 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa! cÃ³Ã³mo    estÃ¡s @magdalena   ?!\"                       www.twitteâ€¦ #>  2 \"  han visto este articulo!? Que horror! https://guardian.com/emâ€¦ www.twitteâ€¦ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.faceboâ€¦ #>  4 \"jajajajaja eres un wn!\"                                          www.faceboâ€¦ #>  5 \"RT dale un click a ver una mujer baila con su perro\"             www.twitteâ€¦ #>  6 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  7 \"yo soy el mejor , no eres nada!!  \"                              www.instagâ€¦ #>  8 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  9 \"grax ntonces q?\"                                                 www.youtubâ€¦ #> 10 \"grax ntonces q?\"                                                 www.youtubâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_alphanumeric","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents","what":"limpiar_alphanumeric","title":"Introduction to LimpiaR","text":"Similar non-ASCII characters, can retain alphanumeric characters (-zA-Z0-9 + spaces). heavy-duty option remove accented characters. want use limpiar_alphanumeric retain accented characters, recode accents first limpiar_accents: â€™ll need make informed choice limpiar_alphanumeric, limpiar_non_ascii functions like limpiar_accents limpiar_*_emojis","code":"data %>%   limpiar_alphanumeric(mention_content) #> # A tibble: 10 Ã— 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa cmo    ests magdalena   \"                              www.twitteâ€¦ #>  2 \"  han visto este articulo Que horror httpsguardiancomemojisbannâ€¦ www.twitteâ€¦ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa llevame porfavor\"   www.faceboâ€¦ #>  4 \"jajajajaja eres un wn\"                                           www.faceboâ€¦ #>  5 \"RT dale un click a ver una mujer baila con su perro\"             www.twitteâ€¦ #>  6 \"grax ntonces q\"                                                  www.youtubâ€¦ #>  7 \"yo soy el mejor  no eres nada  \"                                 www.instagâ€¦ #>  8 \"grax ntonces q\"                                                  www.youtubâ€¦ #>  9 \"grax ntonces q\"                                                  www.youtubâ€¦ #> 10 \"grax ntonces q\"                                                  www.youtubâ€¦ data %>%   limpiar_accents(mention_content) %>%   limpiar_alphanumeric(mention_content) #> # A tibble: 10 Ã— 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa coomo    estas magdalena   \"                           www.twitteâ€¦ #>  2 \"  han visto este articulo Que horror httpsguardiancomemojisbannâ€¦ www.twitteâ€¦ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa llevame porfavor\"   www.faceboâ€¦ #>  4 \"jajajajaja eres un wn\"                                           www.faceboâ€¦ #>  5 \"RT dale un click a ver una mujer baila con su perro\"             www.twitteâ€¦ #>  6 \"grax ntonces q\"                                                  www.youtubâ€¦ #>  7 \"yo soy el mejor  no eres nada  \"                                 www.instagâ€¦ #>  8 \"grax ntonces q\"                                                  www.youtubâ€¦ #>  9 \"grax ntonces q\"                                                  www.youtubâ€¦ #> 10 \"grax ntonces q\"                                                  www.youtubâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_stopwords","dir":"Articles","previous_headings":"Walkthrough > limpiar_accents","what":"limpiar_stopwords","title":"Introduction to LimpiaR","text":"Stop words common words provide us much information utteranceâ€™s meaning. example, sentence: â€˜man prison theftâ€™, knew one word sentence, word â€˜â€™, â€˜â€™, â€˜â€™, â€˜â€™ wouldnâ€™t much idea sentence . However, â€˜prisonâ€™ â€˜theftâ€™, give us lot information. many analyses, remove stop words help us see â€˜highest informationâ€™ words, get high-level understanding large bodies texts (topic modelling bigram networks.) virtually scenarios, want use limpiar_stopwords() argument stop_words = â€œtopicsâ€ like : However, sometimes want keep words usually treated stopwords specific purpose. example, â€™re analysing sentiment â€˜negativesâ€™ can invert sentiment text - â€˜gustaâ€™ vs â€˜gustaâ€™. remove instances â€˜â€™ data, worse job analysing sentiment. Spanish slightly shorter list stopwords sentiment topics, removed choice terms. Warning - sentences can look quite strange without stopwords, lot social posts virtually meaningless altogether! â€™s also worth pointing , lot information can lost removing stop words. Many phrases English Spanish different meanings stop word removed, stopwords lists contain negatives, can drastically change meaning sentence! use care.","code":"data %>%   limpiar_stopwords(stop_words = \"topics\") %>%   limpiar_spaces() #to clear the spaces of words that were removed #> # A tibble: 10 Ã— 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 holaaaaaa! cÃ³Ã³mo estÃ¡s @magdalena?!                               www.twitteâ€¦ #>  2 visto articulo!? Que horror! https://guardian.com/emojisbanned Nâ€¦ www.twitteâ€¦ #>  3 ayyyyyy gustaria londres yaaa #llevame #porfavor                  www.faceboâ€¦ #>  4 jajajajaja wn!                                                    www.faceboâ€¦ #>  5 RT dale click mujer baila perro                                   www.twitteâ€¦ #>  6 grax ntonces q?                                                   www.youtubâ€¦ #>  7 ðŸ˜‚ðŸ˜‚ðŸ˜‚,!! ðŸ¤£ðŸ¤£                                                    www.instagâ€¦ #>  8 grax ntonces q?                                                   www.youtubâ€¦ #>  9 grax ntonces q?                                                   www.youtubâ€¦ #> 10 grax ntonces q?                                                   www.youtubâ€¦ data %>%   limpiar_stopwords(stop_words = \"sentiment\") %>%   limpiar_spaces()  #> # A tibble: 10 Ã— 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 holaaaaaa! cÃ³Ã³mo estÃ¡s @magdalena?!                               www.twitteâ€¦ #>  2 visto articulo!? Que horror! https://guardian.com/emojisbanned Nâ€¦ www.twitteâ€¦ #>  3 ayyyyyy gustaria londres yaaa #llevame #porfavor                  www.faceboâ€¦ #>  4 jajajajaja wn!                                                    www.faceboâ€¦ #>  5 RT dale click mujer baila perro                                   www.twitteâ€¦ #>  6 grax ntonces q?                                                   www.youtubâ€¦ #>  7 mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no nada!! ðŸ¤£ðŸ¤£                                      www.instagâ€¦ #>  8 grax ntonces q?                                                   www.youtubâ€¦ #>  9 grax ntonces q?                                                   www.youtubâ€¦ #> 10 grax ntonces q?                                                   www.youtubâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"utility-functions","dir":"Articles","previous_headings":"Walkthrough","what":"Utility Functions","title":"Introduction to LimpiaR","text":"nearly end introduction LimpiaR, finish, letâ€™s look two utility functions may useful. â€™ve conjoured new data frame called df, use show last two functions chain everything together.","code":"df #> # A tibble: 10 Ã— 3 #>    mention_content                                            mention_url na_col #>    <chr>                                                      <chr>       <chr>  #>  1 \"holaaaaaa! cÃ³Ã³mo    estÃ¡s @magdalena   ?!\"                www.twitteâ€¦ NA     #>  2 \"  han visto este articulo!? Que horror! https://guardianâ€¦ www.twitteâ€¦ NA     #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #pâ€¦ www.faceboâ€¦ NA     #>  4 \"jajajajaja eres un wn!\"                                   www.faceboâ€¦ NA     #>  5 \"RT dale un click a ver una mujer baila con su perro\"      www.twitteâ€¦ NA     #>  6 \"grax ntonces q?\"                                          www.youtubâ€¦ NA     #>  7 \"yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£ \"             www.instagâ€¦ NA     #>  8 \"grax ntonces q?\"                                          www.youtubâ€¦ NA     #>  9 \"grax ntonces q?\"                                          www.youtubâ€¦ tadaa  #> 10 \"grax ntonces q?\"                                          www.youtubâ€¦ NA"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_inpsect","dir":"Articles","previous_headings":"Walkthrough > Utility Functions","what":"limpiar_inpsect","title":"Introduction to LimpiaR","text":", imagine see strange pattern, want check â€™s going specific pattern. can use limpiar_inspect view posts contain pattern interactive frame! Whilst â€™s pretty obvious â€˜grax ntonces q?â€™ posts exactly , real world â€™re going 10,000 times many posts, searching suspicious patterns may take lot time.","code":"limpiar_inspect(df,                  pattern = \"ntonces\",                  text_var = mention_content,                 url_var = mention_url,                 title = \"ntonces\")"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"limpiar_na_cols","dir":"Articles","previous_headings":"Walkthrough > Utility Functions","what":"limpiar_na_cols","title":"Introduction to LimpiaR","text":"final function useful want remove â€˜mostly NAâ€™ columns data frame. may want save memory, example 400,000 posts 80 columns. case â€™ll get rid columns 25% values NA.","code":"limpiar_na_cols(df,threshold =  0.25) #> # A tibble: 10 Ã— 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa! cÃ³Ã³mo    estÃ¡s @magdalena   ?!\"                       www.twitteâ€¦ #>  2 \"  han visto este articulo!? Que horror! https://guardian.com/emâ€¦ www.twitteâ€¦ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.faceboâ€¦ #>  4 \"jajajajaja eres un wn!\"                                          www.faceboâ€¦ #>  5 \"RT dale un click a ver una mujer baila con su perro\"             www.twitteâ€¦ #>  6 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  7 \"yo soy el mejor ðŸ˜‚ðŸ˜‚ðŸ˜‚, no eres nada!! ðŸ¤£ðŸ¤£ \"                    www.instagâ€¦ #>  8 \"grax ntonces q?\"                                                 www.youtubâ€¦ #>  9 \"grax ntonces q?\"                                                 www.youtubâ€¦ #> 10 \"grax ntonces q?\"                                                 www.youtubâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/limpiar_intro.html","id":"putting-it-all-together","dir":"Articles","previous_headings":"Walkthrough","what":"Putting It All Together","title":"Introduction to LimpiaR","text":"speed things , call functions together one big long pipe. However, generally want check effects transformations data, lot operations like without intermediate checks risky.","code":"df %>%   limpiar_na_cols(threshold = 0.25)%>%   limpiar_accents()%>%   limpiar_retweets()%>%   limpiar_shorthands()%>%   limpiar_repeat_chars()%>%   limpiar_url()%>%   limpiar_remove_emojis()%>%   limpiar_shorthands()%>%   limpiar_spaces()%>%   limpiar_duplicates() #> # A tibble: 6 Ã— 2 #>   mention_content                                        mention_url             #>   <chr>                                                  <chr>                   #> 1 hola! coomo estas @magdalena?!                         www.twitter.com/post1   #> 2 han visto este articulo!? Que horror! NO SE PUEDE!!    www.twitter.com/post2   #> 3 ay a mi me gustaria ir a londres ya #llevame #porfavor www.facebook.com/post1  #> 4 jaja eres un wuevon!                                   www.facebook.com/post2  #> 5 gracias entonces que?                                  www.youtube.com/post1   #> 6 yo soy el mejor, no eres nada!!                        www.instagram.com/post1"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/near_duplicates.html","id":"quick-start---code","dir":"Articles","previous_headings":"","what":"Quick Start - Code","title":"Near Duplicate Removal","text":"Quick example run code function - get dataset text variable : Now read docs!","code":"limpiar_examples %>%   limpiar_spam_grams(mention_content, n_gram = 3, min_freq = 2) #> $spam_grams #> # A tibble: 13 Ã— 2 #>    ngrams                    n #>    <chr>                 <int> #>  1 amigo sancho es           2 #>  2 de vdd jajaja             2 #>  3 en una muy                2 #>  4 es un wn                  2 #>  5 han metido en             2 #>  6 metido en una             2 #>  7 mi amigo sancho           2 #>  8 muy dificil situaciÃ³n     2 #>  9 nos han metido            2 #> 10 sancho es un              2 #> 11 un wn de                  2 #> 12 una muy dificil           2 #> 13 wn de vdd                 2 #>  #> $data #> # A tibble: 6 Ã— 6 #>   doc_id author_name  mention_content mention_url platform_interactions document #>    <int> <chr>        <chr>           <chr>       <lgl>                    <int> #> 1      3 edmond_dantâ€¦ \"@don_quijote â€¦ www.twitteâ€¦ NA                           3 #> 2      6 robert_jordâ€¦ \"   Lo q no teâ€¦ www.youtubâ€¦ NA                           6 #> 3      7 anselmo      \"a mi es muy gâ€¦ www.twitteâ€¦ NA                           7 #> 4      8 maria        \"ayyy nooo @roâ€¦ www.twitteâ€¦ NA                           8 #> 5      9 pablo        \"todos se unenâ€¦ www.instagâ€¦ NA                           9 #> 6     10 pilar        \"a mi me gustaâ€¦ www.instagâ€¦ NA                          10 #>  #> $deleted #> # A tibble: 4 Ã— 6 #>   doc_id author_name  mention_content mention_url platform_interactions document #>    <int> <chr>        <chr>           <chr>       <lgl>                    <int> #> 1      1 don_quijote  mi amigo sanchâ€¦ www.twitteâ€¦ NA                           1 #> 2      2 sancho_panza RT mi amigo saâ€¦ www.twitteâ€¦ NA                           2 #> 3      4 el_sordo     nos han metidoâ€¦ www.fakeboâ€¦ NA                           4 #> 4      5 commander_mâ€¦ nos han metidoâ€¦ www.fakeboâ€¦ NA                           5"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/near_duplicates.html","id":"the-problem","dir":"Articles","previous_headings":"","what":"The Problem","title":"Near Duplicate Removal","text":"Extracting useful information large, unstructured internet datasets difficult task even datasets clean. impossible task datasets riddled bot network content, spammers, duplicates, near-duplicates. Sadly datasets come ready-cleaned, often remove hundreds thousands almost identical documents. Without automated, semi-automated, assistance, process extremely time consuming quickly becomes intractable size dataset grows. make bad situation worse, one-size fits definition â€˜near duplicateâ€™. Letâ€™s look two pairs documents: Pair 1 Arsenal favourite team Liverpool favourite team Pair 2 @jiryan_2 wants make rich! Click amazing crypto opportunity www.definitelynotascam.com/get_rich_quick @jiryan_2 wants make wealthy! Click amazing crypto opportunity www.definitelynotascam.com/get_rich_quick quite clear one pairs documents problematic , yet documents differ single world. even principle, wouldnâ€™t want write code â€˜check another document differs one word, remove documents â€™ - need something bit nuanced.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/articles/near_duplicates.html","id":"a-solution---limpiar_spam_grams","dir":"Articles","previous_headings":"","what":"A Solution - limpiar_spam_grams()","title":"Near Duplicate Removal","text":"developed -house solution looks recurrent n-grams 1 within dataset, grams words - n = 1, bigrams n = 2, trigrams n = 3 . algorithm language specific, can applied across language clear delimiters words. algorithm works follows: count occurrence every n-gram across documents dataset. value n set user n_gram parameter. filter n-grams occur min_freq. filter documents n-gram list, retain documents n-gram. return list : â€˜spam_gramsâ€™ - n-grams occur min_freq, remaining data, deleted data, user inspect.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/articles/near_duplicates.html","id":"why-does-it-work","dir":"Articles","previous_headings":"A Solution - limpiar_spam_grams()","what":"Why does it work?","title":"Near Duplicate Removal","text":"limpiar_spam_grams() heuristic approach cleaning, simple effective. key insight understanding works think perplexity Information Theory/language modelling sense. Sequences natural language high perplexity - ideas want communicate, many words choose communicate idea. means two people describing idea unlikely use words. ideas communicated exact words, likely come source, .e.Â process generated independent. can recognise unsophisticated spammers bot networks. Sentence 1: unexpected shower forced beachgoers quickly gather belongings seek shelter nearby buildings. Sentence 2: sudden downpour prompted seaside visitors hastily collect items dash adjacent structures. idea communicated , words different.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/articles/near_duplicates.html","id":"example-code","dir":"Articles","previous_headings":"A Solution - limpiar_spam_grams()","what":"Example code","title":"Near Duplicate Removal","text":"Letâ€™s run code see function applied. â€™ll save output limpiar_spam_grams() variable - variable contain list $spam_grams, $data, $deleted. â€™ll use limpiar_examples dataset, small, toy dataset 10 rows (documents). can read every document dataset, quickly verify outputs; handy learning, reality working much larger datasets verification take time. run code low values n_gram min_freq dataset small, contain many near duplicates. ask function find substring within document 3 words seen entire dataset seen least 2 documents. can check â€™s list simply names(). confirm three elements list expect. Now can take look spam_grams see ngrams function suggesting delete: look ngrams picked filtering, â€™s clear ar e robust. imagine many documents fact want remove datset, containing least one ngrams. â€™s vital user inspects ngrams considers whether documents containing likely spam-like. move inspecting actual documents deleted, see ngrams $spam_grams coming two pairs documents - one tweet + retweet - authors Don Quijote & Sancho Panza. way retweets formulated, makes sense treated near duplicates. also pair exact duplicates EL Sordo Commander Miranda - expect duplicates flagged near duplicates. case function working intended. However, given noticed $spam_grams output, want use exact parameters larger dataset, many ngrams used non-spam-like documents. raises question, set starting parameters working limpiar_spam_grams() larger datasets?","code":"spam_grams_output <- limpiar_examples %>%   limpiar_spam_grams(mention_content, n_gram = 3, min_freq = 2) names(spam_grams_output) #> [1] \"spam_grams\" \"data\"       \"deleted\" spam_grams_output$spam_grams #> # A tibble: 13 Ã— 2 #>    ngrams                    n #>    <chr>                 <int> #>  1 amigo sancho es           2 #>  2 de vdd jajaja             2 #>  3 en una muy                2 #>  4 es un wn                  2 #>  5 han metido en             2 #>  6 metido en una             2 #>  7 mi amigo sancho           2 #>  8 muy dificil situaciÃ³n     2 #>  9 nos han metido            2 #> 10 sancho es un              2 #> 11 un wn de                  2 #> 12 una muy dificil           2 #> 13 wn de vdd                 2 spam_grams_output$deleted #> # A tibble: 4 Ã— 6 #>   doc_id author_name  mention_content mention_url platform_interactions document #>    <int> <chr>        <chr>           <chr>       <lgl>                    <int> #> 1      1 don_quijote  mi amigo sanchâ€¦ www.twitteâ€¦ NA                           1 #> 2      2 sancho_panza RT mi amigo saâ€¦ www.twitteâ€¦ NA                           2 #> 3      4 el_sordo     nos han metidoâ€¦ www.fakeboâ€¦ NA                           4 #> 4      5 commander_mâ€¦ nos han metidoâ€¦ www.fakeboâ€¦ NA                           5"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/near_duplicates.html","id":"parameter-settings","dir":"Articles","previous_headings":"A Solution - limpiar_spam_grams()","what":"Parameter Settings","title":"Near Duplicate Removal","text":"experience early research, suggest setting ngram = 7 starting point, min_freq scaled number documents dataset. Good starting points take log size dataset, square root. log square root? Ultimately decision empirical - generating data use function, .e.Â set min_freq = 10, â€™ll need grab $deleted data tibble functionâ€™s output, sample documents, counting recording many think truly spam-like removed. Keep find parameter ~90% documents spam-like. Record outputs experiments.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/articles/near_duplicates.html","id":"trade-off","dir":"Articles","previous_headings":"A Solution - limpiar_spam_grams() > Parameter Settings","what":"Trade-off","title":"Near Duplicate Removal","text":"Setting parameters results trade-precision & recall - set ngram = 1, min_freq =1, remove every single document dataset 1 word . result 100% recall spam documents - remove every document. However, precision low. results early research shows precision / recall trade-","code":"limpiar_examples %>%   limpiar_spam_grams(mention_content, 1, 1) #> $spam_grams #> # A tibble: 56 Ã— 2 #>    ngrams      n #>    <chr>   <int> #>  1 es          6 #>  2 mi          4 #>  3 muy         4 #>  4 un          4 #>  5 a           3 #>  6 amigo       3 #>  7 sancho      3 #>  8 de          2 #>  9 dificil     2 #> 10 en          2 #> # â„¹ 46 more rows #>  #> $data #> # A tibble: 0 Ã— 6 #> # â„¹ 6 variables: doc_id <int>, author_name <chr>, mention_content <chr>, #> #   mention_url <chr>, platform_interactions <lgl>, document <int> #>  #> $deleted #> # A tibble: 10 Ã— 6 #>    doc_id author_name mention_content mention_url platform_interactions document #>     <int> <chr>       <chr>           <chr>       <lgl>                    <int> #>  1      1 don_quijote \"mi amigo sancâ€¦ www.twitteâ€¦ NA                           1 #>  2      2 sancho_panâ€¦ \"RT mi amigo sâ€¦ www.twitteâ€¦ NA                           2 #>  3      3 edmond_danâ€¦ \"@don_quijote â€¦ www.twitteâ€¦ NA                           3 #>  4      4 el_sordo    \"nos han metidâ€¦ www.fakeboâ€¦ NA                           4 #>  5      5 commander_â€¦ \"nos han metidâ€¦ www.fakeboâ€¦ NA                           5 #>  6      6 robert_jorâ€¦ \"   Lo q no teâ€¦ www.youtubâ€¦ NA                           6 #>  7      7 anselmo     \"a mi es muy gâ€¦ www.twitteâ€¦ NA                           7 #>  8      8 maria       \"ayyy nooo @roâ€¦ www.twitteâ€¦ NA                           8 #>  9      9 pablo       \"todos se unenâ€¦ www.instagâ€¦ NA                           9 #> 10     10 pilar       \"a mi me gustaâ€¦ www.instagâ€¦ NA                          10"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/near_duplicates.html","id":"limitations","dir":"Articles","previous_headings":"A Solution - limpiar_spam_grams()","what":"Limitations","title":"Near Duplicate Removal","text":"Sometimes long substrings fact indicate documents want remove, example, long substrings naturally occur many documents - example researching web browsers â€œset Chrome default browserâ€ ngram =7 may occur many times without indicating spam-like documents. Likewise quotes, probably seen quote many times: â€˜Life like riding bicycle. keep balance, must keep moving.â€™ â€™s often used blogs, articles, explainers etc. hook supporting evidence. much longer usually set value ngram = parameter, meaning seen across documents min_freq times, documents deleted. Although case â€™s actually quite unlikely problem, general principle limitation approach. Another limitation function scales somewhat poorly size documents - many long documents large corpus text, need lot time memory use limpiar_spam_grams currently works. often need iterate parameters, can barrier usage.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/articles/near_duplicates.html","id":"another-solution","dir":"Articles","previous_headings":"","what":"Another Solution","title":"Near Duplicate Removal","text":"Coming soon: Approximate Nearest Neighbours, Locality Sensitive Hashing, Jaccard Similarity","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/articles/parts_of_speech_workflow.html","id":"workflow","dir":"Articles","previous_headings":"","what":"Workflow","title":"LimpiaR - Parts of Speech Workflow","text":"main motivation processing extracting parts speech home particular aspects language informative answering given research question. example, want know people think something, might focus adjectives nouns adjectives describe things, nouns things. want know people using something, might focus verbs/phrasal verbs â€˜wordsâ€™.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/articles/parts_of_speech_workflow.html","id":"loading-packages","dir":"Articles","previous_headings":"Workflow","what":"Loading packages","title":"LimpiaR - Parts of Speech Workflow","text":"","code":"library(LimpiaR) library(tibble) library(dplyr) library(stringr)"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/parts_of_speech_workflow.html","id":"getting-started","dir":"Articles","previous_headings":"Workflow","what":"Getting started","title":"LimpiaR - Parts of Speech Workflow","text":"LimpiaRâ€™s PoS workflow leans pre-existing functionality {udpipe} package allow users import pre-trained model parts speech analysis whilst enabling advanced users perform â€˜dependency parsingâ€™. â€œtechnique provides word sentence link another word sentence, called syntactical head. link 2 words furthermore certain type relationship giving details â€ see info.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/articles/parts_of_speech_workflow.html","id":"importing-a-udpipe-model","dir":"Articles","previous_headings":"Workflow","what":"Importing a udpipe model","title":"LimpiaR - Parts of Speech Workflow","text":"First, import model want use. UDPipe pre-trained models built upon Universal Dependencies treebanks made available 65 languages based 101 treebanks. demonstrative purposes â€™ll use language = \"english\".","code":"model <- limpiar_pos_import_model(language = \"english\")"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/parts_of_speech_workflow.html","id":"example-data","dir":"Articles","previous_headings":"Workflow","what":"Example data","title":"LimpiaR - Parts of Speech Workflow","text":"Now model imported session, letâ€™s get data tag, tokenise, lemmatise, extract parts speech. need data frame text variable ID column uniquely identifies text. now data also desired model loaded session, function need next limpiar_pos_annotate(). begin, â€™s important note output sensitive cleaning steps. example, possible removal punctuation, PoS Tagging may -perform. also said POS tagging process nouns(NOUN) proper-nouns(PROPN) harder differentiate punctuation removed well text lowercase. careful pre-processing text variable â€™re intending follow PoS workflow.","code":"( data <-tibble(text = tolower(stringr::sentences[1:100]),                       universal_message_id = paste0(\"TWITTER\", 1:100)) ) ## # A tibble: 100 Ã— 2 ##    text                                        universal_message_id ##    <chr>                                       <chr>                ##  1 the birch canoe slid on the smooth planks.  TWITTER1             ##  2 glue the sheet to the dark blue background. TWITTER2             ##  3 it's easy to tell the depth of a well.      TWITTER3             ##  4 these days a chicken leg is a rare dish.    TWITTER4             ##  5 rice is often served in round bowls.        TWITTER5             ##  6 the juice of lemons makes fine punch.       TWITTER6             ##  7 the box was thrown beside the parked truck. TWITTER7             ##  8 the hogs were fed chopped corn and garbage. TWITTER8             ##  9 four hours of steady work faced us.         TWITTER9             ## 10 a large size in stockings is hard to sell.  TWITTER10            ## # â„¹ 90 more rows"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/parts_of_speech_workflow.html","id":"extracting-parts-of-speech","dir":"Articles","previous_headings":"Workflow","what":"Extracting parts of speech","title":"LimpiaR - Parts of Speech Workflow","text":"Thelimpiar_pos_annotate function converts text document tokens, lemmas, POS tags, dependency relationships*. input data frame one document per row, return data frame one row per token. means return data frame many rows input data frame! limpiar_pos_annotate can sped using in_parallel = TRUE, argument makes function process multiple rows data frame parallel. Running function parallel prevents progress updated. default function print progress every 100 rows, way know â€™s still working roughly long take finish running. donâ€™t want progress updates print console set update_progress = 0. Now texts tokenized, dependencies parsed, POS annotations complete, letâ€™s take look output. annotating 882 rows, 8.82x increase! Output additional columns, pay closest attention paragraph_id, sentence_id, token, lemma, pos_tag, dependency_tag. new columns? paragraph_id sentence_id tell us documentâ€™s paragraphs, sentence token . data set contains single sentences, values 1. token word rest columns refer . lemma lemmatised version token, example lemmatisation â€˜servedâ€™ converted â€˜serveâ€™. pos_tag displays PoS labels ; Noun(NOUN), Proper Noun(PROPN), Pronoun(PRON), Verb(VERB), Adjective(ADJ) etc. exact tags mean see PoS Tags UPOS tags table top document. dependency_tag displays dependency relation token. Dependency relations significantly complicated parts speech. see Dependency Relations details. head_token_id, dependency_tag feats present select dependency_parse = TRUE, tell us id token (inside document, paragraph, sentence) token row dependency relation , dependency relation additional features relationship Finally, column inserted id_var =. variable help join results back original data frame.","code":"# annotate texts and perform dependency parsing annotations <- limpiar_pos_annotate(data = data, text_var = text, id_var = universal_message_id, pos_model = model, dependency_parse = TRUE, in_parallel = FALSE, update_progress = 25) ## 2025-10-02 10:16:22.195173 Annotating text fragment 1/100 ## 2025-10-02 10:16:22.340397 Annotating text fragment 26/100 ## 2025-10-02 10:16:22.461573 Annotating text fragment 51/100 ## 2025-10-02 10:16:22.599985 Annotating text fragment 76/100 annotations %>%   select(-c(sentence, feats, xpos, doc_id)) %>%   relocate(universal_message_id) ## # A tibble: 882 Ã— 9 ##    universal_message_id paragraph_id sentence_id token_id token  lemma  pos_tag ##    <chr>                       <int>       <int> <chr>    <chr>  <chr>  <chr>   ##  1 TWITTER1                        1           1 1        the    the    DET     ##  2 TWITTER1                        1           1 2        birch  birch  NOUN    ##  3 TWITTER1                        1           1 3        canoe  canoe  NOUN    ##  4 TWITTER1                        1           1 4        slid   slid   NOUN    ##  5 TWITTER1                        1           1 5        on     on     ADP     ##  6 TWITTER1                        1           1 6        the    the    DET     ##  7 TWITTER1                        1           1 7        smooth smooth ADJ     ##  8 TWITTER1                        1           1 8        planks plank  NOUN    ##  9 TWITTER1                        1           1 9        .      .      PUNCT   ## 10 TWITTER2                        1           1 1        glue   glue   VERB    ## # â„¹ 872 more rows ## # â„¹ 2 more variables: head_token_id <chr>, dependency_tag <chr>"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/parts_of_speech_workflow.html","id":"manipulating-the-output","dir":"Articles","previous_headings":"Workflow","what":"Manipulating the output","title":"LimpiaR - Parts of Speech Workflow","text":"Now adequate context, letâ€™s look can output. can combine commonly-used {dplyr} functions data wrangling summarisation filter certain parts speech count occurrences. Given size data set limited inferences can make counts, just want know adjectives seen frequently: Alternatively thing nouns, time count lemma (case plurals).","code":"# Count adjectives annotations %>%    filter(pos_tag %in% c(\"ADJ\")) %>%   count(token, pos_tag, sort = TRUE) ## # A tibble: 65 Ã— 3 ##    token pos_tag     n ##    <chr> <chr>   <int> ##  1 blue  ADJ         3 ##  2 clear ADJ         3 ##  3 third ADJ         3 ##  4 large ADJ         2 ##  5 more  ADJ         2 ##  6 sharp ADJ         2 ##  7 small ADJ         2 ##  8 wide  ADJ         2 ##  9 young ADJ         2 ## 10 bent  ADJ         1 ## # â„¹ 55 more rows #Count the lemma of each noun annotations %>%    filter(pos_tag %in% c(\"NOUN\")) %>% # this time selecting nouns   count(lemma, sort = TRUE) ## # A tibble: 214 Ã— 2 ##    lemma     n ##    <chr> <int> ##  1 fence     3 ##  2 week      3 ##  3 air       2 ##  4 boy       2 ##  5 cat       2 ##  6 coat      2 ##  7 day       2 ##  8 fish      2 ##  9 girl      2 ## 10 grass     2 ## # â„¹ 204 more rows"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/parts_of_speech_workflow.html","id":"adjective---noun-collocations","dir":"Articles","previous_headings":"Workflow > Manipulating the output","what":"Adjective - Noun collocations","title":"LimpiaR - Parts of Speech Workflow","text":"adjective -> noun pairs seen ? task gets slightly involved need look value token value next row, first need dplyr::filter dplyr::lag + dplyr::lead, group doc_id, paragraph_id sentence_id. â€™ll need perform {dplyr} magic count ADJ-NOUN collocations. data now set row 1+2, 3+4, 4+5 forth ADJ-NOUN collocations, can use information extract pairs combination floor division, exploiting cumsum meshes logicals R, â€˜grouped, summarise paste flourish collapseâ€™ trick. ADJ-NOUN collocation seen â€˜third weekâ€™. larger data set expect find higher-frequency ADJ-NOUN collocations. However, manipulating data way, onus user know understand transformations created.","code":"adj_to_noun <- annotations %>%    filter(     pos_tag == \"ADJ\" & lead(pos_tag) == \"NOUN\" | pos_tag == \"NOUN\" & lag(pos_tag) == \"ADJ\", .by = c(doc_id, paragraph_id, sentence_id)) adj_to_noun %>%   mutate(collocation_id = row_number() %% 2, .before = 1) %>%    mutate(collocation_id = cumsum(collocation_id == 1)) %>%   summarise(collocation = paste0(token, collapse = \" \"), .by = collocation_id) %>%   count(collocation, sort = TRUE) ## # A tibble: 56 Ã— 2 ##    collocation         n ##    <chr>           <int> ##  1 third week          2 ##  2 bent hook           1 ##  3 big task            1 ##  4 blue air            1 ##  5 blue background     1 ##  6 blue fish           1 ##  7 clear response      1 ##  8 clear spring        1 ##  9 clear water         1 ## 10 cloud hung          1 ## # â„¹ 46 more rows"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/parts_of_speech_workflow.html","id":"repairing-the-sentence","dir":"Articles","previous_headings":"Workflow > Manipulating the output","what":"Repairing the sentence","title":"LimpiaR - Parts of Speech Workflow","text":"next example â€™re going convert pos_tag == ADJ pos_tag == â€œNOUNâ€ lemma, stitch sentences back together â€™re ready visualisations. â€™ll make use case_when now-familiar grouped summarise paste trick. group universal_message_id â€™ll get back 100 row data frame can join back original data frame. information regarding co-occurrences nouns adjectives within documents, able evidence entities appear throughout data, well type language used conjunction toward . can later add , visualizing frequency terms appearing within document well explicitly used one another, just like example , see â€˜hot sunâ€™ appear frequent.","code":"annotations <- annotations %>%   mutate(token =            case_when(pos_tag == \"ADJ\" ~ lemma,                      pos_tag == \"NOUN\" ~ lemma,                      TRUE ~ lemma)) %>%   select(token, universal_message_id) %>%   summarise(sentence = paste0(token, collapse = \" \"),              .by = universal_message_id)"},{"path":"https://jpcompartir.github.io/LimpiaR/articles/parts_of_speech_workflow.html","id":"other-languages---spanish","dir":"Articles","previous_headings":"","what":"Other languages - Spanish","title":"LimpiaR - Parts of Speech Workflow","text":"workflow Spanish similar English. â€™ll select language = â€œspanishâ€ â€™ll take look output Spanish document. â€™ll take paragraph El Pais store data frame named â€˜spanishâ€™ â€œHace muchos aÃ±os, cuando llegÃ³ La Habana, encontrÃ³ el mejor acto de amor â€”el estudio de la escritura, que es aÃºn mejor que el estudio de la vidaâ€” por parte de la joven Yalenis Velazco, quien dedicÃ³ un intenso ensayo su obra.â€ see immediately â€˜haceâ€™ conjugation verb â€˜hacerâ€™ converted hacer, muchos -> mucho, aÃ±os -> aÃ±o. changes generally agreeable, â€™ll help us reduce overall number words data set, text-based visualisations cleaner. However, always good idea carefully consider effects transformation perform data, rather blindly. can convert verbs lemma stitch document back together using tricks English: Clearly sentence now ungrammatical, topic modelling workflow n-gram networks, data cleaner counts better reflect real frequencies verb. models 100% correct, expect see occasional false output. particularly true comes languages English tend less training data, grammatically complex.","code":"spanish_model <- limpiar_pos_import_model(\"spanish\") # Load the model   spanish <- spanish %>%   limpiar_pos_annotate(text, id, spanish_model, udpate_progress = 0) # Extract Pos Tags ## 2025-10-02 10:16:25.14254 Annotating text fragment 1/1 spanish %>%   select(token, lemma, pos_tag) # Select relevant columns ## # A tibble: 49 Ã— 3 ##    token  lemma  pos_tag ##    <chr>  <chr>  <chr>   ##  1 Hace   hacer  VERB    ##  2 muchos mucho  DET     ##  3 aÃ±os   aÃ±o    NOUN    ##  4 ,      ,      PUNCT   ##  5 cuando cuando SCONJ   ##  6 llegÃ³  llegar VERB    ##  7 a      a      ADP     ##  8 La     el     DET     ##  9 Habana habana PROPN   ## 10 ,      ,      PUNCT   ## # â„¹ 39 more rows spanish %>%   mutate(     token = ifelse(pos_tag == \"VERB\", lemma, token)   ) %>%   summarise(.by = id,             text = paste0(token, collapse = \" \")) ## # A tibble: 1 Ã— 2 ##      id text                                                                     ##   <dbl> <chr>                                                                    ## 1     1 hacer muchos aÃ±os , cuando llegar a La Habana , encontrar el mejor actoâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jack Penzer. Maintainer. Tim Mooney. Author. SHARE Creative. Copyright holder.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mooney T (2025). LimpiaR: LimpiaR. R package version 1.1.0, https://jpcompartir.github.io/LimpiaR.","code":"@Manual{,   title = {LimpiaR: LimpiaR},   author = {Tim Mooney},   year = {2025},   note = {R package version 1.1.0},   url = {https://jpcompartir.github.io/LimpiaR}, }"},{"path":"https://jpcompartir.github.io/LimpiaR/index.html","id":"what-is-limpiar","dir":"","previous_headings":"","what":"What is LimpiaR?","title":"LimpiaR","text":"LimpiaR R library functions cleaning & pre-processing text data. name comes â€˜limpiarâ€™ Spanish verbâ€™cleanâ€™. Generally calling LimpiaR function, can think â€˜cleanâ€¦â€™. LimpiaR primarily used cleaning unstructured text data, comes social media reviews. initial release, focused around Spanish language, however, functions language-ambivalent.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"LimpiaR","text":"can install development version LimpiaR GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"jpcompartir/LimpiaR\")"},{"path":"https://jpcompartir.github.io/LimpiaR/index.html","id":"limpiar-functions-overview","dir":"","previous_headings":"","what":"LimpiaR","title":"LimpiaR","text":"LimpiaR provides comprehensive suite text cleaning processing functions, primarily focused preparing text data machine learning analytics tasks. â€™ll find functions organised primary purpose. Functions editing text variable place.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/index.html","id":"removing-posts","dir":"","previous_headings":"","what":"Removing Posts","title":"LimpiaR","text":"Functions removing unwanted posts entirely (rather cleaning).","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/index.html","id":"utility","dir":"","previous_headings":"","what":"Utility","title":"LimpiaR","text":"Miscellaneous functions designed speed aspects cleaning text.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/index.html","id":"parts-of-speech-processing","dir":"","previous_headings":"","what":"Parts of Speech Processing","title":"LimpiaR","text":"collection functions collectively make Parts Speech (POS) analysis workflow.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/code_browser_emojis.html","id":null,"dir":"Reference","previous_headings":"","what":"spanish_emojis_df â€” code_browser_emojis","title":"spanish_emojis_df â€” code_browser_emojis","text":"data frame columns emoji converting Spanish data frame columns emoji converting","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/code_browser_emojis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"spanish_emojis_df â€” code_browser_emojis","text":"","code":"data(\"spanish_emojis_df\")  data(\"code_browser_emojis\")"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/code_browser_emojis.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"spanish_emojis_df â€” code_browser_emojis","text":"6 columns 1307 rows Message Data frame emoji conversions Spanish 3 columns 1853 rows Message Data frame emoji conversions","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/data_param.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper for consistent documentation of data â€” data_param","title":"Helper for consistent documentation of data â€” data_param","text":"Use @inheritParams data_param consistently document data.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/data_param.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper for consistent documentation of data â€” data_param","text":"data Name Data Frame Tibble object","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/entities.html","id":null,"dir":"Reference","previous_headings":"","what":"entities â€” entities","title":"entities â€” entities","text":"data frame entities converting removing","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/entities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"entities â€” entities","text":"","code":"data(\"entities\")"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/entities.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"entities â€” entities","text":"4 columns 193 rows token regex pattern product replacement string replace n count entity original data word_count number words entity's string comprises","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_accents.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean accented characters â€” limpiar_accents","title":"Clean accented characters â€” limpiar_accents","text":"Useful reducing overall number tokens. Warning: removing accents results loss information, done care.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_accents.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean accented characters â€” limpiar_accents","text":"","code":"limpiar_accents(df, text_var = mention_content)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_accents.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean accented characters â€” limpiar_accents","text":"df Name Data Frame Tibble object text_var Name text variable/character vector","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_accents.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean accented characters â€” limpiar_accents","text":"Data Frame Tibble object accents text variable replaced","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_accents.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean accented characters â€” limpiar_accents","text":"","code":"limpiar_examples %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  5 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  6 \"   Lo q no tenemos es tiempo.   MaÃ±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustarÃ­a quedarme un ratito mÃ¡s\"                             limpiar_examples %>% limpiar_accents() %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situacion\"                         #>  5 \"nos han metido en una muy dificil situacion\"                         #>  6 \"   Lo q no tenemos es tiempo.   Manyana    debemos luchar.   \"       #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustaria quedarme un ratito mas\""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_alphanumeric.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove everything except letters, numbers, and spaces â€” limpiar_alphanumeric","title":"Remove everything except letters, numbers, and spaces â€” limpiar_alphanumeric","text":"simple regex retaining -z, -Z 0-9 well white space characters, including new lines. function remove accented characters, non-English characters, punctuation, etc. heavy-duty approach cleaning used prudently. know need keep accents, try limpiar_non_ascii first, avoiding functions altogether.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_alphanumeric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove everything except letters, numbers, and spaces â€” limpiar_alphanumeric","text":"","code":"limpiar_alphanumeric(data, text_var = mention_content)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_alphanumeric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove everything except letters, numbers, and spaces â€” limpiar_alphanumeric","text":"data Name Data Frame Tibble object text_var Name text variable. Can given 'string' symbol - refer column inside data","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_alphanumeric.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove everything except letters, numbers, and spaces â€” limpiar_alphanumeric","text":"Data frame text variable changed place","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_alphanumeric.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove everything except letters, numbers, and spaces â€” limpiar_alphanumeric","text":"","code":"test_df <- data.frame( text = c(   \"Simple text 123\",              # Basic ASCII only   \"Hello! How are you? ðŸ˜Š ðŸŒŸ\",    # ASCII + punctuation + emojis   \"cafÃ© MÃ¼nchen niÃ±o\",            # Latin-1 accented characters   \"#special@chars&(~)|[$]\",       # Special characters and symbols   \"æ··åˆæ±‰å­—ã¨æ—¥æœ¬èªž â†’ âŒ˜ Â£â‚¬Â¥\"      # CJK characters + symbols + arrows ) )  limpiar_alphanumeric(test_df, text) #>                  text #> 1     Simple text 123 #> 2 Hello How are you   #> 3      caf Mnchen nio #> 4        specialchars #> 5"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_duplicates.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean the text variable of duplicate posts â€” limpiar_duplicates","title":"Clean the text variable of duplicate posts â€” limpiar_duplicates","text":"Removes duplicate posts, posts deleted protected APIs","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_duplicates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean the text variable of duplicate posts â€” limpiar_duplicates","text":"","code":"limpiar_duplicates(data, text_var = mention_content)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_duplicates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean the text variable of duplicate posts â€” limpiar_duplicates","text":"data Data Frame Tibble object text_var Name text variable/character vector","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_duplicates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean the text variable of duplicate posts â€” limpiar_duplicates","text":"Data Frame Tibble object duplicate posts removed text variable","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_duplicates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean the text variable of duplicate posts â€” limpiar_duplicates","text":"","code":"df <- data.frame(text_variable = cbind(c( \"Deleted or protected mention\", \"hello\", \"goodbye\", \"goodbye\"))) limpiar_duplicates(df, text_variable) #>   text_variable #> 1         hello #> 2       goodbye  limpiar_examples #> # A tibble: 10 Ã— 5 #>    doc_id author_name       mention_content    mention_url platform_interactions #>     <int> <chr>             <chr>              <chr>       <lgl>                 #>  1      1 don_quijote       \"mi amigo sancho â€¦ www.twitteâ€¦ NA                    #>  2      2 sancho_panza      \"RT mi amigo sancâ€¦ www.twitteâ€¦ NA                    #>  3      3 edmond_dantes     \"@don_quijote no â€¦ www.twitteâ€¦ NA                    #>  4      4 el_sordo          \"nos han metido eâ€¦ www.fakeboâ€¦ NA                    #>  5      5 commander_miranda \"nos han metido eâ€¦ www.fakeboâ€¦ NA                    #>  6      6 robert_jordan     \"   Lo q no tenemâ€¦ www.youtubâ€¦ NA                    #>  7      7 anselmo           \"a mi es muy gravâ€¦ www.twitteâ€¦ NA                    #>  8      8 maria             \"ayyy nooo @roberâ€¦ www.twitteâ€¦ NA                    #>  9      9 pablo             \"todos se unen a â€¦ www.instagâ€¦ NA                    #> 10     10 pilar             \"a mi me gustarÃ­aâ€¦ www.instagâ€¦ NA                      limpiar_examples %>% limpiar_duplicates() #> # A tibble: 9 Ã— 5 #>   doc_id author_name   mention_content         mention_url platform_interactions #>    <int> <chr>         <chr>                   <chr>       <lgl>                 #> 1      1 don_quijote   \"mi amigo sancho es unâ€¦ www.twitteâ€¦ NA                    #> 2      2 sancho_panza  \"RT mi amigo sancho esâ€¦ www.twitteâ€¦ NA                    #> 3      3 edmond_dantes \"@don_quijote no digasâ€¦ www.twitteâ€¦ NA                    #> 4      4 el_sordo      \"nos han metido en unaâ€¦ www.fakeboâ€¦ NA                    #> 5      6 robert_jordan \"   Lo q no tenemos esâ€¦ www.youtubâ€¦ NA                    #> 6      7 anselmo       \"a mi es muy grave quiâ€¦ www.twitteâ€¦ NA                    #> 7      8 maria         \"ayyy nooo @robert_jorâ€¦ www.twitteâ€¦ NA                    #> 8      9 pablo         \"todos se unen a nuestâ€¦ www.instagâ€¦ NA                    #> 9     10 pilar         \"a mi me gustarÃ­a quedâ€¦ www.instagâ€¦ NA"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_emojis_es.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace emojis with a Spanish textual description â€” limpiar_emojis_es","title":"Replace emojis with a Spanish textual description â€” limpiar_emojis_es","text":"Spanish version limpiar_emojis function. Main usage pre-processing text variable part Deep Learning pipeline. important argument whether add emoji tag, also print snake case.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_emojis_es.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace emojis with a Spanish textual description â€” limpiar_emojis_es","text":"","code":"limpiar_emojis_es(df, text_var = mention_content, with_emoji_tag = FALSE)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_emojis_es.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace emojis with a Spanish textual description â€” limpiar_emojis_es","text":"df Name Data Frame Tibble Object text_var Name text variable with_emoji_tag Whether replace snakecase linked words ","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_emojis_es.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Replace emojis with a Spanish textual description â€” limpiar_emojis_es","text":"Data Frame Tibble object emojis cleaned text variable","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_emojis_es.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Replace emojis with a Spanish textual description â€” limpiar_emojis_es","text":"","code":"limpiar_examples %>% limpiar_emojis_es() %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                     #>    <chr>                                                               #>  1 mi amigo sancho es un wn de vdd jajaja                              #>  2 RT mi amigo sancho es un wn de vdd jajaja                           #>  3 @don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho #>  4 nos han metido en una muy dificil situaciÃ³n                         #>  5 nos han metido en una muy dificil situaciÃ³n                         #>  6 Lo q no tenemos es tiempo. MaÃ±ana debemos luchar.                   #>  7 a mi es muy grave quitarle la vida al otro                          #>  8 ayyy nooo @robert_jordan cara llorosa cara llorosa cara llorosa     #>  9 todos se unen a nuestro grupo hagale un clic https::larebelion.es   #> 10 a mi me gustarÃ­a quedarme un ratito mÃ¡s"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_ex_subreddits.html","id":null,"dir":"Reference","previous_headings":"","what":"Quickly extract subreddits from a link variable â€” limpiar_ex_subreddits","title":"Quickly extract subreddits from a link variable â€” limpiar_ex_subreddits","text":"Quickly extract subreddits link variable","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_ex_subreddits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quickly extract subreddits from a link variable â€” limpiar_ex_subreddits","text":"","code":"limpiar_ex_subreddits(df, url_var = permalink)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_ex_subreddits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quickly extract subreddits from a link variable â€” limpiar_ex_subreddits","text":"df Data Frame Tibble Object url_var variable containing URLS e.g. permalink","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_ex_subreddits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quickly extract subreddits from a link variable â€” limpiar_ex_subreddits","text":"df additional column","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_examples.html","id":null,"dir":"Reference","previous_headings":"","what":"Tibble of examples for LimpiaR functions â€” limpiar_examples","title":"Tibble of examples for LimpiaR functions â€” limpiar_examples","text":"Tibble examples LimpiaR functions","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_examples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tibble of examples for LimpiaR functions â€” limpiar_examples","text":"","code":"data(\"limpiar_examples\")"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_examples.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Tibble of examples for LimpiaR functions â€” limpiar_examples","text":"tibble 10 rows 5 columns doc_id Post ID author_name author's public username mention_content Text variable mention_url Link original post platform_interactions Number interactions parent platform","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_inspect.html","id":null,"dir":"Reference","previous_headings":"","what":"Inspect every post and URL which contains a pattern â€” limpiar_inspect","title":"Inspect every post and URL which contains a pattern â€” limpiar_inspect","text":"Produces viewable data frame posts matching regular expression    Useful investigating suspected spam posts, patterns interest. Set name title avoid new frames overwriting old ones.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_inspect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inspect every post and URL which contains a pattern â€” limpiar_inspect","text":"","code":"limpiar_inspect(   data,   pattern,   text_var = mention_content,   url_var = mention_url,   title = \"inspect\",   open_view = TRUE,   ignore_case = TRUE )"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_inspect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inspect every post and URL which contains a pattern â€” limpiar_inspect","text":"data Name Data Frame Tibble object pattern Pattern wish inspect e.g. \"link bio\" text_var Name text variable. Can given 'string' symbol - refer column inside data url_var Name data frame's URL-column title Name viewable pane open_view testing purposes, default set TRUE ignore_case Whether pattern ignore upper case/lower case distinction","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_inspect.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Inspect every post and URL which contains a pattern â€” limpiar_inspect","text":"add boundary tags e.g. \\\\b either side pattern wish match words rather parts words. example, pattern=\"cats\" match '#cats', also 'catch '. add word boundary: pattern = \\\\bcats\\\\b match either '#cats' 'catch '.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_inspect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inspect every post and URL which contains a pattern â€” limpiar_inspect","text":"","code":"df <- data.frame( text_variable = rbind(\"check me out\", \"don't look at me\"), text_url = rbind(\"www.twitter.com\", \"www.facebook.com\")) limpiar_inspect(df, \"check\", text_var = text_variable, url_var = text_url)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_link_click.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare a URL column to be clickable in Shiny/Data Table â€” limpiar_link_click","title":"Prepare a URL column to be clickable in Shiny/Data Table â€” limpiar_link_click","text":"allow click hyperlink load URL, e.g. selecting image.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_link_click.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare a URL column to be clickable in Shiny/Data Table â€” limpiar_link_click","text":"","code":"limpiar_link_click(df, url_var)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_link_click.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare a URL column to be clickable in Shiny/Data Table â€” limpiar_link_click","text":"df Data Frame Tibble Object url_var URL Column","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_link_click.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare a URL column to be clickable in Shiny/Data Table â€” limpiar_link_click","text":"data frame URL column edited clickable","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_link_click.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare a URL column to be clickable in Shiny/Data Table â€” limpiar_link_click","text":"Make sure DataTable rendered argument 'escape = FALSE' column text. function now checks url_var clickable link, add new formatting.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_link_click.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare a URL column to be clickable in Shiny/Data Table â€” limpiar_link_click","text":"","code":"df <- LimpiaR::limpiar_examples[1, ] df[\"mention_url\"] #> # A tibble: 1 Ã— 1 #>   mention_url                    #>   <chr>                          #> 1 www.twitter.com/ejemplo/124864  df <- df %>% limpiar_link_click(mention_url) df[\"mention_url\"] #> # A tibble: 1 Ã— 1 #>   mention_url                                                               #>   <chr>                                                                     #> 1 <a href='www.twitter.com/ejemplo/124864' target='blank'>Click to View<\/a>"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_link_click_reverse.html","id":null,"dir":"Reference","previous_headings":"","what":"Reverses (inverts) limpiar_link_click â€” limpiar_link_click_reverse","title":"Reverses (inverts) limpiar_link_click â€” limpiar_link_click_reverse","text":"Undoes effects limpiar_link_click function, giving original url variable back.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_link_click_reverse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reverses (inverts) limpiar_link_click â€” limpiar_link_click_reverse","text":"","code":"limpiar_link_click_reverse(df, url_var)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_link_click_reverse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reverses (inverts) limpiar_link_click â€” limpiar_link_click_reverse","text":"df Data Farame Tibble Object url_var URL Column","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_link_click_reverse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reverses (inverts) limpiar_link_click â€” limpiar_link_click_reverse","text":"Data frame url_var original form","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_link_click_reverse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reverses (inverts) limpiar_link_click â€” limpiar_link_click_reverse","text":"","code":"df <- LimpiaR::limpiar_examples[1, ]  df <- df %>% limpiar_link_click(mention_url) df %>% limpiar_link_click_reverse(mention_url) #> # A tibble: 1 Ã— 5 #>   doc_id author_name mention_content           mention_url platform_interactions #>    <int> <chr>       <chr>                     <chr>       <lgl>                 #> 1      1 don_quijote mi amigo sancho es un wnâ€¦ www.twitteâ€¦ NA"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_na_cols.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean NA-heavy columns from a Data Frame or Tibble â€” limpiar_na_cols","title":"Clean NA-heavy columns from a Data Frame or Tibble â€” limpiar_na_cols","text":"Remove columns whose proportion NAs higher determined threshold. Setting threshold 0.25 asks R remove columns 25% NA values. Can useful dealing large data frames, many columns redundant.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_na_cols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean NA-heavy columns from a Data Frame or Tibble â€” limpiar_na_cols","text":"","code":"limpiar_na_cols(df, threshold)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_na_cols.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean NA-heavy columns from a Data Frame or Tibble â€” limpiar_na_cols","text":"df Data Frame Tibble object threshold Threshold non-NA entries column must exceed retained.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_na_cols.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean NA-heavy columns from a Data Frame or Tibble â€” limpiar_na_cols","text":"Data Frame Tibble NA-heavy columns purged","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_na_cols.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean NA-heavy columns from a Data Frame or Tibble â€” limpiar_na_cols","text":"","code":"limpiar_examples #> # A tibble: 10 Ã— 5 #>    doc_id author_name       mention_content    mention_url platform_interactions #>     <int> <chr>             <chr>              <chr>       <lgl>                 #>  1      1 don_quijote       \"mi amigo sancho â€¦ www.twitteâ€¦ NA                    #>  2      2 sancho_panza      \"RT mi amigo sancâ€¦ www.twitteâ€¦ NA                    #>  3      3 edmond_dantes     \"@don_quijote no â€¦ www.twitteâ€¦ NA                    #>  4      4 el_sordo          \"nos han metido eâ€¦ www.fakeboâ€¦ NA                    #>  5      5 commander_miranda \"nos han metido eâ€¦ www.fakeboâ€¦ NA                    #>  6      6 robert_jordan     \"   Lo q no tenemâ€¦ www.youtubâ€¦ NA                    #>  7      7 anselmo           \"a mi es muy gravâ€¦ www.twitteâ€¦ NA                    #>  8      8 maria             \"ayyy nooo @roberâ€¦ www.twitteâ€¦ NA                    #>  9      9 pablo             \"todos se unen a â€¦ www.instagâ€¦ NA                    #> 10     10 pilar             \"a mi me gustarÃ­aâ€¦ www.instagâ€¦ NA                     limpiar_examples %>% limpiar_na_cols(0.1) #> # A tibble: 10 Ã— 4 #>    doc_id author_name       mention_content                          mention_url #>     <int> <chr>             <chr>                                    <chr>       #>  1      1 don_quijote       \"mi amigo sancho es un wn de vdd jajaja\" www.twitteâ€¦ #>  2      2 sancho_panza      \"RT mi amigo sancho es un wn de vdd jajâ€¦ www.twitteâ€¦ #>  3      3 edmond_dantes     \"@don_quijote no digas eso, tu amigo esâ€¦ www.twitteâ€¦ #>  4      4 el_sordo          \"nos han metido en una muy dificil situâ€¦ www.fakeboâ€¦ #>  5      5 commander_miranda \"nos han metido en una muy dificil situâ€¦ www.fakeboâ€¦ #>  6      6 robert_jordan     \"   Lo q no tenemos es tiempo.   MaÃ±anaâ€¦ www.youtubâ€¦ #>  7      7 anselmo           \"a mi es muy grave quitarle la vida al â€¦ www.twitteâ€¦ #>  8      8 maria             \"ayyy nooo @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"     www.twitteâ€¦ #>  9      9 pablo             \"todos se unen a nuestro grupo hagale uâ€¦ www.instagâ€¦ #> 10     10 pilar             \"a mi me gustarÃ­a quedarme un ratito mÃ¡â€¦ www.instagâ€¦"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_non_ascii.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove non-ASCII characters except those with latin accents â€” limpiar_non_ascii","title":"Remove non-ASCII characters except those with latin accents â€” limpiar_non_ascii","text":"Function uses simple RegEx retain basic ASCII characters plus attempts retain characters latin accents. know want remove everything including accented characters use limpiar_alphanumeric.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_non_ascii.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove non-ASCII characters except those with latin accents â€” limpiar_non_ascii","text":"","code":"limpiar_non_ascii(data, text_var = mention_content)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_non_ascii.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove non-ASCII characters except those with latin accents â€” limpiar_non_ascii","text":"data Name Data Frame Tibble object text_var Name text variable. Can given 'string' symbol - refer column inside data","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_non_ascii.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove non-ASCII characters except those with latin accents â€” limpiar_non_ascii","text":"Data frame text variable changed place","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_non_ascii.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove non-ASCII characters except those with latin accents â€” limpiar_non_ascii","text":"","code":"test_df <- data.frame( text = c(   \"Simple text 123\",              # Basic ASCII only   \"Hello! How are you? ðŸ˜Š ðŸŒŸ\",    # ASCII + punctuation + emojis   \"cafÃ© MÃ¼nchen niÃ±o\",            # Latin-1 accented characters   \"#special@chars&(~)|[$]\",       # Special characters and symbols   \"æ··åˆæ±‰å­—ã¨æ—¥æœ¬èªž â†’ âŒ˜ Â£â‚¬Â¥\"      # CJK characters + symbols + arrows ) )  limpiar_non_ascii(test_df, text) #>                     text #> 1        Simple text 123 #> 2  Hello! How are you?   #> 3      cafÃ© MÃ¼nchen niÃ±o #> 4 #special@chars&(~)|[$] #> 5"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pos_annotate.html","id":null,"dir":"Reference","previous_headings":"","what":"Annotate Texts for Parts of Speech Analysis using udpipe models. â€” limpiar_pos_annotate","title":"Annotate Texts for Parts of Speech Analysis using udpipe models. â€” limpiar_pos_annotate","text":"Take data frame text id variable extract parts speech using udpipe models import specific language limpiar_pos_import_model. function annotate data frame according language imported model.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pos_annotate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Annotate Texts for Parts of Speech Analysis using udpipe models. â€” limpiar_pos_annotate","text":"","code":"limpiar_pos_annotate(   data,   text_var,   id_var,   pos_model,   ...,   in_parallel = FALSE,   dependency_parse = FALSE,   update_progress = 100 )"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pos_annotate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Annotate Texts for Parts of Speech Analysis using udpipe models. â€” limpiar_pos_annotate","text":"data data.frame tibble object containing texts user wishes conduct parts speech analysis . text_var texts sentences user wishes perform parts speech annotations . id_var Unique identifier document. default supplied. recommended use 'universal_message_id' using social listening export. pos_model UDPipe model imported using limpiar_pos_import_model - must class 'udpipe_model'. ... enable user supply additional arguments udpipe::udpipe. in_parallel logical argument allowing user initiate parallel processing speed annotate function . set TRUE,  function select number available cores minus one, processing efficiently(faster), leaving one core manage computations. default FALSE. dependency_parse Whether perform dependency parsing tokens. default set FALSE parsing dependencies takes considerable time always needed. update_progress user option state often like progress report annotation process, posted console stating whether want message every 100, 500 1000 documents. useful annotating large sets data serves sanity check ensure session used available memory annotations stopped running.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pos_annotate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Annotate Texts for Parts of Speech Analysis using udpipe models. â€” limpiar_pos_annotate","text":"Returns data frame documents broken token sentence level, addition existing variables present data supplied function. returned object contains parts speech annotations CONLL-U formatting, row annotation word. find formatting methods, read . additional arguments tagged POS information follows: paragraph_id: identifier indicating paragraph annotated token derived . sentence_id: Similar paragraph_id sentence level. sentence: sentence annotated token derived . token_id: Token index, integer starting 1 new sentence. May range multiword tokens decimal number empty nodes. token: token(word) annotated parts speech. lemma: lemmatized version annotated token. pos_tag: universal parts speech tag token. information. xpos: treebank-specific parts speech tag token. feats: morphological features token, used dependency parsing visualisations,. head_token_id: Indicating token id head token, indicating token sentence related. dependency_tag: information regarding dependency parsing tokens, displaying type relation token head_token_id. information.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pos_annotate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Annotate Texts for Parts of Speech Analysis using udpipe models. â€” limpiar_pos_annotate","text":"leg work done udpipe. implemented LimpiaR : Taking data first argument allows integrate Tidyerse workflows (makes function pipe-able) want mental model consistent possible, .e. using LimpiaR pre-processing pipeline user mainly calls LimpiaR, remember another package. many potential workflows try enumerate . However,  clear use-cases emerge create new limpiar_pos_ functions case--case basis. example workflow convert adjectives nouns lemma visualise results.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pos_annotate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Annotate Texts for Parts of Speech Analysis using udpipe models. â€” limpiar_pos_annotate","text":"","code":"data <- dplyr::tibble(text = tolower(stringr::sentences[1:100]), document = 1:100) model <- LimpiaR::limpiar_pos_import_model(language = \"english\") #> Downloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/english-ewt-ud-2.5-191206.udpipe to /home/runner/work/_temp/Library/LimpiaR/model_cache/english-ewt-ud-2.5-191206.udpipe #>  - This model has been trained on version 2.5 of data from https://universaldependencies.org #>  - The model is distributed under the CC-BY-SA-NC license: https://creativecommons.org/licenses/by-nc-sa/4.0 #>  - Visit https://github.com/jwijffels/udpipe.models.ud.2.5 for model license details. #>  - For a list of all models and their licenses (most models you can download with this package have either a CC-BY-SA or a CC-BY-SA-NC license) read the documentation at ?udpipe_download_model. For building your own models: visit the documentation by typing vignette('udpipe-train', package = 'udpipe') #> Downloading finished, model stored at '/home/runner/work/_temp/Library/LimpiaR/model_cache/english-ewt-ud-2.5-191206.udpipe' annotations <- limpiar_pos_annotate(data = data,                                    text_var = text,                                    id_var = document,                                    pos_model = model,                                    in_parallel = FALSE,                                    dependency_parse = TRUE,                                    progress = \"100\") #> 2025-10-02 10:16:05.063033 Annotating text fragment 1/100"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pos_import_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Import UDPipe models to begin Parts of Speech Analysis â€” limpiar_pos_import_model","title":"Import UDPipe models to begin Parts of Speech Analysis â€” limpiar_pos_import_model","text":"function retrieves downloads pre-built models made UDPipe community, covering 65 different languages. information models available, visit:UDPipe Documentation","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pos_import_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Import UDPipe models to begin Parts of Speech Analysis â€” limpiar_pos_import_model","text":"","code":"limpiar_pos_import_model(language)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pos_import_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Import UDPipe models to begin Parts of Speech Analysis â€” limpiar_pos_import_model","text":"language chosen language user wishes select. 65 options choose ","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pos_import_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Import UDPipe models to begin Parts of Speech Analysis â€” limpiar_pos_import_model","text":"Loads model memory, ready annotation steps parts speech workflow.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pos_import_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Import UDPipe models to begin Parts of Speech Analysis â€” limpiar_pos_import_model","text":"","code":"pos_model <- limpiar_pos_import_model(language = \"english\")"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pp_companies.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove known companies for pits & peaks â€” limpiar_pp_companies","title":"Remove known companies for pits & peaks â€” limpiar_pp_companies","text":"Remove known companies pits & peaks","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pp_companies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove known companies for pits & peaks â€” limpiar_pp_companies","text":"","code":"limpiar_pp_companies(df, text_var)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pp_companies.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove known companies for pits & peaks â€” limpiar_pp_companies","text":"df Data Frame Tibble object text_var Name text variable","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pp_companies.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove known companies for pits & peaks â€” limpiar_pp_companies","text":"Data Frame Tibble object text variable edited inline","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pp_products.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace entities for the Peaks&Pit classifier â€” limpiar_pp_products","title":"Replace entities for the Peaks&Pit classifier â€” limpiar_pp_products","text":"Replace entities Peaks&Pit classifier","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pp_products.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace entities for the Peaks&Pit classifier â€” limpiar_pp_products","text":"","code":"limpiar_pp_products(df, text_var)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pp_products.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace entities for the Peaks&Pit classifier â€” limpiar_pp_products","text":"df Data Frame Tibble object text_var Name text variable","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_pp_products.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Replace entities for the Peaks&Pit classifier â€” limpiar_pp_products","text":"Data Frame Tibble object text variable edited inline","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_recode_emojis.html","id":null,"dir":"Reference","previous_headings":"","what":"Recode emojis with a textual description â€” limpiar_recode_emojis","title":"Recode emojis with a textual description â€” limpiar_recode_emojis","text":"Main usage pre-processing text variable part Deep Learning pipeline. important argument whether add emoji tag, also print snake case.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_recode_emojis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recode emojis with a textual description â€” limpiar_recode_emojis","text":"","code":"limpiar_recode_emojis(data, text_var = mention_content, with_emoji_tag = FALSE)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_recode_emojis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recode emojis with a textual description â€” limpiar_recode_emojis","text":"data Name Data Frame Tibble object text_var Name text variable. Can given 'string' symbol - refer column inside data with_emoji_tag Whether replace snakecase linked words ","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_recode_emojis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recode emojis with a textual description â€” limpiar_recode_emojis","text":"Data Frame Tibble object emojis cleaned text variable","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_recode_emojis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recode emojis with a textual description â€” limpiar_recode_emojis","text":"","code":"emojis <- data.frame(  text = c(\"Hello ðŸ‘‹ World\",   \"Family: ðŸ‘¨â€ðŸ‘©â€ðŸ‘§â€ðŸ‘¦\",   \"Coding ðŸ‘¨ðŸ½â€ðŸ’»\",   \"Flags ðŸ³ï¸â€ðŸŒˆ ðŸ‡ºðŸ‡¸\",   \"Weather â˜€ï¸ â›ˆï¸ â„ï¸\") )  emojis #>               text #> 1   Hello ðŸ‘‹ World #> 2 Family: ðŸ‘¨â€ðŸ‘©â€ðŸ‘§â€ðŸ‘¦ #> 3    Coding ðŸ‘¨ðŸ½â€ðŸ’» #> 4     Flags ðŸ³ï¸â€ðŸŒˆ ðŸ‡ºðŸ‡¸ #> 5    Weather â˜€ï¸ â›ˆï¸ â„ï¸  # Without tagging and combining: limpiar_recode_emojis(emojis, text) #>                                                     text #> 1                                Hello waving hand World #> 2                          Family: man â€ woman â€ girl â€ boy #> 3                                   Coding man ðŸ½â€ laptop #> 4          Flags white flag ï¸â€ rainbow flag: United States #> 5 Weather sun ï¸ cloud with lightning and rain ï¸ snowflake ï¸  # With tagging and combining: limpiar_recode_emojis(emojis, text, TRUE) #>                                                                       text #> 1                                            Hello waving_hand_emoji World #> 2                    Family: man_emoji â€ woman_emoji â€ girl_emoji â€ boy_emoji #> 3                                         Coding man_emoji ðŸ½â€ laptop_emoji #> 4          Flags white_flag_emoji ï¸â€ rainbow_emoji flag:_United_States_emoji #> 5 Weather sun_emoji ï¸ cloud_with_lightning_and_rain_emoji ï¸ snowflake_emoji ï¸  # using limpiar_remove_emojis() to remove them entirely: limpiar_remove_emojis(emojis, text) #>           text #> 1 Hello  World #> 2     Family: â€â€â€ #> 3      Coding â€ #> 4      Flags â€  #> 5   Weather"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_remove_emojis.html","id":null,"dir":"Reference","previous_headings":"","what":"Completely Remove Most Emojis from Text â€” limpiar_remove_emojis","title":"Completely Remove Most Emojis from Text â€” limpiar_remove_emojis","text":"uses simple Regular Expression (RegEx) clear emojis text variable. Attempts handle emojis joined together - like family emojis, 'edited emojis' like skin tones etc. set","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_remove_emojis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Completely Remove Most Emojis from Text â€” limpiar_remove_emojis","text":"","code":"limpiar_remove_emojis(data, text_var = mention_content)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_remove_emojis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Completely Remove Most Emojis from Text â€” limpiar_remove_emojis","text":"data Name Data Frame Tibble object text_var Name text variable. Can given 'string' symbol - refer column inside data","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_remove_emojis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Completely Remove Most Emojis from Text â€” limpiar_remove_emojis","text":"Data Frame text variable cleaned place","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_remove_emojis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Completely Remove Most Emojis from Text â€” limpiar_remove_emojis","text":"","code":"emojis <- data.frame(  text = c(\"Hello ðŸ‘‹ World\",   \"Family: ðŸ‘¨â€ðŸ‘©â€ðŸ‘§â€ðŸ‘¦\",   \"Coding ðŸ‘¨ðŸ½â€ðŸ’»\",   \"Flags ðŸ³ï¸â€ðŸŒˆ ðŸ‡ºðŸ‡¸\",   \"Weather â˜€ï¸ â›ˆï¸ â„ï¸\") )  emojis #>               text #> 1   Hello ðŸ‘‹ World #> 2 Family: ðŸ‘¨â€ðŸ‘©â€ðŸ‘§â€ðŸ‘¦ #> 3    Coding ðŸ‘¨ðŸ½â€ðŸ’» #> 4     Flags ðŸ³ï¸â€ðŸŒˆ ðŸ‡ºðŸ‡¸ #> 5    Weather â˜€ï¸ â›ˆï¸ â„ï¸  # using limpiar_remove_emojis() to remove them entirely: limpiar_remove_emojis(emojis, text) #>           text #> 1 Hello  World #> 2     Family: â€â€â€ #> 3      Coding â€ #> 4      Flags â€  #> 5   Weather"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_repeat_chars.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean repeated charaaaacters â€” limpiar_repeat_chars","title":"Clean repeated charaaaacters â€” limpiar_repeat_chars","text":"Removes multiple vowels (holaaaa) normalises common laughing patterns (jajaja, jejeje, ajajaaaaja). Useful visualisations, reducing overall number tokens present text variable.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_repeat_chars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean repeated charaaaacters â€” limpiar_repeat_chars","text":"","code":"limpiar_repeat_chars(df, text_var = mention_content)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_repeat_chars.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean repeated charaaaacters â€” limpiar_repeat_chars","text":"df Name Data Frame Tibble object text_var Name text variable/character vector. Default mention_content","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_repeat_chars.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean repeated charaaaacters â€” limpiar_repeat_chars","text":"Data Frame Tibble object repeat vowels & laughing patterns removed text variable","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_repeat_chars.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean repeated charaaaacters â€” limpiar_repeat_chars","text":"","code":"limpiar_examples %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  5 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  6 \"   Lo q no tenemos es tiempo.   MaÃ±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustarÃ­a quedarme un ratito mÃ¡s\"                              limpiar_examples %>% limpiar_repeat_chars() %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jaja\"                                #>  2 \"RT mi amigo sancho es un wn de vdd jaja\"                             #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  5 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  6 \"   Lo q no tenemos es tiempo.   MaÃ±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ay no @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"                                      #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustarÃ­a quedarme un ratito mÃ¡s\""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_retweets.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean retweets from the text variable â€” limpiar_retweets","title":"Clean retweets from the text variable â€” limpiar_retweets","text":"Removes posts 'rt' 'RT' tag. Particularly effective used conjunction ParseR & SegmentR visualisations.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_retweets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean retweets from the text variable â€” limpiar_retweets","text":"","code":"limpiar_retweets(df, text_var = mention_content)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_retweets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean retweets from the text variable â€” limpiar_retweets","text":"df Name Data Frame Tibble Object text_var Name text variable/character vector","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_retweets.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean retweets from the text variable â€” limpiar_retweets","text":"","code":"df <- data.frame(text_variable = cbind(c(\"rt <3\", \"RT <3\", \"original tweet\"))) limpiar_retweets(df, text_variable) #>    text_variable #> 1 original tweet  limpiar_examples #> # A tibble: 10 Ã— 5 #>    doc_id author_name       mention_content    mention_url platform_interactions #>     <int> <chr>             <chr>              <chr>       <lgl>                 #>  1      1 don_quijote       \"mi amigo sancho â€¦ www.twitteâ€¦ NA                    #>  2      2 sancho_panza      \"RT mi amigo sancâ€¦ www.twitteâ€¦ NA                    #>  3      3 edmond_dantes     \"@don_quijote no â€¦ www.twitteâ€¦ NA                    #>  4      4 el_sordo          \"nos han metido eâ€¦ www.fakeboâ€¦ NA                    #>  5      5 commander_miranda \"nos han metido eâ€¦ www.fakeboâ€¦ NA                    #>  6      6 robert_jordan     \"   Lo q no tenemâ€¦ www.youtubâ€¦ NA                    #>  7      7 anselmo           \"a mi es muy gravâ€¦ www.twitteâ€¦ NA                    #>  8      8 maria             \"ayyy nooo @roberâ€¦ www.twitteâ€¦ NA                    #>  9      9 pablo             \"todos se unen a â€¦ www.instagâ€¦ NA                    #> 10     10 pilar             \"a mi me gustarÃ­aâ€¦ www.instagâ€¦ NA                     limpiar_examples %>% limpiar_retweets() #> # A tibble: 9 Ã— 5 #>   doc_id author_name       mention_content     mention_url platform_interactions #>    <int> <chr>             <chr>               <chr>       <lgl>                 #> 1      1 don_quijote       \"mi amigo sancho eâ€¦ www.twitteâ€¦ NA                    #> 2      3 edmond_dantes     \"@don_quijote no dâ€¦ www.twitteâ€¦ NA                    #> 3      4 el_sordo          \"nos han metido enâ€¦ www.fakeboâ€¦ NA                    #> 4      5 commander_miranda \"nos han metido enâ€¦ www.fakeboâ€¦ NA                    #> 5      6 robert_jordan     \"   Lo q no tenemoâ€¦ www.youtubâ€¦ NA                    #> 6      7 anselmo           \"a mi es muy graveâ€¦ www.twitteâ€¦ NA                    #> 7      8 maria             \"ayyy nooo @robertâ€¦ www.twitteâ€¦ NA                    #> 8      9 pablo             \"todos se unen a nâ€¦ www.instagâ€¦ NA                    #> 9     10 pilar             \"a mi me gustarÃ­a â€¦ www.instagâ€¦ NA"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_shorthands.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean shorthands and abbreviations â€” limpiar_shorthands","title":"Clean shorthands and abbreviations â€” limpiar_shorthands","text":"Replaces common Spanish shorthands abbreviations longer form equivalents. Choose whether link replacements snake case , spaces_as_underscores. Useful primarily normalising text ahead sentiment classification.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_shorthands.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean shorthands and abbreviations â€” limpiar_shorthands","text":"","code":"limpiar_shorthands(   df,   text_var = mention_content,   spaces_as_underscores = FALSE )"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_shorthands.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean shorthands and abbreviations â€” limpiar_shorthands","text":"df Name Data Frame Tibble object text_var Name text variable/character vector spaces_as_underscores Whether multi-word corrections e.g. 'te quiero mucho' spaces underscores. Default = FALSE","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_shorthands.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean shorthands and abbreviations â€” limpiar_shorthands","text":"text variable shorthands replaced","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_shorthands.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean shorthands and abbreviations â€” limpiar_shorthands","text":"","code":"limpiar_examples %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  5 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  6 \"   Lo q no tenemos es tiempo.   MaÃ±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustarÃ­a quedarme un ratito mÃ¡s\"                              limpiar_examples %>% limpiar_shorthands() %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wuevon de verdad jajaja\"                       #>  2 \"RT mi amigo sancho es un wuevon de verdad jajaja\"                    #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  5 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  6 \"   Lo que no tenemos es tiempo.   MaÃ±ana    debemos luchar.   \"      #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustarÃ­a quedarme un ratito mÃ¡s\""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_slang.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean slang from multiple Spanish dialects â€” limpiar_slang","title":"Clean slang from multiple Spanish dialects â€” limpiar_slang","text":"Replaces slang phrases various Spanish dialects everyday terms. Function's primary use normalise text Deep Learning sentiment algorithm. Care taken using function, e.g. panda -> grupo, far common usage texts use. However, data set many people talk panda bears 'oso panda', unwanted changes. tried avoid problem possible, including things like 'la suda' instead changing 'suda'.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_slang.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean slang from multiple Spanish dialects â€” limpiar_slang","text":"","code":"limpiar_slang(df, text_var = mention_content)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_slang.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean slang from multiple Spanish dialects â€” limpiar_slang","text":"df Name Data Frame Tibble object text_var Name text variable/character vector","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_slang.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean slang from multiple Spanish dialects â€” limpiar_slang","text":"Data Frame Tibble object text variable altered","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_slang.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean slang from multiple Spanish dialects â€” limpiar_slang","text":"","code":"if (FALSE) { # \\dontrun{ df %>% limpiar_slang(text_var = text_var)} # }"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_spaces.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean redundant spaces â€” limpiar_spaces","title":"Clean redundant spaces â€” limpiar_spaces","text":"Remove excess spaces text variable.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_spaces.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean redundant spaces â€” limpiar_spaces","text":"","code":"limpiar_spaces(df, text_var = mention_content)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_spaces.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean redundant spaces â€” limpiar_spaces","text":"df Name Data Frame Tibble object text_var Name text variable/character vector","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_spaces.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean redundant spaces â€” limpiar_spaces","text":"text variable/character vector excess spaces removed","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_spaces.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean redundant spaces â€” limpiar_spaces","text":"","code":"df <- data.frame(text_variable = \"clean   the   spaces please\") limpiar_spaces(df, text_var = text_variable) #>             text_variable #> 1 clean the spaces please  limpiar_examples %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  5 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  6 \"   Lo q no tenemos es tiempo.   MaÃ±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustarÃ­a quedarme un ratito mÃ¡s\"                              limpiar_examples %>% limpiar_spaces() %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                     #>    <chr>                                                               #>  1 mi amigo sancho es un wn de vdd jajaja                              #>  2 RT mi amigo sancho es un wn de vdd jajaja                           #>  3 @don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho #>  4 nos han metido en una muy dificil situaciÃ³n                         #>  5 nos han metido en una muy dificil situaciÃ³n                         #>  6 Lo q no tenemos es tiempo. MaÃ±ana debemos luchar.                   #>  7 a mi es muy grave quitarle la vida al otro                          #>  8 ayyy nooo @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢                                   #>  9 todos se unen a nuestro grupo hagale un clic https::larebelion.es   #> 10 a mi me gustarÃ­a quedarme un ratito mÃ¡s"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_spam_grams.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove posts containing spam-like n-grams â€” limpiar_spam_grams","title":"Remove posts containing spam-like n-grams â€” limpiar_spam_grams","text":"#' Function identifies posts contain suspicious-looking n-gram patterns. Posts can removed, pattern inspected, posts removed . can re-assign current data frame 'clean' data frame third element list.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_spam_grams.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove posts containing spam-like n-grams â€” limpiar_spam_grams","text":"","code":"limpiar_spam_grams(data, text_var, n_gram, min_freq)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_spam_grams.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove posts containing spam-like n-grams â€” limpiar_spam_grams","text":"data Name Data Frame Tibble object text_var Name text variable. Can given 'string' symbol - refer column inside data n_gram Number words n-gram .e. n = 2 = bigram min_freq Minimum number times n-gram seen removed","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_spam_grams.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove posts containing spam-like n-grams â€” limpiar_spam_grams","text":"list 3 data frames 1. suspicious-looking n-grams, 2. data removed, 3. rows data frame removed","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_stopwords.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean stop words for visualisations â€” limpiar_stopwords","title":"Clean stop words for visualisations â€” limpiar_stopwords","text":"two lists - sentiment & topics, similar, words lists. However, sentiment analysis sensitive negation, negation cues e.g. \"\", \"nada\" etc. removed sentiment list. purposes, topics go-lists, care always advised removing stop words.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_stopwords.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean stop words for visualisations â€” limpiar_stopwords","text":"","code":"limpiar_stopwords(data, text_var = mention_content, stop_words)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_stopwords.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean stop words for visualisations â€” limpiar_stopwords","text":"data Name Data Frame Tibble object text_var Name text variable. Can given 'string' symbol - refer column inside data stop_words \"sentiment\" \"topics\" - sentiment retains negation cues","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_stopwords.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean stop words for visualisations â€” limpiar_stopwords","text":"text variable stop words specified list removed","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_stopwords.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Clean stop words for visualisations â€” limpiar_stopwords","text":"stop word list editable via data(\"sentiment_stops\") data(\"topic_stops\").","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_stopwords.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean stop words for visualisations â€” limpiar_stopwords","text":"","code":"limpiar_examples %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  5 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  6 \"   Lo q no tenemos es tiempo.   MaÃ±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustarÃ­a quedarme un ratito mÃ¡s\"                              limpiar_examples %>% limpiar_stopwords(stop_words = \"topics\") %>% dplyr::select(mention_content) %>% limpiar_spaces() #> # A tibble: 10 Ã— 1 #>    mention_content                               #>    <chr>                                         #>  1 amigo sancho wn vdd jajaja                    #>  2 RT amigo sancho wn vdd jajaja                 #>  3 @don_quijote digas, amigo honorable # #sancho #>  4 metido dificil situaciÃ³n                      #>  5 metido dificil situaciÃ³n                      #>  6 Lo q. MaÃ±ana debemos luchar.                  #>  7 grave quitarle vida                           #>  8 ayyy nooo @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢             #>  9 unen grupo hagale clic https::larebelion.     #> 10 gustarÃ­a quedarme ratito mÃ¡s"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_tags.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean user handles and hashtags â€” limpiar_tags","title":"Clean user handles and hashtags â€” limpiar_tags","text":"Function replaces user handles hashtags neutral tags. can choose whether replace hashtags & users either one.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_tags.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean user handles and hashtags â€” limpiar_tags","text":"","code":"limpiar_tags(df, text_var = mention_content, user = TRUE, hashtag = TRUE)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_tags.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean user handles and hashtags â€” limpiar_tags","text":"df Name Data Frame Tibble object text_var Name text variable/character vector user Whether replace user handles TRUE = replace hashtag Whether replace hashtags TRUE = replace","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_tags.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean user handles and hashtags â€” limpiar_tags","text":"Data Frame Tibble object user handles /hashtags removed text variable","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_tags.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean user handles and hashtags â€” limpiar_tags","text":"","code":"limpiar_examples #> # A tibble: 10 Ã— 5 #>    doc_id author_name       mention_content    mention_url platform_interactions #>     <int> <chr>             <chr>              <chr>       <lgl>                 #>  1      1 don_quijote       \"mi amigo sancho â€¦ www.twitteâ€¦ NA                    #>  2      2 sancho_panza      \"RT mi amigo sancâ€¦ www.twitteâ€¦ NA                    #>  3      3 edmond_dantes     \"@don_quijote no â€¦ www.twitteâ€¦ NA                    #>  4      4 el_sordo          \"nos han metido eâ€¦ www.fakeboâ€¦ NA                    #>  5      5 commander_miranda \"nos han metido eâ€¦ www.fakeboâ€¦ NA                    #>  6      6 robert_jordan     \"   Lo q no tenemâ€¦ www.youtubâ€¦ NA                    #>  7      7 anselmo           \"a mi es muy gravâ€¦ www.twitteâ€¦ NA                    #>  8      8 maria             \"ayyy nooo @roberâ€¦ www.twitteâ€¦ NA                    #>  9      9 pablo             \"todos se unen a â€¦ www.instagâ€¦ NA                    #> 10     10 pilar             \"a mi me gustarÃ­aâ€¦ www.instagâ€¦ NA                     #Both user and hashtags limpiar_examples %>% limpiar_tags() %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                     #>    <chr>                                                               #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                            #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                         #>  3 \"@user no digas eso, tu amigo es muy honorable hashtag hashtag\"     #>  4 \"nos han metido en una muy dificil situaciÃ³n\"                       #>  5 \"nos han metido en una muy dificil situaciÃ³n\"                       #>  6 \"   Lo q no tenemos es tiempo.   MaÃ±ana    debemos luchar.   \"      #>  7 \"a mi es muy grave quitarle la vida al otro\"                        #>  8 \"ayyy nooo @user ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"                                         #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\" #> 10 \"a mi me gustarÃ­a quedarme un ratito mÃ¡s\"                            #Just user tags limpiar_examples %>% limpiar_tags(hashtag = FALSE) %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                     #>    <chr>                                                               #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                            #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                         #>  3 \"@user no digas eso, tu amigo es muy honorable #vamos #sancho\"      #>  4 \"nos han metido en una muy dificil situaciÃ³n\"                       #>  5 \"nos han metido en una muy dificil situaciÃ³n\"                       #>  6 \"   Lo q no tenemos es tiempo.   MaÃ±ana    debemos luchar.   \"      #>  7 \"a mi es muy grave quitarle la vida al otro\"                        #>  8 \"ayyy nooo @user ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"                                         #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\" #> 10 \"a mi me gustarÃ­a quedarme un ratito mÃ¡s\"                            #Just hashtags limpiar_examples %>% limpiar_tags(user = FALSE) %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                        #>    <chr>                                                                  #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                               #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                            #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable hashtag hashtag\" #>  4 \"nos han metido en una muy dificil situaciÃ³n\"                          #>  5 \"nos han metido en una muy dificil situaciÃ³n\"                          #>  6 \"   Lo q no tenemos es tiempo.   MaÃ±ana    debemos luchar.   \"         #>  7 \"a mi es muy grave quitarle la vida al otro\"                           #>  8 \"ayyy nooo @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"                                   #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"    #> 10 \"a mi me gustarÃ­a quedarme un ratito mÃ¡s\""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_url.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean URLs from the text variable â€” limpiar_url","title":"Clean URLs from the text variable â€” limpiar_url","text":"Removes common forms URLs text variable","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_url.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean URLs from the text variable â€” limpiar_url","text":"","code":"limpiar_url(df, text_var = mention_content)"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_url.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean URLs from the text variable â€” limpiar_url","text":"df Data Frame Tibble Object text_var Name text variable/character vector","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_url.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean URLs from the text variable â€” limpiar_url","text":"Data Frame Tibble object URLs removed text variable","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_url.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean URLs from the text variable â€” limpiar_url","text":"","code":"limpiar_examples %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  5 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  6 \"   Lo q no tenemos es tiempo.   MaÃ±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustarÃ­a quedarme un ratito mÃ¡s\"                              limpiar_examples %>% limpiar_url() %>% dplyr::select(mention_content) #> # A tibble: 10 Ã— 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  5 \"nos han metido en una muy dificil situaciÃ³n\"                         #>  6 \"   Lo q no tenemos es tiempo.   MaÃ±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan ðŸ˜¢ ðŸ˜¢ ðŸ˜¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic \"                       #> 10 \"a mi me gustarÃ­a quedarme un ratito mÃ¡s\""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_wrap.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrap strings for visual ease â€” limpiar_wrap","title":"Wrap strings for visual ease â€” limpiar_wrap","text":"Useful pre-processing dataset need read many documents, scan lot documents, e.g. rendering interactive scatter plot using plotly's hover, using DT::datatable(escape = FALSE).","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_wrap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrap strings for visual ease â€” limpiar_wrap","text":"","code":"limpiar_wrap(   data,   text_var = mention_content,   n = 15,   newline_char = \"<br><br>\" )"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_wrap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrap strings for visual ease â€” limpiar_wrap","text":"data Name Data Frame Tibble object text_var Name text variable. Can given 'string' symbol - refer column inside data n number words newline_char specific delimiter wrap texts ","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_wrap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrap strings for visual ease â€” limpiar_wrap","text":"Data Frame text variable edited place","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/limpiar_wrap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrap strings for visual ease â€” limpiar_wrap","text":"","code":"limpiar_examples %>% limpiar_wrap(mention_content, n = 5, newline_char = \"<br>\") #> # A tibble: 10 Ã— 5 #>    doc_id author_name       mention_content    mention_url platform_interactions #>     <int> <chr>             <chr>              <chr>       <lgl>                 #>  1      1 don_quijote       \"mi amigo sancho â€¦ www.twitteâ€¦ NA                    #>  2      2 sancho_panza      \"RT mi amigo sancâ€¦ www.twitteâ€¦ NA                    #>  3      3 edmond_dantes     \"@don_quijote no â€¦ www.twitteâ€¦ NA                    #>  4      4 el_sordo          \"nos han metido eâ€¦ www.fakeboâ€¦ NA                    #>  5      5 commander_miranda \"nos han metido eâ€¦ www.fakeboâ€¦ NA                    #>  6      6 robert_jordan     \" Lo q no tenemosâ€¦ www.youtubâ€¦ NA                    #>  7      7 anselmo           \"a mi es muy gravâ€¦ www.twitteâ€¦ NA                    #>  8      8 maria             \"ayyy nooo @roberâ€¦ www.twitteâ€¦ NA                    #>  9      9 pablo             \"todos se unen a â€¦ www.instagâ€¦ NA                    #> 10     10 pilar             \"a mi me gustarÃ­aâ€¦ www.instagâ€¦ NA"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator â€” %>%","title":"Pipe operator â€” %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator â€” %>%","text":"","code":"lhs %>% rhs"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator â€” %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator â€” %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/sentiment_stops.html","id":null,"dir":"Reference","previous_headings":"","what":"sentiment_stops â€” sentiment_stops","title":"sentiment_stops â€” sentiment_stops","text":"Stop words sentiment analysis Spanish (negation cues left ).","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/sentiment_stops.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"sentiment_stops â€” sentiment_stops","text":"","code":"data(\"sentiment_stops\")"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/sentiment_stops.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"sentiment_stops â€” sentiment_stops","text":"tibble 2 columns & 682 rows Message Data frame stop word vectors","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/text_var.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper for consistent documentation of text_var â€” text_var","title":"Helper for consistent documentation of text_var â€” text_var","text":"Use @inheritParams text_var consistently document text_var.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/text_var.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper for consistent documentation of text_var â€” text_var","text":"text_var Name text variable. Can given 'string' symbol - refer column inside data","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/topic_stops.html","id":null,"dir":"Reference","previous_headings":"","what":"topic_stops â€” topic_stops","title":"topic_stops â€” topic_stops","text":"Stop words topic modelling Spanish.","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/reference/topic_stops.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"topic_stops â€” topic_stops","text":"","code":"data(\"topic_stops\")"},{"path":"https://jpcompartir.github.io/LimpiaR/reference/topic_stops.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"topic_stops â€” topic_stops","text":"tibble 2 columns & 704 rows Message Data frame stop word vectors","code":""},{"path":"https://jpcompartir.github.io/LimpiaR/news/index.html","id":"limpiar-110","dir":"Changelog","previous_headings":"","what":"LimpiaR 1.1.0","title":"LimpiaR 1.1.0","text":"Breaking Change - refactoring limpiar_emojis limpiar_recode_emojis limpiar_remove_emojis - limpiar_emojis longer exists code using need updated. Introduced limpiar_non_ascii limpiar_alphanumeric make removing special characters quicker easier Introduced limpiar_wrap_string reading outputs {DT}â€™s Data Tables {plotly} hovers. Introduced vignette - near_duplicates.Rmd using limpiar_spam_grams function remove near duplicates. Updated limpiar_intro.Rmd vignette. behind doors improvements documentation, error handling, tests","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/LimpiaR/news/index.html","id":"parts-of-speech-workflow-1-0-0","dir":"Changelog","previous_headings":"","what":"Parts of Speech Workflow","title":"LimpiaR 1.0.0","text":"Multilingual Caching support Annotate models limpiar_pos_annotate Removal limpiar_df function (mindful individual cleaning steps!)","code":""}]
