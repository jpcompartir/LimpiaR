[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021 Jack Penzer Permission hereby granted, free charge, person obtaining copy software associated documentation files (‚ÄúSoftware‚Äù), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED ‚Äú‚Äù, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/limpiar_intro.html","id":"loading-everything-into-r","dir":"Articles","previous_headings":"","what":"Loading Everything Into R","title":"Introduction to LimpiaR","text":"Productivity tip just setting programming/R journey: LimpiaR‚Äôs functions begin limpiar_. library loaded, typing limpiar_ Rstudio script Rmarkdown code block, produce drop menu LimpiaR functions, don‚Äôt need remember names .","code":"library(magrittr) library(dplyr) library(stringr) library(LimpiaR) data #> # A tibble: 10 √ó 2 #>    Mention.Content                                                   Mention.Url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa! c√≥√≥mo    est√°s @magdalena   ?!\"                       www.twitte‚Ä¶ #>  2 \"  han visto este articulo!? Que horror! https://guardian.com/em‚Ä¶ www.twitte‚Ä¶ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.facebo‚Ä¶ #>  4 \"jajajajaja eres un wn!\"                                          www.facebo‚Ä¶ #>  5 \"RT dale un click a ver una mujer baila con su perro\"             www.twitte‚Ä¶ #>  6 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #>  7 \"yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£ \"                    www.instag‚Ä¶ #>  8 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #>  9 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #> 10 \"grax ntonces q?\"                                                 www.youtub‚Ä¶"},{"path":"/articles/limpiar_intro.html","id":"column-names","dir":"Articles","previous_headings":"Loading Everything Into R","what":"Column Names","title":"Introduction to LimpiaR","text":"created data frame posts URLs. loading libraries data, first part workflow clean column names, makes using tab completion, accessing column names much faster (long run = big productivity gains). , ‚Äôll use janitor package. can uncomment code install janitor already installed machine.","code":"# ifelse(!\"janitor\" %in% installed.packages(), #    install.packages(\"janitor\"), library(janitor))  (data <- data %>%     janitor::clean_names()) #> # A tibble: 10 √ó 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa! c√≥√≥mo    est√°s @magdalena   ?!\"                       www.twitte‚Ä¶ #>  2 \"  han visto este articulo!? Que horror! https://guardian.com/em‚Ä¶ www.twitte‚Ä¶ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.facebo‚Ä¶ #>  4 \"jajajajaja eres un wn!\"                                          www.facebo‚Ä¶ #>  5 \"RT dale un click a ver una mujer baila con su perro\"             www.twitte‚Ä¶ #>  6 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #>  7 \"yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£ \"                    www.instag‚Ä¶ #>  8 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #>  9 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #> 10 \"grax ntonces q?\"                                                 www.youtub‚Ä¶"},{"path":"/articles/limpiar_intro.html","id":"lower-case-text-variable","dir":"Articles","previous_headings":"","what":"Lower Case Text Variable","title":"Introduction to LimpiaR","text":"workflows, next step make text variable lower case. make tokens like ‚ÄòAMAZING‚Äô ‚ÄòAmazing‚Äô -> ‚Äòamazing‚Äô. need LimpiaR function , base R function tolower() works just fine.","code":"(data <- data %>%   mutate(mention_content = tolower(mention_content))) #> # A tibble: 10 √ó 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa! c√≥√≥mo    est√°s @magdalena   ?!\"                       www.twitte‚Ä¶ #>  2 \"  han visto este articulo!? que horror! https://guardian.com/em‚Ä¶ www.twitte‚Ä¶ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.facebo‚Ä¶ #>  4 \"jajajajaja eres un wn!\"                                          www.facebo‚Ä¶ #>  5 \"rt dale un click a ver una mujer baila con su perro\"             www.twitte‚Ä¶ #>  6 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #>  7 \"yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£ \"                    www.instag‚Ä¶ #>  8 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #>  9 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #> 10 \"grax ntonces q?\"                                                 www.youtub‚Ä¶"},{"path":[]},{"path":"/articles/limpiar_intro.html","id":"limpiar_accents","dir":"Articles","previous_headings":"","what":"limpiar_accents","title":"Introduction to LimpiaR","text":"Now ‚Äôre going look LimpiaR‚Äôs functions individually. first function limpiar_accents, replace accents common Spanish words text variable, Latin-alphabet equivalents e.g.¬†‚Äò√© -> e‚Äô . use assignment operator make sure changes saved. Tip: can type ?limpiar_accents access documentation, see arguments need fill .","code":"(data <- data %>%   limpiar_accents(text_var = mention_content)) #> # A tibble: 10 √ó 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa! coomo    estas @magdalena   ?!\"                       www.twitte‚Ä¶ #>  2 \"  han visto este articulo!? que horror! https://guardian.com/em‚Ä¶ www.twitte‚Ä¶ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.facebo‚Ä¶ #>  4 \"jajajajaja eres un wn!\"                                          www.facebo‚Ä¶ #>  5 \"rt dale un click a ver una mujer baila con su perro\"             www.twitte‚Ä¶ #>  6 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #>  7 \"yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£ \"                    www.instag‚Ä¶ #>  8 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #>  9 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #> 10 \"grax ntonces q?\"                                                 www.youtub‚Ä¶"},{"path":"/articles/limpiar_intro.html","id":"limpiar_duplicates","dir":"Articles","previous_headings":"limpiar_accents","what":"limpiar_duplicates","title":"Introduction to LimpiaR","text":"Now ‚Äôll remove duplicate posts, notice don‚Äôt actually need type text_var = mention_content, default argument text_var already mention_content. Note: text column data frame called ‚Äòtext‚Äô specify text_var = text, call:","code":"(data <- data %>%   limpiar_duplicates()) #> # A tibble: 7 √ó 2 #>   mention_content                                                    mention_url #>   <chr>                                                              <chr>       #> 1 \"holaaaaaa! coomo    estas @magdalena   ?!\"                        www.twitte‚Ä¶ #> 2 \"  han visto este articulo!? que horror! https://guardian.com/emo‚Ä¶ www.twitte‚Ä¶ #> 3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\"  www.facebo‚Ä¶ #> 4 \"jajajajaja eres un wn!\"                                           www.facebo‚Ä¶ #> 5 \"rt dale un click a ver una mujer baila con su perro\"              www.twitte‚Ä¶ #> 6 \"grax ntonces q?\"                                                  www.youtub‚Ä¶ #> 7 \"yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£ \"                     www.instag‚Ä¶ data %>% rename(mention_content = text)"},{"path":"/articles/limpiar_intro.html","id":"limpiar_retweets","dir":"Articles","previous_headings":"limpiar_accents","what":"limpiar_retweets","title":"Introduction to LimpiaR","text":"need remove retweets, example create bigram network, limpiar function just .","code":"(data <- data %>%     limpiar_retweets()) #> # A tibble: 6 √ó 2 #>   mention_content                                                    mention_url #>   <chr>                                                              <chr>       #> 1 \"holaaaaaa! coomo    estas @magdalena   ?!\"                        www.twitte‚Ä¶ #> 2 \"  han visto este articulo!? que horror! https://guardian.com/emo‚Ä¶ www.twitte‚Ä¶ #> 3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\"  www.facebo‚Ä¶ #> 4 \"jajajajaja eres un wn!\"                                           www.facebo‚Ä¶ #> 5 \"grax ntonces q?\"                                                  www.youtub‚Ä¶ #> 6 \"yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£ \"                     www.instag‚Ä¶"},{"path":"/articles/limpiar_intro.html","id":"limpiar_url","dir":"Articles","previous_headings":"limpiar_accents","what":"limpiar_url","title":"Introduction to LimpiaR","text":"generally don‚Äôt want URLs appearing charts analyses, can remove limpiar_url function.","code":"(data <- data %>%    limpiar_url()) #> # A tibble: 6 √ó 2 #>   mention_content                                                   mention_url  #>   <chr>                                                             <chr>        #> 1 \"holaaaaaa! coomo    estas @magdalena   ?!\"                       www.twitter‚Ä¶ #> 2 \"  han visto este articulo!? que horror!  no se puede!!\"          www.twitter‚Ä¶ #> 3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.faceboo‚Ä¶ #> 4 \"jajajajaja eres un wn!\"                                          www.faceboo‚Ä¶ #> 5 \"grax ntonces q?\"                                                 www.youtube‚Ä¶ #> 6 \"yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£ \"                    www.instagr‚Ä¶"},{"path":"/articles/limpiar_intro.html","id":"limpiar_spaces","dir":"Articles","previous_headings":"limpiar_accents","what":"limpiar_spaces","title":"Introduction to LimpiaR","text":"Next ‚Äôll look use LimpiaR remove annoying white spaces, like beginning sentence, punctuation, multiple white spaces reason; common messy data often encounter.","code":"(data <- data %>%   limpiar_spaces()) #> # A tibble: 6 √ó 2 #>   mention_content                                               mention_url      #>   <chr>                                                         <chr>            #> 1 holaaaaaa! coomo estas @magdalena?!                           www.twitter.com‚Ä¶ #> 2 han visto este articulo!? que horror! no se puede!!           www.twitter.com‚Ä¶ #> 3 ayyyyyy a mi me gustaria ir a londres yaaa #llevame #porfavor www.facebook.co‚Ä¶ #> 4 jajajajaja eres un wn!                                        www.facebook.co‚Ä¶ #> 5 grax ntonces q?                                               www.youtube.com‚Ä¶ #> 6 yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£                   www.instagram.c‚Ä¶"},{"path":"/articles/limpiar_intro.html","id":"limpiar_tags","dir":"Articles","previous_headings":"limpiar_accents","what":"limpiar_tags","title":"Introduction to LimpiaR","text":"can also remove user handles (e.g. @magdalena) hashtags limpiar_tags function. Remember, can type ?limpiar_tags access documentation. Replace hashtags: Replace user tags: Replace hashtags user handles:","code":"data %>%   limpiar_tags(user = FALSE, hashtag = TRUE) #> # A tibble: 6 √ó 2 #>   mention_content                                            mention_url         #>   <chr>                                                      <chr>               #> 1 holaaaaaa! coomo estas @magdalena?!                        www.twitter.com/po‚Ä¶ #> 2 han visto este articulo!? que horror! no se puede!!        www.twitter.com/po‚Ä¶ #> 3 ayyyyyy a mi me gustaria ir a londres yaaa hashtag hashtag www.facebook.com/p‚Ä¶ #> 4 jajajajaja eres un wn!                                     www.facebook.com/p‚Ä¶ #> 5 grax ntonces q?                                            www.youtube.com/po‚Ä¶ #> 6 yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£                www.instagram.com/‚Ä¶ data %>%   limpiar_tags(user = TRUE, hashtag = FALSE) #> # A tibble: 6 √ó 2 #>   mention_content                                               mention_url      #>   <chr>                                                         <chr>            #> 1 holaaaaaa! coomo estas @user?!                                www.twitter.com‚Ä¶ #> 2 han visto este articulo!? que horror! no se puede!!           www.twitter.com‚Ä¶ #> 3 ayyyyyy a mi me gustaria ir a londres yaaa #llevame #porfavor www.facebook.co‚Ä¶ #> 4 jajajajaja eres un wn!                                        www.facebook.co‚Ä¶ #> 5 grax ntonces q?                                               www.youtube.com‚Ä¶ #> 6 yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£                   www.instagram.c‚Ä¶ data %>%   limpiar_tags() #> # A tibble: 6 √ó 2 #>   mention_content                                            mention_url         #>   <chr>                                                      <chr>               #> 1 holaaaaaa! coomo estas @user?!                             www.twitter.com/po‚Ä¶ #> 2 han visto este articulo!? que horror! no se puede!!        www.twitter.com/po‚Ä¶ #> 3 ayyyyyy a mi me gustaria ir a londres yaaa hashtag hashtag www.facebook.com/p‚Ä¶ #> 4 jajajajaja eres un wn!                                     www.facebook.com/p‚Ä¶ #> 5 grax ntonces q?                                            www.youtube.com/po‚Ä¶ #> 6 yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£                www.instagram.com/‚Ä¶"},{"path":"/articles/limpiar_intro.html","id":"quick-recap---weve-looked-at","dir":"Articles","previous_headings":"limpiar_accents","what":"Quick recap - we‚Äôve looked at:","title":"Introduction to LimpiaR","text":"cleaning column names janitor::clean_names() making text variable lower case mutate() & tolower() cleaning accents limpiar_accents() cleaning duplicate posts limpiar_duplicates() cleaning retweets limpiar_retweets() cleaning urls limpiar_url() cleaning spaces limpiar_spaces() cleaning user handles hashtags limpiar_tags()","code":""},{"path":"/articles/limpiar_intro.html","id":"limpiar_shorthands","dir":"Articles","previous_headings":"limpiar_accents","what":"limpiar_shorthands","title":"Introduction to LimpiaR","text":"One biggest problems messy data encounter, shorthands. Generally, algorithms trained clean, standard language, encounter shorthands abbreviations. Shorthands also change time, making impractical continuously train algorithms new shorthands arise. function attempts bridge gap, normalising common shorthands.","code":"(data <- data %>%    limpiar_shorthands()) #> # A tibble: 6 √ó 2 #>   mention_content                                               mention_url      #>   <chr>                                                         <chr>            #> 1 holaaaaaa! coomo estas @magdalena?!                           www.twitter.com‚Ä¶ #> 2 han visto este articulo!? que horror! no se puede!!           www.twitter.com‚Ä¶ #> 3 ayyyyyy a mi me gustaria ir a londres yaaa #llevame #porfavor www.facebook.co‚Ä¶ #> 4 jajajajaja eres un wuevon!                                    www.facebook.co‚Ä¶ #> 5 gracias entonces que?                                         www.youtube.com‚Ä¶ #> 6 yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£                   www.instagram.c‚Ä¶"},{"path":"/articles/limpiar_intro.html","id":"limpiar_repeated_chars","dir":"Articles","previous_headings":"limpiar_accents","what":"limpiar_repeated_chars","title":"Introduction to LimpiaR","text":"don‚Äôt want algorithm learn difference ‚Äòajajaj‚Äô ‚Äòjaja‚Äô ‚Äòay‚Äô ‚Äòayyyy‚Äô practically speaking, none. also don‚Äôt want introduce unnecessary tokens, normalise common occurrences repeated additional characters. Generally, steps ‚Äôve taken far used every analysis/project help clean data. now look circumstantial functions, .e.¬†used every analysis.","code":"(data <- data %>%    limpiar_repeat_chars()) #> # A tibble: 6 √ó 2 #>   mention_content                                        mention_url             #>   <chr>                                                  <chr>                   #> 1 hola! coomo estas @magdalena?!                         www.twitter.com/post1   #> 2 han visto este articulo!? que horror! no se puede!!    www.twitter.com/post2   #> 3 ay a mi me gustaria ir a londres ya #llevame #porfavor www.facebook.com/post1  #> 4 jaja eres un wuevon!                                   www.facebook.com/post2  #> 5 gracias entonces que?                                  www.youtube.com/post1   #> 6 yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£            www.instagram.com/post1"},{"path":"/articles/limpiar_intro.html","id":"limpiar_emojis","dir":"Articles","previous_headings":"limpiar_accents","what":"limpiar_emojis","title":"Introduction to LimpiaR","text":"don‚Äôt need use limpiar_emojis every analysis, many ParseR & SegmentR functions ignore implicitly. However, know need special purpose, can can use limpiar_emojis(). One problem , reason special cases , emoji‚Äôs encodings English. may, point, translate Spanish, seems unlikely. default settings:","code":"data %>%   limpiar_emojis(text_var = mention_content, with_emoji_tag = FALSE) #> # A tibble: 6 √ó 2 #>   mention_content                                                    mention_url #>   <chr>                                                              <chr>       #> 1 hola! coomo estas @magdalena?!                                     www.twitte‚Ä¶ #> 2 han visto este articulo!? que horror! no se puede!!                www.twitte‚Ä¶ #> 3 ay a mi me gustaria ir a londres ya #llevame #porfavor             www.facebo‚Ä¶ #> 4 jaja eres un wuevon!                                               www.facebo‚Ä¶ #> 5 gracias entonces que?                                              www.youtub‚Ä¶ #> 6 yo soy el mejor face with tears of joy face with tears of joy fac‚Ä¶ www.instag‚Ä¶ data %>%   limpiar_emojis() #> # A tibble: 6 √ó 2 #>   mention_content                                                    mention_url #>   <chr>                                                              <chr>       #> 1 hola! coomo estas @magdalena?!                                     www.twitte‚Ä¶ #> 2 han visto este articulo!? que horror! no se puede!!                www.twitte‚Ä¶ #> 3 ay a mi me gustaria ir a londres ya #llevame #porfavor             www.facebo‚Ä¶ #> 4 jaja eres un wuevon!                                               www.facebo‚Ä¶ #> 5 gracias entonces que?                                              www.youtub‚Ä¶ #> 6 yo soy el mejor face with tears of joy face with tears of joy fac‚Ä¶ www.instag‚Ä¶"},{"path":"/articles/limpiar_intro.html","id":"limpiar_stopwords","dir":"Articles","previous_headings":"limpiar_accents","what":"limpiar_stopwords","title":"Introduction to LimpiaR","text":"Stop words common words provide us much information utterance‚Äôs meaning. example, sentence: ‚Äòman prison theft‚Äô, knew one word sentence, word ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò‚Äô wouldn‚Äôt much idea sentence . However, ‚Äòprison‚Äô ‚Äòtheft‚Äô, give us lot information. many analyses, remove stop words help us see ‚Äòhighest information‚Äô words, get high-level understanding large bodies texts (topic modelling bigram networks.) virtually scenarios, want use limpiar_stopwords() argument stop_words = ‚Äútopics‚Äù like : Sentences can look quite strange without stopwords, lot social posts virtually meaningless. ‚Äôs also worth pointing , lot information can lost removing stop words. Many phrases English Spanish different meanings stop word removed.","code":"data %>%   limpiar_stopwords(stop_words = \"topics\") %>%   limpiar_spaces() #to clear the spaces of words that were removed #> # A tibble: 6 √ó 2 #>   mention_content                        mention_url             #>   <chr>                                  <chr>                   #> 1 hola! coomo @magdalena?!               www.twitter.com/post1   #> 2 visto articulo!? horror!!!             www.twitter.com/post2   #> 3 ay gustaria londres #llevame #porfavor www.facebook.com/post1  #> 4 jaja wuevon!                           www.facebook.com/post2  #> 5 gracias?                               www.youtube.com/post1   #> 6 üòÇüòÇüòÇ,!! ü§£ü§£                         www.instagram.com/post1"},{"path":"/articles/limpiar_intro.html","id":"utility-functions","dir":"Articles","previous_headings":"","what":"Utility Functions","title":"Introduction to LimpiaR","text":"nearly end introduction LimpiaR, finish, let‚Äôs look two utility functions may useful. ‚Äôve conjoured new data frame called df, use show last two functions chain everything together.","code":"df #> # A tibble: 10 √ó 3 #>    mention_content                                            mention_url na_col #>    <chr>                                                      <chr>       <chr>  #>  1 \"holaaaaaa! c√≥√≥mo    est√°s @magdalena   ?!\"                www.twitte‚Ä¶ NA     #>  2 \"  han visto este articulo!? Que horror! https://guardian‚Ä¶ www.twitte‚Ä¶ NA     #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #p‚Ä¶ www.facebo‚Ä¶ NA     #>  4 \"jajajajaja eres un wn!\"                                   www.facebo‚Ä¶ NA     #>  5 \"RT dale un click a ver una mujer baila con su perro\"      www.twitte‚Ä¶ NA     #>  6 \"grax ntonces q?\"                                          www.youtub‚Ä¶ NA     #>  7 \"yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£ \"             www.instag‚Ä¶ NA     #>  8 \"grax ntonces q?\"                                          www.youtub‚Ä¶ NA     #>  9 \"grax ntonces q?\"                                          www.youtub‚Ä¶ tadaa  #> 10 \"grax ntonces q?\"                                          www.youtub‚Ä¶ NA"},{"path":"/articles/limpiar_intro.html","id":"limpiar_inpsect","dir":"Articles","previous_headings":"Utility Functions","what":"limpiar_inpsect","title":"Introduction to LimpiaR","text":", imagine see strange pattern, want check ‚Äôs going specific pattern. can use limpiar_inspect view posts contain pattern interactive frame! Whilst ‚Äôs pretty obvious ‚Äògrax ntonces q?‚Äô posts exactly , real world ‚Äôre going 10,000 times many posts, searching suspicious patterns take lot time. vie","code":"limpiar_inspect(df, pattern = \"ntonces\", text_var = mention_content,                 url_var = mention_url, title = \"ntonces\") #> # A tibble: 10 √ó 3 #>    mention_content                                            mention_url na_col #>    <chr>                                                      <chr>       <chr>  #>  1 \"holaaaaaa! c√≥√≥mo    est√°s @magdalena   ?!\"                www.twitte‚Ä¶ NA     #>  2 \"  han visto este articulo!? Que horror! https://guardian‚Ä¶ www.twitte‚Ä¶ NA     #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #p‚Ä¶ www.facebo‚Ä¶ NA     #>  4 \"jajajajaja eres un wn!\"                                   www.facebo‚Ä¶ NA     #>  5 \"RT dale un click a ver una mujer baila con su perro\"      www.twitte‚Ä¶ NA     #>  6 \"grax ntonces q?\"                                          www.youtub‚Ä¶ NA     #>  7 \"yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£ \"             www.instag‚Ä¶ NA     #>  8 \"grax ntonces q?\"                                          www.youtub‚Ä¶ NA     #>  9 \"grax ntonces q?\"                                          www.youtub‚Ä¶ tadaa  #> 10 \"grax ntonces q?\"                                          www.youtub‚Ä¶ NA"},{"path":"/articles/limpiar_intro.html","id":"limpiar_na_cols","dir":"Articles","previous_headings":"Utility Functions","what":"limpiar_na_cols","title":"Introduction to LimpiaR","text":"final function useful want remove ‚Äòmostly NA‚Äô columns data frame. may want save memory, example 400,000 posts 80 columns. case ‚Äôll get rid columns 25% values NA.","code":"limpiar_na_cols(df,threshold =  0.25) #> # A tibble: 10 √ó 2 #>    mention_content                                                   mention_url #>    <chr>                                                             <chr>       #>  1 \"holaaaaaa! c√≥√≥mo    est√°s @magdalena   ?!\"                       www.twitte‚Ä¶ #>  2 \"  han visto este articulo!? Que horror! https://guardian.com/em‚Ä¶ www.twitte‚Ä¶ #>  3 \"ayyyyyy a mi me   gustaria ir a londres yaaa #llevame #porfavor\" www.facebo‚Ä¶ #>  4 \"jajajajaja eres un wn!\"                                          www.facebo‚Ä¶ #>  5 \"RT dale un click a ver una mujer baila con su perro\"             www.twitte‚Ä¶ #>  6 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #>  7 \"yo soy el mejor üòÇüòÇüòÇ, no eres nada!! ü§£ü§£ \"                    www.instag‚Ä¶ #>  8 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #>  9 \"grax ntonces q?\"                                                 www.youtub‚Ä¶ #> 10 \"grax ntonces q?\"                                                 www.youtub‚Ä¶"},{"path":"/articles/limpiar_intro.html","id":"putting-it-all-together","dir":"Articles","previous_headings":"","what":"Putting It All Together","title":"Introduction to LimpiaR","text":"speed things , can call functions together one big long pipe. However, large data frames, often better prioritise one, functions time, assigning data frame go.","code":"df %>%   limpiar_na_cols(threshold = 0.25)%>%   limpiar_accents()%>%   limpiar_retweets()%>%   limpiar_shorthands()%>%   limpiar_repeat_chars()%>%   limpiar_url()%>%   limpiar_emojis()%>%   limpiar_shorthands()%>%   limpiar_spaces()%>%   limpiar_duplicates() #> # A tibble: 6 √ó 2 #>   mention_content                                                    mention_url #>   <chr>                                                              <chr>       #> 1 hola! coomo estas @magdalena?!                                     www.twitte‚Ä¶ #> 2 han visto este articulo!? Que horror! NO SE PUEDE!!                www.twitte‚Ä¶ #> 3 ay a mi me gustaria ir a londres ya #llevame #porfavor             www.facebo‚Ä¶ #> 4 jaja eres un wuevon!                                               www.facebo‚Ä¶ #> 5 gracias entonces que?                                              www.youtub‚Ä¶ #> 6 yo soy el mejor face with tears of joy face with tears of joy fac‚Ä¶ www.instag‚Ä¶"},{"path":"/articles/parts_of_speech_workflow.html","id":"workflow","dir":"Articles","previous_headings":"","what":"Workflow","title":"LimpiaR - Parts of Speech Workflow","text":"main motivation processing extracting parts speech home particular aspects language informative answering given research question. example, want know people think something, might focus adjectives nouns adjectives describe things, nouns things. want know people using something, might focus verbs/phrasal verbs ‚Äòwords‚Äô.","code":""},{"path":"/articles/parts_of_speech_workflow.html","id":"loading-packages","dir":"Articles","previous_headings":"Workflow","what":"Loading packages","title":"LimpiaR - Parts of Speech Workflow","text":"","code":"library(LimpiaR) library(tibble) library(dplyr) library(stringr)"},{"path":"/articles/parts_of_speech_workflow.html","id":"getting-started","dir":"Articles","previous_headings":"Workflow","what":"Getting started","title":"LimpiaR - Parts of Speech Workflow","text":"LimpiaR‚Äôs PoS workflow leans pre-existing functionality {udpipe} package allow users import pre-trained model parts speech analysis whilst enabling advanced users perform ‚Äòdependency parsing‚Äô. ‚Äútechnique provides word sentence link another word sentence, called syntactical head. link 2 words furthermore certain type relationship giving details ‚Äù see info.","code":""},{"path":"/articles/parts_of_speech_workflow.html","id":"importing-a-udpipe-model","dir":"Articles","previous_headings":"Workflow","what":"Importing a udpipe model","title":"LimpiaR - Parts of Speech Workflow","text":"First, import model want use. UDPipe pre-trained models built upon Universal Dependencies treebanks made available 65 languages based 101 treebanks. demonstrative purposes ‚Äôll use language = \"english\".","code":"model <- limpiar_pos_import_model(language = \"english\")"},{"path":"/articles/parts_of_speech_workflow.html","id":"example-data","dir":"Articles","previous_headings":"Workflow","what":"Example data","title":"LimpiaR - Parts of Speech Workflow","text":"Now model imported session, let‚Äôs get data tag, tokenise, lemmatise, extract parts speech. need data frame text variable ID column uniquely identifies text. now data also desired model loaded session, function need next limpiar_pos_annotate(). begin, ‚Äôs important note output sensitive cleaning steps. example, possible removal punctuation, PoS Tagging may -perform. also said POS tagging process nouns(NOUN) proper-nouns(PROPN) harder differentiate punctuation removed well text lowercase. careful pre-processing text variable ‚Äôre intending follow PoS workflow.","code":"( data <-tibble(text = tolower(stringr::sentences[1:100]),                       universal_message_id = paste0(\"TWITTER\", 1:100)) ) ## # A tibble: 100 √ó 2 ##    text                                        universal_message_id ##    <chr>                                       <chr>                ##  1 the birch canoe slid on the smooth planks.  TWITTER1             ##  2 glue the sheet to the dark blue background. TWITTER2             ##  3 it's easy to tell the depth of a well.      TWITTER3             ##  4 these days a chicken leg is a rare dish.    TWITTER4             ##  5 rice is often served in round bowls.        TWITTER5             ##  6 the juice of lemons makes fine punch.       TWITTER6             ##  7 the box was thrown beside the parked truck. TWITTER7             ##  8 the hogs were fed chopped corn and garbage. TWITTER8             ##  9 four hours of steady work faced us.         TWITTER9             ## 10 a large size in stockings is hard to sell.  TWITTER10            ## # ‚Ñπ 90 more rows"},{"path":"/articles/parts_of_speech_workflow.html","id":"extracting-parts-of-speech","dir":"Articles","previous_headings":"Workflow","what":"Extracting parts of speech","title":"LimpiaR - Parts of Speech Workflow","text":"Thelimpiar_pos_annotate function converts text document tokens, lemmas, POS tags, dependency relationships*. input data frame one document per row, return data frame one row per token. means return data frame many rows input data frame!  limpiar_pos_annotate can sped using in_parallel = TRUE, argument makes function process multiple rows data frame parallel. Running function parallel prevents progress updated. default function print progress every 100 rows, way know ‚Äôs still working roughly long take finish running. don‚Äôt want progress updates print console set update_progress = 0. Now texts tokenized, dependencies parsed, POS annotations complete, let‚Äôs take look output. annotating 882 rows, 8.82x increase! Output additional columns, pay closest attention paragraph_id, sentence_id, token, lemma, pos_tag, dependency_tag. new columns? paragraph_id sentence_id tell us document‚Äôs paragraphs, sentence token . data set contains single sentences, values 1. token word rest columns refer . lemma lemmatised version token, example lemmatisation ‚Äòserved‚Äô converted ‚Äòserve‚Äô. pos_tag displays PoS labels ; Noun(NOUN), Proper Noun(PROPN), Pronoun(PRON), Verb(VERB), Adjective(ADJ) etc. exact tags mean see PoS Tags UPOS tags table top document. dependency_tag displays dependency relation token. Dependency relations significantly complicated parts speech. see Dependency Relations details. head_token_id, dependency_tag feats present select dependency_parse = TRUE, tell us id token (inside document, paragraph, sentence) token row dependency relation , dependency relation additional features relationship Finally, column inserted id_var =. variable help join results back original data frame.","code":"# annotate texts and perform dependency parsing annotations <- limpiar_pos_annotate(data = data, text_var = text, id_var = universal_message_id, pos_model = model, dependency_parse = TRUE, in_parallel = FALSE, update_progress = 25) ## 2024-01-17 18:40:40.406146 Annotating text fragment 1/100 ## 2024-01-17 18:40:40.472827 Annotating text fragment 26/100 ## 2024-01-17 18:40:40.534859 Annotating text fragment 51/100 ## 2024-01-17 18:40:40.599207 Annotating text fragment 76/100 annotations %>%   select(-c(sentence, feats, xpos, doc_id)) %>%   relocate(universal_message_id) ## # A tibble: 882 √ó 9 ##    universal_message_id paragraph_id sentence_id token_id token  lemma  pos_tag ##    <chr>                       <int>       <int> <chr>    <chr>  <chr>  <chr>   ##  1 TWITTER1                        1           1 1        the    the    DET     ##  2 TWITTER1                        1           1 2        birch  birch  NOUN    ##  3 TWITTER1                        1           1 3        canoe  canoe  NOUN    ##  4 TWITTER1                        1           1 4        slid   slid   NOUN    ##  5 TWITTER1                        1           1 5        on     on     ADP     ##  6 TWITTER1                        1           1 6        the    the    DET     ##  7 TWITTER1                        1           1 7        smooth smooth ADJ     ##  8 TWITTER1                        1           1 8        planks plank  NOUN    ##  9 TWITTER1                        1           1 9        .      .      PUNCT   ## 10 TWITTER2                        1           1 1        glue   glue   VERB    ## # ‚Ñπ 872 more rows ## # ‚Ñπ 2 more variables: head_token_id <chr>, dependency_tag <chr>"},{"path":"/articles/parts_of_speech_workflow.html","id":"manipulating-the-output","dir":"Articles","previous_headings":"Workflow","what":"Manipulating the output","title":"LimpiaR - Parts of Speech Workflow","text":"Now adequate context, let‚Äôs look can output. can combine commonly-used {dplyr} functions data wrangling summarisation filter certain parts speech count occurrences. Given size data set limited inferences can make counts, just want know adjectives seen frequently: Alternatively thing nouns, time count lemma (case plurals).","code":"# Count adjectives annotations %>%    filter(pos_tag %in% c(\"ADJ\")) %>%   count(token, pos_tag, sort = TRUE) ## # A tibble: 65 √ó 3 ##    token pos_tag     n ##    <chr> <chr>   <int> ##  1 blue  ADJ         3 ##  2 clear ADJ         3 ##  3 third ADJ         3 ##  4 large ADJ         2 ##  5 more  ADJ         2 ##  6 sharp ADJ         2 ##  7 small ADJ         2 ##  8 wide  ADJ         2 ##  9 young ADJ         2 ## 10 bent  ADJ         1 ## # ‚Ñπ 55 more rows #Count the lemma of each noun annotations %>%    filter(pos_tag %in% c(\"NOUN\")) %>% # this time selecting nouns   count(lemma, sort = TRUE) ## # A tibble: 214 √ó 2 ##    lemma     n ##    <chr> <int> ##  1 fence     3 ##  2 week      3 ##  3 air       2 ##  4 boy       2 ##  5 cat       2 ##  6 coat      2 ##  7 day       2 ##  8 fish      2 ##  9 girl      2 ## 10 grass     2 ## # ‚Ñπ 204 more rows"},{"path":"/articles/parts_of_speech_workflow.html","id":"adjective---noun-collocations","dir":"Articles","previous_headings":"Workflow > Manipulating the output","what":"Adjective - Noun collocations","title":"LimpiaR - Parts of Speech Workflow","text":"adjective -> noun pairs seen ? task gets slightly involved need look value token value next row, first need dplyr::filter dplyr::lag + dplyr::lead, group doc_id, paragraph_id sentence_id. ‚Äôll need perform {dplyr} magic count ADJ-NOUN collocations. data now set row 1+2, 3+4, 4+5 forth ADJ-NOUN collocations, can use information extract pairs combination floor division, exploiting cumsum meshes logicals R, ‚Äògrouped, summarise paste flourish collapse‚Äô trick. ADJ-NOUN collocation seen ‚Äòthird week‚Äô. larger data set expect find higher-frequency ADJ-NOUN collocations. However, manipulating data way, onus user know understand transformations created.","code":"adj_to_noun <- annotations %>%    filter(     pos_tag == \"ADJ\" & lead(pos_tag) == \"NOUN\" | pos_tag == \"NOUN\" & lag(pos_tag) == \"ADJ\", .by = c(doc_id, paragraph_id, sentence_id)) adj_to_noun %>%   mutate(collocation_id = row_number() %% 2, .before = 1) %>%    mutate(collocation_id = cumsum(collocation_id == 1)) %>%   summarise(collocation = paste0(token, collapse = \" \"), .by = collocation_id) %>%   count(collocation, sort = TRUE) ## # A tibble: 56 √ó 2 ##    collocation         n ##    <chr>           <int> ##  1 third week          2 ##  2 bent hook           1 ##  3 big task            1 ##  4 blue air            1 ##  5 blue background     1 ##  6 blue fish           1 ##  7 clear response      1 ##  8 clear spring        1 ##  9 clear water         1 ## 10 cloud hung          1 ## # ‚Ñπ 46 more rows"},{"path":"/articles/parts_of_speech_workflow.html","id":"repairing-the-sentence","dir":"Articles","previous_headings":"Workflow > Manipulating the output","what":"Repairing the sentence","title":"LimpiaR - Parts of Speech Workflow","text":"next example ‚Äôre going convert pos_tag == ADJ pos_tag == ‚ÄúNOUN‚Äù lemma, stitch sentences back together ‚Äôre ready visualisations. ‚Äôll make use case_when now-familiar grouped summarise paste trick. group universal_message_id ‚Äôll get back 100 row data frame can join back original data frame. information regarding co-occurrences nouns adjectives within documents, able evidence entities appear throughout data, well type language used conjunction toward . can later add , visualizing frequency terms appearing within document well explicitly used one another, just like example , see ‚Äòhot sun‚Äô appear frequent.","code":"annotations <- annotations %>%   mutate(token =            case_when(pos_tag == \"ADJ\" ~ lemma,                      pos_tag == \"NOUN\" ~ lemma,                      TRUE ~ lemma)) %>%   select(token, universal_message_id) %>%   summarise(sentence = paste0(token, collapse = \" \"),              .by = universal_message_id)"},{"path":"/articles/parts_of_speech_workflow.html","id":"other-languages---spanish","dir":"Articles","previous_headings":"","what":"Other languages - Spanish","title":"LimpiaR - Parts of Speech Workflow","text":"workflow Spanish similar English. ‚Äôll select language = ‚Äúspanish‚Äù ‚Äôll take look output Spanish document. ‚Äôll take paragraph El Pais store data frame named ‚Äòspanish‚Äô ‚ÄúHace muchos a√±os, cuando lleg√≥ La Habana, encontr√≥ el mejor acto de amor ‚Äîel estudio de la escritura, que es a√∫n mejor que el estudio de la vida‚Äî por parte de la joven Yalenis Velazco, quien dedic√≥ un intenso ensayo su obra.‚Äù see immediately ‚Äòhace‚Äô conjugation verb ‚Äòhacer‚Äô converted hacer, muchos -> mucho, a√±os -> a√±o. changes generally agreeable, ‚Äôll help us reduce overall number words data set, text-based visualisations cleaner. However, always good idea carefully consider effects transformation perform data, rather blindly. can convert verbs lemma stitch document back together using tricks English: Clearly sentence now ungrammatical, topic modelling workflow n-gram networks, data cleaner counts better reflect real frequencies verb. models 100% correct, expect see occasional false output. particularly true comes languages English tend less training data, grammatically complex.","code":"spanish_model <- limpiar_pos_import_model(\"spanish\") # Load the model   spanish <- spanish %>%   limpiar_pos_annotate(text, id, spanish_model, udpate_progress = 0) # Extract Pos Tags ## 2024-01-17 18:40:48.526593 Annotating text fragment 1/1 spanish %>%   select(token, lemma, pos_tag) # Select relevant columns ## # A tibble: 49 √ó 3 ##    token  lemma  pos_tag ##    <chr>  <chr>  <chr>   ##  1 Hace   hacer  VERB    ##  2 muchos mucho  DET     ##  3 a√±os   a√±o    NOUN    ##  4 ,      ,      PUNCT   ##  5 cuando cuando SCONJ   ##  6 lleg√≥  llegar VERB    ##  7 a      a      ADP     ##  8 La     el     DET     ##  9 Habana habana PROPN   ## 10 ,      ,      PUNCT   ## # ‚Ñπ 39 more rows spanish %>%   mutate(     token = ifelse(pos_tag == \"VERB\", lemma, token)   ) %>%   summarise(.by = id,             text = paste0(token, collapse = \" \")) ## # A tibble: 1 √ó 2 ##      id text                                                                     ##   <dbl> <chr>                                                                    ## 1     1 hacer muchos a√±os , cuando llegar a La Habana , encontrar el mejor acto‚Ä¶"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jack Penzer. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Penzer J (2024). LimpiaR: LimpiaR. R package version 0.0.0.9000.","code":"@Manual{,   title = {LimpiaR: LimpiaR},   author = {Jack Penzer},   year = {2024},   note = {R package version 0.0.0.9000}, }"},{"path":"/index.html","id":"what-is-limpiar","dir":"","previous_headings":"","what":"What is LimpiaR?","title":"LimpiaR","text":"LimpiaR R library functions cleaning & pre-processing text data. name comes ‚Äòlimpiar‚Äô Spanish verb‚Äôclean‚Äô. Generally calling LimpiaR function, can think ‚Äòclean‚Ä¶‚Äô. LimpiaR primarily used cleaning unstructured text data, comes social media reviews. initial release, focused around Spanish language, however, functions language-ambivalent.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"LimpiaR","text":"can install development version LimpiaR GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"jpcompartir/LimpiaR\")"},{"path":"/reference/code_browser_emojis.html","id":null,"dir":"Reference","previous_headings":"","what":"spanish_emojis_df ‚Äî code_browser_emojis","title":"spanish_emojis_df ‚Äî code_browser_emojis","text":"data frame columns emoji converting Spanish data frame columns emoji converting","code":""},{"path":"/reference/code_browser_emojis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"spanish_emojis_df ‚Äî code_browser_emojis","text":"","code":"data(\"spanish_emojis_df\")  data(\"code_browser_emojis\")"},{"path":"/reference/code_browser_emojis.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"spanish_emojis_df ‚Äî code_browser_emojis","text":"6 columns 1307 rows Message Data frame emoji conversions Spanish 3 columns 1853 rows Message Data frame emoji conversions","code":""},{"path":"/reference/entities.html","id":null,"dir":"Reference","previous_headings":"","what":"entities ‚Äî entities","title":"entities ‚Äî entities","text":"data frame entities converting removing","code":""},{"path":"/reference/entities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"entities ‚Äî entities","text":"","code":"data(\"entities\")"},{"path":"/reference/entities.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"entities ‚Äî entities","text":"4 columns 193 rows token regex pattern product replacement string replace n count entity original data word_count number words entity's string comprises","code":""},{"path":"/reference/limpiar_accents.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean accented characters ‚Äî limpiar_accents","title":"Clean accented characters ‚Äî limpiar_accents","text":"Useful reducing overall number tokens. Warning: removing accents results loss information, done care.","code":""},{"path":"/reference/limpiar_accents.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean accented characters ‚Äî limpiar_accents","text":"","code":"limpiar_accents(df, text_var = mention_content)"},{"path":"/reference/limpiar_accents.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean accented characters ‚Äî limpiar_accents","text":"df Name Data Frame Tibble object text_var Name text variable/character vector","code":""},{"path":"/reference/limpiar_accents.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean accented characters ‚Äî limpiar_accents","text":"Data Frame Tibble object accents text variable replaced","code":""},{"path":"/reference/limpiar_accents.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean accented characters ‚Äî limpiar_accents","text":"","code":"limpiar_examples %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaci√≥n\"                         #>  5 \"nos han metido en una muy dificil situaci√≥n\"                         #>  6 \"   Lo q no tenemos es tiempo.   Ma√±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan üò¢ üò¢ üò¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustar√≠a quedarme un ratito m√°s\"                             limpiar_examples %>% limpiar_accents() %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situacion\"                         #>  5 \"nos han metido en una muy dificil situacion\"                         #>  6 \"   Lo q no tenemos es tiempo.   Manyana    debemos luchar.   \"       #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan üò¢ üò¢ üò¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustaria quedarme un ratito mas\""},{"path":"/reference/limpiar_duplicates.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean the text variable of duplicate posts ‚Äî limpiar_duplicates","title":"Clean the text variable of duplicate posts ‚Äî limpiar_duplicates","text":"Removes duplicate posts, posts deleted protected APIs","code":""},{"path":"/reference/limpiar_duplicates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean the text variable of duplicate posts ‚Äî limpiar_duplicates","text":"","code":"limpiar_duplicates(data, text_var = mention_content)"},{"path":"/reference/limpiar_duplicates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean the text variable of duplicate posts ‚Äî limpiar_duplicates","text":"data Data Frame Tibble object text_var Name text variable/character vector","code":""},{"path":"/reference/limpiar_duplicates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean the text variable of duplicate posts ‚Äî limpiar_duplicates","text":"Data Frame Tibble object duplicate posts removed text variable","code":""},{"path":"/reference/limpiar_duplicates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean the text variable of duplicate posts ‚Äî limpiar_duplicates","text":"","code":"df <- data.frame(text_variable = cbind(c( \"Deleted or protected mention\", \"hello\", \"goodbye\", \"goodbye\"))) limpiar_duplicates(df, text_variable) #>   text_variable #> 1         hello #> 2       goodbye  limpiar_examples #> # A tibble: 10 √ó 5 #>    doc_id author_name       mention_content    mention_url platform_interactions #>     <int> <chr>             <chr>              <chr>       <lgl>                 #>  1      1 don_quijote       \"mi amigo sancho ‚Ä¶ www.twitte‚Ä¶ NA                    #>  2      2 sancho_panza      \"RT mi amigo sanc‚Ä¶ www.twitte‚Ä¶ NA                    #>  3      3 edmond_dantes     \"@don_quijote no ‚Ä¶ www.twitte‚Ä¶ NA                    #>  4      4 el_sordo          \"nos han metido e‚Ä¶ www.fakebo‚Ä¶ NA                    #>  5      5 commander_miranda \"nos han metido e‚Ä¶ www.fakebo‚Ä¶ NA                    #>  6      6 robert_jordan     \"   Lo q no tenem‚Ä¶ www.youtub‚Ä¶ NA                    #>  7      7 anselmo           \"a mi es muy grav‚Ä¶ www.twitte‚Ä¶ NA                    #>  8      8 maria             \"ayyy nooo @rober‚Ä¶ www.twitte‚Ä¶ NA                    #>  9      9 pablo             \"todos se unen a ‚Ä¶ www.instag‚Ä¶ NA                    #> 10     10 pilar             \"a mi me gustar√≠a‚Ä¶ www.instag‚Ä¶ NA                      limpiar_examples %>% limpiar_duplicates() #> # A tibble: 9 √ó 5 #>   doc_id author_name   mention_content         mention_url platform_interactions #>    <int> <chr>         <chr>                   <chr>       <lgl>                 #> 1      1 don_quijote   \"mi amigo sancho es un‚Ä¶ www.twitte‚Ä¶ NA                    #> 2      2 sancho_panza  \"RT mi amigo sancho es‚Ä¶ www.twitte‚Ä¶ NA                    #> 3      3 edmond_dantes \"@don_quijote no digas‚Ä¶ www.twitte‚Ä¶ NA                    #> 4      4 el_sordo      \"nos han metido en una‚Ä¶ www.fakebo‚Ä¶ NA                    #> 5      6 robert_jordan \"   Lo q no tenemos es‚Ä¶ www.youtub‚Ä¶ NA                    #> 6      7 anselmo       \"a mi es muy grave qui‚Ä¶ www.twitte‚Ä¶ NA                    #> 7      8 maria         \"ayyy nooo @robert_jor‚Ä¶ www.twitte‚Ä¶ NA                    #> 8      9 pablo         \"todos se unen a nuest‚Ä¶ www.instag‚Ä¶ NA                    #> 9     10 pilar         \"a mi me gustar√≠a qued‚Ä¶ www.instag‚Ä¶ NA"},{"path":"/reference/limpiar_emojis.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace emojis with a textual description ‚Äî limpiar_emojis","title":"Replace emojis with a textual description ‚Äî limpiar_emojis","text":"Main usage pre-processing text variable part Deep Learning pipeline. important argument whether add emoji tag, also print snake case.","code":""},{"path":"/reference/limpiar_emojis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace emojis with a textual description ‚Äî limpiar_emojis","text":"","code":"limpiar_emojis(df, text_var = mention_content, with_emoji_tag = FALSE)"},{"path":"/reference/limpiar_emojis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace emojis with a textual description ‚Äî limpiar_emojis","text":"df Name Data Frame Tibble Object text_var Name text variable with_emoji_tag Whether replace snakecase linked words ","code":""},{"path":"/reference/limpiar_emojis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Replace emojis with a textual description ‚Äî limpiar_emojis","text":"Data Frame Tibble object emojis cleaned text variable","code":""},{"path":"/reference/limpiar_emojis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Replace emojis with a textual description ‚Äî limpiar_emojis","text":"","code":"limpiar_examples %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaci√≥n\"                         #>  5 \"nos han metido en una muy dificil situaci√≥n\"                         #>  6 \"   Lo q no tenemos es tiempo.   Ma√±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan üò¢ üò¢ üò¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustar√≠a quedarme un ratito m√°s\"                              limpiar_examples %>% limpiar_emojis() %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                     #>    <chr>                                                               #>  1 mi amigo sancho es un wn de vdd jajaja                              #>  2 RT mi amigo sancho es un wn de vdd jajaja                           #>  3 @don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho #>  4 nos han metido en una muy dificil situaci√≥n                         #>  5 nos han metido en una muy dificil situaci√≥n                         #>  6 Lo q no tenemos es tiempo. Ma√±ana debemos luchar.                   #>  7 a mi es muy grave quitarle la vida al otro                          #>  8 ayyy nooo @robert_jordan crying face crying face crying face        #>  9 todos se unen a nuestro grupo hagale un clic https::larebelion.es   #> 10 a mi me gustar√≠a quedarme un ratito m√°s"},{"path":"/reference/limpiar_emojis_es.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace emojis with a Spanish textual description ‚Äî limpiar_emojis_es","title":"Replace emojis with a Spanish textual description ‚Äî limpiar_emojis_es","text":"Spanish version limpiar_emojis function. Main usage pre-processing text variable part Deep Learning pipeline. important argument whether add emoji tag, also print snake case.","code":""},{"path":"/reference/limpiar_emojis_es.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace emojis with a Spanish textual description ‚Äî limpiar_emojis_es","text":"","code":"limpiar_emojis_es(df, text_var = mention_content, with_emoji_tag = FALSE)"},{"path":"/reference/limpiar_emojis_es.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace emojis with a Spanish textual description ‚Äî limpiar_emojis_es","text":"df Name Data Frame Tibble Object text_var Name text variable with_emoji_tag Whether replace snakecase linked words ","code":""},{"path":"/reference/limpiar_emojis_es.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Replace emojis with a Spanish textual description ‚Äî limpiar_emojis_es","text":"Data Frame Tibble object emojis cleaned text variable","code":""},{"path":"/reference/limpiar_emojis_es.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Replace emojis with a Spanish textual description ‚Äî limpiar_emojis_es","text":"","code":"limpiar_examples %>% limpiar_emojis_es() %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                     #>    <chr>                                                               #>  1 mi amigo sancho es un wn de vdd jajaja                              #>  2 RT mi amigo sancho es un wn de vdd jajaja                           #>  3 @don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho #>  4 nos han metido en una muy dificil situaci√≥n                         #>  5 nos han metido en una muy dificil situaci√≥n                         #>  6 Lo q no tenemos es tiempo. Ma√±ana debemos luchar.                   #>  7 a mi es muy grave quitarle la vida al otro                          #>  8 ayyy nooo @robert_jordan cara llorosa cara llorosa cara llorosa     #>  9 todos se unen a nuestro grupo hagale un clic https::larebelion.es   #> 10 a mi me gustar√≠a quedarme un ratito m√°s"},{"path":"/reference/limpiar_ex_subreddits.html","id":null,"dir":"Reference","previous_headings":"","what":"Quickly extract subreddits from a link variable ‚Äî limpiar_ex_subreddits","title":"Quickly extract subreddits from a link variable ‚Äî limpiar_ex_subreddits","text":"Quickly extract subreddits link variable","code":""},{"path":"/reference/limpiar_ex_subreddits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quickly extract subreddits from a link variable ‚Äî limpiar_ex_subreddits","text":"","code":"limpiar_ex_subreddits(df, url_var = permalink)"},{"path":"/reference/limpiar_ex_subreddits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quickly extract subreddits from a link variable ‚Äî limpiar_ex_subreddits","text":"df Data Frame Tibble Object url_var variable containing URLS e.g. permalink","code":""},{"path":"/reference/limpiar_ex_subreddits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quickly extract subreddits from a link variable ‚Äî limpiar_ex_subreddits","text":"df additional column","code":""},{"path":"/reference/limpiar_examples.html","id":null,"dir":"Reference","previous_headings":"","what":"Tibble of examples for LimpiaR functions ‚Äî limpiar_examples","title":"Tibble of examples for LimpiaR functions ‚Äî limpiar_examples","text":"Tibble examples LimpiaR functions","code":""},{"path":"/reference/limpiar_examples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tibble of examples for LimpiaR functions ‚Äî limpiar_examples","text":"","code":"data(\"limpiar_examples\")"},{"path":"/reference/limpiar_examples.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Tibble of examples for LimpiaR functions ‚Äî limpiar_examples","text":"tibble 10 rows 5 columns doc_id Post ID author_name author's public username mention_content Text variable mention_url Link original post platform_interactions Number interactions parent platform","code":""},{"path":"/reference/limpiar_inspect.html","id":null,"dir":"Reference","previous_headings":"","what":"Inspect every post and URL which contains a pattern ‚Äî limpiar_inspect","title":"Inspect every post and URL which contains a pattern ‚Äî limpiar_inspect","text":"Produces viewable data frame posts matching regular expression url. Useful investigating suspected spam posts, patterns interest. Set name title avoid new frames overwriting old ones.","code":""},{"path":"/reference/limpiar_inspect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inspect every post and URL which contains a pattern ‚Äî limpiar_inspect","text":"","code":"limpiar_inspect(   data,   pattern,   text_var = mention_content,   url_var = mention_url,   title = \"inspect\" )"},{"path":"/reference/limpiar_inspect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inspect every post and URL which contains a pattern ‚Äî limpiar_inspect","text":"data Data frame tibble object pattern Pattern wish inspect e.g. \"link bio\" text_var Name text variable/character vector url_var Name data frame's URL-column title Name viewable pane","code":""},{"path":"/reference/limpiar_inspect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inspect every post and URL which contains a pattern ‚Äî limpiar_inspect","text":"","code":"df <- data.frame( text_variable = rbind(\"check me out\", \"don't look at me\"), text_url = rbind(\"www.twitter.com\", \"www.facebook.com\")) limpiar_inspect(df, \"check\", text_var = text_variable, url_var = text_url)"},{"path":"/reference/limpiar_link_click.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare a URL column to be clickable in Shiny/Data Table ‚Äî limpiar_link_click","title":"Prepare a URL column to be clickable in Shiny/Data Table ‚Äî limpiar_link_click","text":"allow click hyperlink load URL, e.g. selecting image.","code":""},{"path":"/reference/limpiar_link_click.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare a URL column to be clickable in Shiny/Data Table ‚Äî limpiar_link_click","text":"","code":"limpiar_link_click(df, url_var)"},{"path":"/reference/limpiar_link_click.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare a URL column to be clickable in Shiny/Data Table ‚Äî limpiar_link_click","text":"df Data Frame Tibble Object url_var URL Column","code":""},{"path":"/reference/limpiar_link_click.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare a URL column to be clickable in Shiny/Data Table ‚Äî limpiar_link_click","text":"data frame URL column edited clickable","code":""},{"path":"/reference/limpiar_link_click.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare a URL column to be clickable in Shiny/Data Table ‚Äî limpiar_link_click","text":"Make sure DataTable rendered argument 'escape = FALSE' column text. function now checks url_var clickable link, add new formatting.","code":""},{"path":[]},{"path":"/reference/limpiar_link_click_reverse.html","id":null,"dir":"Reference","previous_headings":"","what":"Reverses (inverts) limpiar_link_click ‚Äî limpiar_link_click_reverse","title":"Reverses (inverts) limpiar_link_click ‚Äî limpiar_link_click_reverse","text":"Undoes effects limpiar_link_click function, giving original url variable back.","code":""},{"path":"/reference/limpiar_link_click_reverse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reverses (inverts) limpiar_link_click ‚Äî limpiar_link_click_reverse","text":"","code":"limpiar_link_click_reverse(df, url_var)"},{"path":"/reference/limpiar_link_click_reverse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reverses (inverts) limpiar_link_click ‚Äî limpiar_link_click_reverse","text":"df Data Farame Tibble Object url_var URL Column","code":""},{"path":"/reference/limpiar_link_click_reverse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reverses (inverts) limpiar_link_click ‚Äî limpiar_link_click_reverse","text":"Data frame url_var original form","code":""},{"path":"/reference/limpiar_link_click_reverse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reverses (inverts) limpiar_link_click ‚Äî limpiar_link_click_reverse","text":"","code":"df <- LimpiaR::limpiar_examples[1, ]  df <- df %>% limpiar_link_click(mention_url) df$mention_url #> [1] \"<a href='www.twitter.com/ejemplo/124864' target='blank'>Click to View<\/a>\" df %>% limpiar_link_click_reverse(mention_url) #> # A tibble: 1 √ó 5 #>   doc_id author_name mention_content           mention_url platform_interactions #>    <int> <chr>       <chr>                     <chr>       <lgl>                 #> 1      1 don_quijote mi amigo sancho es un wn‚Ä¶ www.twitte‚Ä¶ NA"},{"path":"/reference/limpiar_na_cols.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean NA-heavy columns from a Data Frame or Tibble ‚Äî limpiar_na_cols","title":"Clean NA-heavy columns from a Data Frame or Tibble ‚Äî limpiar_na_cols","text":"Remove columns whose proportion NAs higher determined threshold. Setting threshold 0.25 asks R remove columns 25% NA values. Can useful dealing large data frames, many columns redundant.","code":""},{"path":"/reference/limpiar_na_cols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean NA-heavy columns from a Data Frame or Tibble ‚Äî limpiar_na_cols","text":"","code":"limpiar_na_cols(df, threshold)"},{"path":"/reference/limpiar_na_cols.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean NA-heavy columns from a Data Frame or Tibble ‚Äî limpiar_na_cols","text":"df Data Frame Tibble object threshold Threshold non-NA entries column must exceed retained.","code":""},{"path":"/reference/limpiar_na_cols.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean NA-heavy columns from a Data Frame or Tibble ‚Äî limpiar_na_cols","text":"Data Frame Tibble NA-heavy columns purged","code":""},{"path":"/reference/limpiar_na_cols.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean NA-heavy columns from a Data Frame or Tibble ‚Äî limpiar_na_cols","text":"","code":"limpiar_examples #> # A tibble: 10 √ó 5 #>    doc_id author_name       mention_content    mention_url platform_interactions #>     <int> <chr>             <chr>              <chr>       <lgl>                 #>  1      1 don_quijote       \"mi amigo sancho ‚Ä¶ www.twitte‚Ä¶ NA                    #>  2      2 sancho_panza      \"RT mi amigo sanc‚Ä¶ www.twitte‚Ä¶ NA                    #>  3      3 edmond_dantes     \"@don_quijote no ‚Ä¶ www.twitte‚Ä¶ NA                    #>  4      4 el_sordo          \"nos han metido e‚Ä¶ www.fakebo‚Ä¶ NA                    #>  5      5 commander_miranda \"nos han metido e‚Ä¶ www.fakebo‚Ä¶ NA                    #>  6      6 robert_jordan     \"   Lo q no tenem‚Ä¶ www.youtub‚Ä¶ NA                    #>  7      7 anselmo           \"a mi es muy grav‚Ä¶ www.twitte‚Ä¶ NA                    #>  8      8 maria             \"ayyy nooo @rober‚Ä¶ www.twitte‚Ä¶ NA                    #>  9      9 pablo             \"todos se unen a ‚Ä¶ www.instag‚Ä¶ NA                    #> 10     10 pilar             \"a mi me gustar√≠a‚Ä¶ www.instag‚Ä¶ NA                     limpiar_examples %>% limpiar_na_cols(0.1) #> # A tibble: 10 √ó 4 #>    doc_id author_name       mention_content                          mention_url #>     <int> <chr>             <chr>                                    <chr>       #>  1      1 don_quijote       \"mi amigo sancho es un wn de vdd jajaja\" www.twitte‚Ä¶ #>  2      2 sancho_panza      \"RT mi amigo sancho es un wn de vdd jaj‚Ä¶ www.twitte‚Ä¶ #>  3      3 edmond_dantes     \"@don_quijote no digas eso, tu amigo es‚Ä¶ www.twitte‚Ä¶ #>  4      4 el_sordo          \"nos han metido en una muy dificil situ‚Ä¶ www.fakebo‚Ä¶ #>  5      5 commander_miranda \"nos han metido en una muy dificil situ‚Ä¶ www.fakebo‚Ä¶ #>  6      6 robert_jordan     \"   Lo q no tenemos es tiempo.   Ma√±ana‚Ä¶ www.youtub‚Ä¶ #>  7      7 anselmo           \"a mi es muy grave quitarle la vida al ‚Ä¶ www.twitte‚Ä¶ #>  8      8 maria             \"ayyy nooo @robert_jordan üò¢ üò¢ üò¢ \"     www.twitte‚Ä¶ #>  9      9 pablo             \"todos se unen a nuestro grupo hagale u‚Ä¶ www.instag‚Ä¶ #> 10     10 pilar             \"a mi me gustar√≠a quedarme un ratito m√°‚Ä¶ www.instag‚Ä¶"},{"path":"/reference/limpiar_pos_annotate.html","id":null,"dir":"Reference","previous_headings":"","what":"Annotate Texts for Parts of Speech Analysis using udpipe models. ‚Äî limpiar_pos_annotate","title":"Annotate Texts for Parts of Speech Analysis using udpipe models. ‚Äî limpiar_pos_annotate","text":"Take data frame text id variable extract parts speech using udpipe models import specific language limpiar_pos_import_model. function annotate data frame according language imported model.","code":""},{"path":"/reference/limpiar_pos_annotate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Annotate Texts for Parts of Speech Analysis using udpipe models. ‚Äî limpiar_pos_annotate","text":"","code":"limpiar_pos_annotate(   data,   text_var,   id_var,   pos_model,   ...,   in_parallel = FALSE,   dependency_parse = FALSE,   update_progress = 100 )"},{"path":"/reference/limpiar_pos_annotate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Annotate Texts for Parts of Speech Analysis using udpipe models. ‚Äî limpiar_pos_annotate","text":"data data.frame tibble object containing texts user wishes conduct parts speech analysis . text_var texts sentences user wishes perform parts speech annotations . id_var Unique identifier document. default supplied. recommended use 'universal_message_id' using social listening export. pos_model UDPipe model imported using limpiar_pos_import_model - must class 'udpipe_model'. ... enable user supply additional arguments udpipe::udpipe. in_parallel logical argument allowing user initiate parallel processing speed annotate function . set TRUE,  function select number available cores minus one, processing efficiently(faster), leaving one core manage computations. default FALSE. dependency_parse Whether perform dependency parsing tokens. default set FALSE parsing dependencies takes considerable time always needed. update_progress user option state often like progress report annotation process, posted console stating whether want message every 100, 500 1000 documents. useful annotating large sets data serves sanity check ensure session used available memory annotations stopped running.","code":""},{"path":"/reference/limpiar_pos_annotate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Annotate Texts for Parts of Speech Analysis using udpipe models. ‚Äî limpiar_pos_annotate","text":"Returns data frame documents broken token sentence level, addition existing variables present data supplied function. returned object contains parts speech annotations CONLL-U formatting, row annotation word. find formatting methods, read . additional arguments tagged POS information follows: paragraph_id: identifier indicating paragraph annotated token derived . sentence_id: Similar paragraph_id sentence level. sentence: sentence annotated token derived . token_id: Token index, integer starting 1 new sentence. May range multiword tokens decimal number empty nodes. token: token(word) annotated parts speech. lemma: lemmatized version annotated token. pos_tag: universal parts speech tag token. information. xpos: treebank-specific parts speech tag token. feats: morphological features token, used dependency parsing visualisations,. head_token_id: Indicating token id head token, indicating token sentence related. dependency_tag: information regarding dependency parsing tokens, displaying type relation token head_token_id. information.","code":""},{"path":"/reference/limpiar_pos_annotate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Annotate Texts for Parts of Speech Analysis using udpipe models. ‚Äî limpiar_pos_annotate","text":"leg work done udpipe. implemented LimpiaR : Taking data first argument allows integrate Tidyerse workflows (makes function pipe-able) want mental model consistent possible, .e. using LimpiaR pre-processing pipeline user mainly calls LimpiaR, remember another package. many potential workflows try enumerate . However,  clear use-cases emerge create new limpiar_pos_ functions case--case basis. example workflow convert adjectives nouns lemma visualise results.","code":""},{"path":"/reference/limpiar_pos_annotate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Annotate Texts for Parts of Speech Analysis using udpipe models. ‚Äî limpiar_pos_annotate","text":"","code":"data <- dplyr::tibble(text = tolower(stringr::sentences[1:100]), document = 1:100) model <- LimpiaR::limpiar_pos_import_model(language = \"english\") #> Downloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/english-ewt-ud-2.5-191206.udpipe to /private/var/folders/fv/v1qhddlx1wxdm33yr3yck7v80000gn/T/RtmpixJfeZ/temp_libpath1129377634764/LimpiaR/model_cache/english-ewt-ud-2.5-191206.udpipe #>  - This model has been trained on version 2.5 of data from https://universaldependencies.org #>  - The model is distributed under the CC-BY-SA-NC license: https://creativecommons.org/licenses/by-nc-sa/4.0 #>  - Visit https://github.com/jwijffels/udpipe.models.ud.2.5 for model license details. #>  - For a list of all models and their licenses (most models you can download with this package have either a CC-BY-SA or a CC-BY-SA-NC license) read the documentation at ?udpipe_download_model. For building your own models: visit the documentation by typing vignette('udpipe-train', package = 'udpipe') #> Downloading finished, model stored at '/private/var/folders/fv/v1qhddlx1wxdm33yr3yck7v80000gn/T/RtmpixJfeZ/temp_libpath1129377634764/LimpiaR/model_cache/english-ewt-ud-2.5-191206.udpipe' annotations <- limpiar_pos_annotate(data = data,                                    text_var = text,                                    id_var = document,                                    pos_model = model,                                    in_parallel = FALSE,                                    dependency_parse = TRUE,                                    progress = \"100\") #> 2024-02-22 10:43:13.197644 Annotating text fragment 1/100"},{"path":"/reference/limpiar_pos_import_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Import UDPipe models to begin Parts of Speech Analysis ‚Äî limpiar_pos_import_model","title":"Import UDPipe models to begin Parts of Speech Analysis ‚Äî limpiar_pos_import_model","text":"function retrieves downloads pre-built models made UDPipe community, covering 65 different languages. information models available, visit:UDPipe Documentation","code":""},{"path":"/reference/limpiar_pos_import_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Import UDPipe models to begin Parts of Speech Analysis ‚Äî limpiar_pos_import_model","text":"","code":"limpiar_pos_import_model(language)"},{"path":"/reference/limpiar_pos_import_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Import UDPipe models to begin Parts of Speech Analysis ‚Äî limpiar_pos_import_model","text":"language chosen language user wishes select. 65 options choose ","code":""},{"path":"/reference/limpiar_pos_import_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Import UDPipe models to begin Parts of Speech Analysis ‚Äî limpiar_pos_import_model","text":"Loads model memory, ready annotation steps parts speech workflow.","code":""},{"path":"/reference/limpiar_pos_import_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Import UDPipe models to begin Parts of Speech Analysis ‚Äî limpiar_pos_import_model","text":"","code":"pos_model <- limpiar_pos_import_model(language = \"english\")"},{"path":"/reference/limpiar_pp_companies.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove known companies for pits & peaks ‚Äî limpiar_pp_companies","title":"Remove known companies for pits & peaks ‚Äî limpiar_pp_companies","text":"Remove known companies pits & peaks","code":""},{"path":"/reference/limpiar_pp_companies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove known companies for pits & peaks ‚Äî limpiar_pp_companies","text":"","code":"limpiar_pp_companies(df, text_var)"},{"path":"/reference/limpiar_pp_companies.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove known companies for pits & peaks ‚Äî limpiar_pp_companies","text":"df Data Frame Tibble object text_var Name text variable","code":""},{"path":"/reference/limpiar_pp_companies.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove known companies for pits & peaks ‚Äî limpiar_pp_companies","text":"Data Frame Tibble object text variable edited inline","code":""},{"path":[]},{"path":"/reference/limpiar_pp_products.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace entities for the Peaks&Pit classifier ‚Äî limpiar_pp_products","title":"Replace entities for the Peaks&Pit classifier ‚Äî limpiar_pp_products","text":"Replace entities Peaks&Pit classifier","code":""},{"path":"/reference/limpiar_pp_products.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace entities for the Peaks&Pit classifier ‚Äî limpiar_pp_products","text":"","code":"limpiar_pp_products(df, text_var)"},{"path":"/reference/limpiar_pp_products.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace entities for the Peaks&Pit classifier ‚Äî limpiar_pp_products","text":"df Data Frame Tibble object text_var Name text variable","code":""},{"path":"/reference/limpiar_pp_products.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Replace entities for the Peaks&Pit classifier ‚Äî limpiar_pp_products","text":"Data Frame Tibble object text variable edited inline","code":""},{"path":[]},{"path":"/reference/limpiar_repeat_chars.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean repeated charaaaacters ‚Äî limpiar_repeat_chars","title":"Clean repeated charaaaacters ‚Äî limpiar_repeat_chars","text":"Removes multiple vowels (holaaaa) normalises common laughing patterns (jajaja, jejeje, ajajaaaaja). Useful visualisations, reducing overall number tokens present text variable.","code":""},{"path":"/reference/limpiar_repeat_chars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean repeated charaaaacters ‚Äî limpiar_repeat_chars","text":"","code":"limpiar_repeat_chars(df, text_var = mention_content)"},{"path":"/reference/limpiar_repeat_chars.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean repeated charaaaacters ‚Äî limpiar_repeat_chars","text":"df Name Data Frame Tibble object text_var Name text variable/character vector. Default mention_content","code":""},{"path":"/reference/limpiar_repeat_chars.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean repeated charaaaacters ‚Äî limpiar_repeat_chars","text":"Data Frame Tibble object repeat vowels & laughing patterns removed text variable","code":""},{"path":"/reference/limpiar_repeat_chars.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean repeated charaaaacters ‚Äî limpiar_repeat_chars","text":"","code":"limpiar_examples %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaci√≥n\"                         #>  5 \"nos han metido en una muy dificil situaci√≥n\"                         #>  6 \"   Lo q no tenemos es tiempo.   Ma√±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan üò¢ üò¢ üò¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustar√≠a quedarme un ratito m√°s\"                              limpiar_examples %>% limpiar_repeat_chars() %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jaja\"                                #>  2 \"RT mi amigo sancho es un wn de vdd jaja\"                             #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaci√≥n\"                         #>  5 \"nos han metido en una muy dificil situaci√≥n\"                         #>  6 \"   Lo q no tenemos es tiempo.   Ma√±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ay no @robert_jordan üò¢ üò¢ üò¢ \"                                      #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustar√≠a quedarme un ratito m√°s\""},{"path":"/reference/limpiar_retweets.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean retweets from the text variable ‚Äî limpiar_retweets","title":"Clean retweets from the text variable ‚Äî limpiar_retweets","text":"Removes posts 'rt' 'RT' tag. Particularly effective used conjunction ParseR & SegmentR visualisations.","code":""},{"path":"/reference/limpiar_retweets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean retweets from the text variable ‚Äî limpiar_retweets","text":"","code":"limpiar_retweets(df, text_var = mention_content)"},{"path":"/reference/limpiar_retweets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean retweets from the text variable ‚Äî limpiar_retweets","text":"df Name Data Frame Tibble Object text_var Name text variable/character vector","code":""},{"path":"/reference/limpiar_retweets.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean retweets from the text variable ‚Äî limpiar_retweets","text":"","code":"df <- data.frame(text_variable = cbind(c(\"rt <3\", \"RT <3\", \"original tweet\"))) limpiar_retweets(df, text_variable) #>    text_variable #> 1 original tweet  limpiar_examples #> # A tibble: 10 √ó 5 #>    doc_id author_name       mention_content    mention_url platform_interactions #>     <int> <chr>             <chr>              <chr>       <lgl>                 #>  1      1 don_quijote       \"mi amigo sancho ‚Ä¶ www.twitte‚Ä¶ NA                    #>  2      2 sancho_panza      \"RT mi amigo sanc‚Ä¶ www.twitte‚Ä¶ NA                    #>  3      3 edmond_dantes     \"@don_quijote no ‚Ä¶ www.twitte‚Ä¶ NA                    #>  4      4 el_sordo          \"nos han metido e‚Ä¶ www.fakebo‚Ä¶ NA                    #>  5      5 commander_miranda \"nos han metido e‚Ä¶ www.fakebo‚Ä¶ NA                    #>  6      6 robert_jordan     \"   Lo q no tenem‚Ä¶ www.youtub‚Ä¶ NA                    #>  7      7 anselmo           \"a mi es muy grav‚Ä¶ www.twitte‚Ä¶ NA                    #>  8      8 maria             \"ayyy nooo @rober‚Ä¶ www.twitte‚Ä¶ NA                    #>  9      9 pablo             \"todos se unen a ‚Ä¶ www.instag‚Ä¶ NA                    #> 10     10 pilar             \"a mi me gustar√≠a‚Ä¶ www.instag‚Ä¶ NA                     limpiar_examples %>% limpiar_retweets() #> # A tibble: 9 √ó 5 #>   doc_id author_name       mention_content     mention_url platform_interactions #>    <int> <chr>             <chr>               <chr>       <lgl>                 #> 1      1 don_quijote       \"mi amigo sancho e‚Ä¶ www.twitte‚Ä¶ NA                    #> 2      3 edmond_dantes     \"@don_quijote no d‚Ä¶ www.twitte‚Ä¶ NA                    #> 3      4 el_sordo          \"nos han metido en‚Ä¶ www.fakebo‚Ä¶ NA                    #> 4      5 commander_miranda \"nos han metido en‚Ä¶ www.fakebo‚Ä¶ NA                    #> 5      6 robert_jordan     \"   Lo q no tenemo‚Ä¶ www.youtub‚Ä¶ NA                    #> 6      7 anselmo           \"a mi es muy grave‚Ä¶ www.twitte‚Ä¶ NA                    #> 7      8 maria             \"ayyy nooo @robert‚Ä¶ www.twitte‚Ä¶ NA                    #> 8      9 pablo             \"todos se unen a n‚Ä¶ www.instag‚Ä¶ NA                    #> 9     10 pilar             \"a mi me gustar√≠a ‚Ä¶ www.instag‚Ä¶ NA"},{"path":"/reference/limpiar_shorthands.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean shorthands and abbreviations ‚Äî limpiar_shorthands","title":"Clean shorthands and abbreviations ‚Äî limpiar_shorthands","text":"Replaces common Spanish shorthands abbreviations longer form equivalents. Choose whether link replacements snake case , spaces_as_underscores. Useful primarily normalising text ahead sentiment classification.","code":""},{"path":"/reference/limpiar_shorthands.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean shorthands and abbreviations ‚Äî limpiar_shorthands","text":"","code":"limpiar_shorthands(   df,   text_var = mention_content,   spaces_as_underscores = FALSE )"},{"path":"/reference/limpiar_shorthands.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean shorthands and abbreviations ‚Äî limpiar_shorthands","text":"df Name Data Frame Tibble object text_var Name text variable/character vector spaces_as_underscores Whether multi-word corrections e.g. 'te quiero mucho' spaces underscores. Default = FALSE","code":""},{"path":"/reference/limpiar_shorthands.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean shorthands and abbreviations ‚Äî limpiar_shorthands","text":"text variable shorthands replaced","code":""},{"path":"/reference/limpiar_shorthands.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean shorthands and abbreviations ‚Äî limpiar_shorthands","text":"","code":"limpiar_examples %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaci√≥n\"                         #>  5 \"nos han metido en una muy dificil situaci√≥n\"                         #>  6 \"   Lo q no tenemos es tiempo.   Ma√±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan üò¢ üò¢ üò¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustar√≠a quedarme un ratito m√°s\"                              limpiar_examples %>% limpiar_shorthands() %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wuevon de verdad jajaja\"                       #>  2 \"RT mi amigo sancho es un wuevon de verdad jajaja\"                    #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaci√≥n\"                         #>  5 \"nos han metido en una muy dificil situaci√≥n\"                         #>  6 \"   Lo que no tenemos es tiempo.   Ma√±ana    debemos luchar.   \"      #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan üò¢ üò¢ üò¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustar√≠a quedarme un ratito m√°s\""},{"path":"/reference/limpiar_slang.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean slang from multiple Spanish dialects ‚Äî limpiar_slang","title":"Clean slang from multiple Spanish dialects ‚Äî limpiar_slang","text":"Replaces slang phrases various Spanish dialects everyday terms. Function's primary use normalise text Deep Learning sentiment algorithm. Care taken using function, e.g. panda -> grupo, far common usage texts use. However, data set many people talk panda bears 'oso panda', unwanted changes. tried avoid problem possible, including things like 'la suda' instead changing 'suda'.","code":""},{"path":"/reference/limpiar_slang.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean slang from multiple Spanish dialects ‚Äî limpiar_slang","text":"","code":"limpiar_slang(df, text_var = mention_content)"},{"path":"/reference/limpiar_slang.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean slang from multiple Spanish dialects ‚Äî limpiar_slang","text":"df Name Data Frame Tibble object text_var Name text variable/character vector","code":""},{"path":"/reference/limpiar_slang.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean slang from multiple Spanish dialects ‚Äî limpiar_slang","text":"Data Frame Tibble object text variable altered","code":""},{"path":"/reference/limpiar_slang.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean slang from multiple Spanish dialects ‚Äî limpiar_slang","text":"","code":"if (FALSE) { df %>% limpiar_slang(text_var = text_var)}"},{"path":"/reference/limpiar_spaces.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean redundant spaces ‚Äî limpiar_spaces","title":"Clean redundant spaces ‚Äî limpiar_spaces","text":"Remove excess spaces text variable.","code":""},{"path":"/reference/limpiar_spaces.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean redundant spaces ‚Äî limpiar_spaces","text":"","code":"limpiar_spaces(df, text_var = mention_content)"},{"path":"/reference/limpiar_spaces.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean redundant spaces ‚Äî limpiar_spaces","text":"df Name Data Frame Tibble object text_var Name text variable/character vector","code":""},{"path":"/reference/limpiar_spaces.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean redundant spaces ‚Äî limpiar_spaces","text":"text variable/character vector excess spaces removed","code":""},{"path":"/reference/limpiar_spaces.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean redundant spaces ‚Äî limpiar_spaces","text":"","code":"df <- data.frame(text_variable = \"clean   the   spaces please\") limpiar_spaces(df, text_var = text_variable) #>             text_variable #> 1 clean the spaces please  limpiar_examples %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaci√≥n\"                         #>  5 \"nos han metido en una muy dificil situaci√≥n\"                         #>  6 \"   Lo q no tenemos es tiempo.   Ma√±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan üò¢ üò¢ üò¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustar√≠a quedarme un ratito m√°s\"                              limpiar_examples %>% limpiar_spaces() %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                     #>    <chr>                                                               #>  1 mi amigo sancho es un wn de vdd jajaja                              #>  2 RT mi amigo sancho es un wn de vdd jajaja                           #>  3 @don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho #>  4 nos han metido en una muy dificil situaci√≥n                         #>  5 nos han metido en una muy dificil situaci√≥n                         #>  6 Lo q no tenemos es tiempo. Ma√±ana debemos luchar.                   #>  7 a mi es muy grave quitarle la vida al otro                          #>  8 ayyy nooo @robert_jordan üò¢ üò¢ üò¢                                   #>  9 todos se unen a nuestro grupo hagale un clic https::larebelion.es   #> 10 a mi me gustar√≠a quedarme un ratito m√°s"},{"path":"/reference/limpiar_spam_grams.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove posts containing spam-like n-grams ‚Äî limpiar_spam_grams","title":"Remove posts containing spam-like n-grams ‚Äî limpiar_spam_grams","text":"#' Function identifies posts contain suspicious-looking n-gram patterns. Posts can removed, pattern inspected, posts removed . can re-assign current data frame 'clean' data frame third element list.","code":""},{"path":"/reference/limpiar_spam_grams.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove posts containing spam-like n-grams ‚Äî limpiar_spam_grams","text":"","code":"limpiar_spam_grams(data, text_var, n_gram, min_freq)"},{"path":"/reference/limpiar_spam_grams.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove posts containing spam-like n-grams ‚Äî limpiar_spam_grams","text":"data Data frame tibble object text_var Name text variable n_gram Number words n-gram .e. n = 2 = bigram min_freq Minimum number times n-gram seen removed","code":""},{"path":"/reference/limpiar_spam_grams.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove posts containing spam-like n-grams ‚Äî limpiar_spam_grams","text":"list 3 data frames 1. suspicious-looking n-grams, 2. data removed, 3. rows data frame removed","code":""},{"path":"/reference/limpiar_stopwords.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean stop words for visualisations ‚Äî limpiar_stopwords","title":"Clean stop words for visualisations ‚Äî limpiar_stopwords","text":"two lists - sentiment & topics, similar, words lists. However, sentiment analysis sensitive negation, negation cues e.g. \"\", \"nada\" etc. removed sentiment list. purposes, topics go-lists, care always advised removing stop words.","code":""},{"path":"/reference/limpiar_stopwords.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean stop words for visualisations ‚Äî limpiar_stopwords","text":"","code":"limpiar_stopwords(df, text_var = mention_content, stop_words)"},{"path":"/reference/limpiar_stopwords.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean stop words for visualisations ‚Äî limpiar_stopwords","text":"df Name Data Frame Tibble object text_var name text variable stop_words \"sentiment\" \"topics\" - sentiment retains negation cues","code":""},{"path":"/reference/limpiar_stopwords.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean stop words for visualisations ‚Äî limpiar_stopwords","text":"text variable stop words specified list removed","code":""},{"path":"/reference/limpiar_stopwords.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Clean stop words for visualisations ‚Äî limpiar_stopwords","text":"stop word list editable via data(\"sentiment_stops\") data(\"topic_stops\").","code":""},{"path":"/reference/limpiar_stopwords.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean stop words for visualisations ‚Äî limpiar_stopwords","text":"","code":"limpiar_examples %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaci√≥n\"                         #>  5 \"nos han metido en una muy dificil situaci√≥n\"                         #>  6 \"   Lo q no tenemos es tiempo.   Ma√±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan üò¢ üò¢ üò¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustar√≠a quedarme un ratito m√°s\"                              limpiar_examples %>% limpiar_stopwords(stop_words = \"topics\") %>% dplyr::select(mention_content) %>% limpiar_spaces() #> # A tibble: 10 √ó 1 #>    mention_content                               #>    <chr>                                         #>  1 amigo sancho wn vdd jajaja                    #>  2 RT amigo sancho wn vdd jajaja                 #>  3 @don_quijote digas, amigo honorable # #sancho #>  4 metido dificil situaci√≥n                      #>  5 metido dificil situaci√≥n                      #>  6 Lo q. Ma√±ana debemos luchar.                  #>  7 grave quitarle vida                           #>  8 ayyy nooo @robert_jordan üò¢ üò¢ üò¢             #>  9 unen grupo hagale clic https::larebelion.     #> 10 gustar√≠a quedarme ratito m√°s"},{"path":"/reference/limpiar_tags.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean user handles and hashtags ‚Äî limpiar_tags","title":"Clean user handles and hashtags ‚Äî limpiar_tags","text":"Function replaces user handles hashtags neutral tags. can choose whether replace hashtags & users either one.","code":""},{"path":"/reference/limpiar_tags.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean user handles and hashtags ‚Äî limpiar_tags","text":"","code":"limpiar_tags(df, text_var = mention_content, user = TRUE, hashtag = TRUE)"},{"path":"/reference/limpiar_tags.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean user handles and hashtags ‚Äî limpiar_tags","text":"df Name Data Frame Tibble object text_var Name text variable/character vector user Whether replace user handles TRUE = replace hashtag Whether replace hashtags TRUE = replace","code":""},{"path":"/reference/limpiar_tags.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean user handles and hashtags ‚Äî limpiar_tags","text":"Data Frame Tibble object user handles /hashtags removed text variable","code":""},{"path":"/reference/limpiar_tags.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean user handles and hashtags ‚Äî limpiar_tags","text":"","code":"limpiar_examples #> # A tibble: 10 √ó 5 #>    doc_id author_name       mention_content    mention_url platform_interactions #>     <int> <chr>             <chr>              <chr>       <lgl>                 #>  1      1 don_quijote       \"mi amigo sancho ‚Ä¶ www.twitte‚Ä¶ NA                    #>  2      2 sancho_panza      \"RT mi amigo sanc‚Ä¶ www.twitte‚Ä¶ NA                    #>  3      3 edmond_dantes     \"@don_quijote no ‚Ä¶ www.twitte‚Ä¶ NA                    #>  4      4 el_sordo          \"nos han metido e‚Ä¶ www.fakebo‚Ä¶ NA                    #>  5      5 commander_miranda \"nos han metido e‚Ä¶ www.fakebo‚Ä¶ NA                    #>  6      6 robert_jordan     \"   Lo q no tenem‚Ä¶ www.youtub‚Ä¶ NA                    #>  7      7 anselmo           \"a mi es muy grav‚Ä¶ www.twitte‚Ä¶ NA                    #>  8      8 maria             \"ayyy nooo @rober‚Ä¶ www.twitte‚Ä¶ NA                    #>  9      9 pablo             \"todos se unen a ‚Ä¶ www.instag‚Ä¶ NA                    #> 10     10 pilar             \"a mi me gustar√≠a‚Ä¶ www.instag‚Ä¶ NA                     #Both user and hashtags limpiar_examples %>% limpiar_tags() %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                     #>    <chr>                                                               #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                            #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                         #>  3 \"@user no digas eso, tu amigo es muy honorable hashtag hashtag\"     #>  4 \"nos han metido en una muy dificil situaci√≥n\"                       #>  5 \"nos han metido en una muy dificil situaci√≥n\"                       #>  6 \"   Lo q no tenemos es tiempo.   Ma√±ana    debemos luchar.   \"      #>  7 \"a mi es muy grave quitarle la vida al otro\"                        #>  8 \"ayyy nooo @user üò¢ üò¢ üò¢ \"                                         #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\" #> 10 \"a mi me gustar√≠a quedarme un ratito m√°s\"                            #Just user tags limpiar_examples %>% limpiar_tags(hashtag = FALSE) %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                     #>    <chr>                                                               #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                            #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                         #>  3 \"@user no digas eso, tu amigo es muy honorable #vamos #sancho\"      #>  4 \"nos han metido en una muy dificil situaci√≥n\"                       #>  5 \"nos han metido en una muy dificil situaci√≥n\"                       #>  6 \"   Lo q no tenemos es tiempo.   Ma√±ana    debemos luchar.   \"      #>  7 \"a mi es muy grave quitarle la vida al otro\"                        #>  8 \"ayyy nooo @user üò¢ üò¢ üò¢ \"                                         #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\" #> 10 \"a mi me gustar√≠a quedarme un ratito m√°s\"                            #Just hashtags limpiar_examples %>% limpiar_tags(user = FALSE) %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                        #>    <chr>                                                                  #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                               #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                            #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable hashtag hashtag\" #>  4 \"nos han metido en una muy dificil situaci√≥n\"                          #>  5 \"nos han metido en una muy dificil situaci√≥n\"                          #>  6 \"   Lo q no tenemos es tiempo.   Ma√±ana    debemos luchar.   \"         #>  7 \"a mi es muy grave quitarle la vida al otro\"                           #>  8 \"ayyy nooo @robert_jordan üò¢ üò¢ üò¢ \"                                   #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"    #> 10 \"a mi me gustar√≠a quedarme un ratito m√°s\""},{"path":"/reference/limpiar_url.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean URLs from the text variable ‚Äî limpiar_url","title":"Clean URLs from the text variable ‚Äî limpiar_url","text":"Removes common forms URLs text variable","code":""},{"path":"/reference/limpiar_url.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean URLs from the text variable ‚Äî limpiar_url","text":"","code":"limpiar_url(df, text_var = mention_content)"},{"path":"/reference/limpiar_url.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean URLs from the text variable ‚Äî limpiar_url","text":"df Data Frame Tibble Object text_var Name text variable/character vector","code":""},{"path":"/reference/limpiar_url.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean URLs from the text variable ‚Äî limpiar_url","text":"Data Frame Tibble object URLs removed text variable","code":""},{"path":"/reference/limpiar_url.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean URLs from the text variable ‚Äî limpiar_url","text":"","code":"limpiar_examples %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaci√≥n\"                         #>  5 \"nos han metido en una muy dificil situaci√≥n\"                         #>  6 \"   Lo q no tenemos es tiempo.   Ma√±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan üò¢ üò¢ üò¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic https::larebelion.es\"   #> 10 \"a mi me gustar√≠a quedarme un ratito m√°s\"                              limpiar_examples %>% limpiar_url() %>% dplyr::select(mention_content) #> # A tibble: 10 √ó 1 #>    mention_content                                                       #>    <chr>                                                                 #>  1 \"mi amigo sancho es un wn de vdd jajaja\"                              #>  2 \"RT mi amigo sancho es un wn de vdd jajaja\"                           #>  3 \"@don_quijote no digas eso, tu amigo es muy honorable #vamos #sancho\" #>  4 \"nos han metido en una muy dificil situaci√≥n\"                         #>  5 \"nos han metido en una muy dificil situaci√≥n\"                         #>  6 \"   Lo q no tenemos es tiempo.   Ma√±ana    debemos luchar.   \"        #>  7 \"a mi es muy grave quitarle la vida al otro\"                          #>  8 \"ayyy nooo @robert_jordan üò¢ üò¢ üò¢ \"                                  #>  9 \"todos se unen a nuestro grupo hagale un clic \"                       #> 10 \"a mi me gustar√≠a quedarme un ratito m√°s\""},{"path":"/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator ‚Äî %>%","title":"Pipe operator ‚Äî %>%","text":"See magrittr::%>% details.","code":""},{"path":"/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator ‚Äî %>%","text":"","code":"lhs %>% rhs"},{"path":"/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator ‚Äî %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator ‚Äî %>%","text":"result calling rhs(lhs).","code":""},{"path":"/reference/sentiment_stops.html","id":null,"dir":"Reference","previous_headings":"","what":"sentiment_stops ‚Äî sentiment_stops","title":"sentiment_stops ‚Äî sentiment_stops","text":"Stop words sentiment analysis Spanish (negation cues left ).","code":""},{"path":"/reference/sentiment_stops.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"sentiment_stops ‚Äî sentiment_stops","text":"","code":"data(\"sentiment_stops\")"},{"path":"/reference/sentiment_stops.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"sentiment_stops ‚Äî sentiment_stops","text":"tibble 2 columns & 682 rows Message Data frame stop word vectors","code":""},{"path":"/reference/topic_stops.html","id":null,"dir":"Reference","previous_headings":"","what":"topic_stops ‚Äî topic_stops","title":"topic_stops ‚Äî topic_stops","text":"Stop words topic modelling Spanish.","code":""},{"path":"/reference/topic_stops.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"topic_stops ‚Äî topic_stops","text":"","code":"data(\"topic_stops\")"},{"path":"/reference/topic_stops.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"topic_stops ‚Äî topic_stops","text":"tibble 2 columns & 704 rows Message Data frame stop word vectors","code":""}]
