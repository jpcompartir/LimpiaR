---
title: "LimpiaR - Parts of Speech Workflow"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
      side: left
    theme: "journal"
vignette: >
  %\VignetteIndexEntry{processing_parts_of_speech}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style>
.callout, .callout-tip, .callout-warning {
  background-color: #f7f7f9;
  border-left: 5px solid #007bff;
  padding: 0.5rem;
  margin-top: 0.5rem;
  margin-bottom: 0.5rem;
}

.callout-warning {
  border-left: 5px solid #990000;
}

.callout-tip {
  border-left: 5px solid #66cdaa
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE,
  cache = TRUE
  )
```


```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(LimpiaR)
library(tibble)
library(dplyr)
library(stringr)
```

We want to take a set of documents like:
<blockquote> Who was the first person on the moon? </blockquote>
And extract for each token (where in this case token = word) in the document its part of speech and its lemma. So that for each document we have a table like:

```{r, echo = FALSE, message = FALSE}
model <- limpiar_pos_import_model(language = "english")
data <- tibble(text = c("Who was the first man on the moon?"), id = 1)
data <- limpiar_pos_annotate(
  data,
  text,
  id,
  model,
  update_progress = 0
) %>%
  select(token, lemma, pos_tag)

knitr::kable(data)
```


In this vignette we'll be looking at how to use LimpiaR via {udpipe} to achieve this. For a primer on parts of speech see [Parts of Speech Wikipedia](https://en.wikipedia.org/wiki/Part_of_speech).


<details>

<summary>For all UPOS Tags use the following table as a reference:</summary>

```{r, echo = FALSE}
pos_table <- tibble(
  `Part of Speech` = c("Adjective", "Adposition", "Adverb", "Auxiliary", "Coordinating Conjunction", "Determiner", "Interjection", "Noun", "Numeral", "Particle", "Pronoun", "Proper Noun", "Punctuation", "Subordinating Conjunction", "Symbol", "Verb", "Other"),
  `UPOS Tag` = c("ADJ", "ADP", "ADV", "AUX", "CCONJ", "DET", "INTJ", "NOUN", "NUM", "PART", "PRON", "PROPN", "PUNCT", "SCONJ", "SYM", "VERB", "X")
  ,
  Definition = c(
    'Usually describes or modifies a noun. It provides information about an object\'s size, color, shape, etc. Example: "big", "blue".', 
    'Class of words that includes prepositions and postpositions, used to express spatial or temporal relations or mark various semantic roles. Example: "in", "on", "before", "after".',
    'Modifies a verb, an adjective, another adverb, or even an entire sentence. It provides information about manner, time, place, frequency, degree, etc. Example: "quickly", "very".',
    'Special verb used to add functional or grammatical meaning to the clause in which it appears. It accompanies the main verb and forms different aspects, voices, or moods. Example: "is", "have", "will".',
    'Joins words, phrases, or clauses of similar grammatical status and syntactic importance. Example: "and", "but", "or".',
    'Introduces a noun and provides context in terms of definiteness, where it belongs in a sequence, quantity, or ownership. Example: "a", "the", "this", "those".',
    'An abrupt remark, made apart from the main sentence structure, often used to express strong emotion or surprise. Example: "Oh!", "Wow!", "Ugh!".',
    'Represents a person, place, thing, or idea. Nouns can be subjects, objects, or complement in a sentence. Example: "dog", "city", "happiness".',
    'A word, symbol, or group of words representing a number. Example: "one", "first", "100".',
    'A function word that does not fit into the other categories but is used to express grammatical relationships with other words or to specify the attitude of the speaker. Example: "up" in "stand up".',
    'Replaces a noun, often used to avoid repetition. Pronouns can do most things that nouns can do. Example: "he", "they", "who".',
    'Names a specific person, place, thing, or idea and is typically capitalized. Example: "Elizabeth", "London", "Microsoft".', 
    'Symbols that organize writing into clauses, phrases, and sentences, clarify meaning, and indicate pauses. Example: ".", ",", "!".',
    'Introduces a subordinate clause and indicates the relationship between the subordinate clause and the rest of the sentence. Example: "because", "although", "when".',
    'A mark or character used as a conventional representation of an object, function, or process. Example: "$", "%", "&".',
    'Expresses an action, occurrence, or state of being. Verbs are central to a clause. Example: "run", "be", "have".',
    'A category used for words or tokens that do not fit into the above categories. This is less common and often used in tagging to mark anomalies or unclassifiable items.'
  )
)
knitr::kable(pos_table, caption = "Source: universaldependencies.org/u/pos")
```

</details>

# Workflow

The main motivation for processing and extracting parts of speech is to home in on the particular aspects of language which are most informative for answering a given research question. For example, if you want to know what people think about something, you might focus on adjectives and nouns because adjectives describe things, and nouns are things. If you want to know how people are using something, you might focus more on verbs/phrasal verbs as they are 'doing words'.

## Loading packages

```{r, eval = FALSE, message=FALSE, warning=FALSE}
library(LimpiaR)
library(tibble)
library(dplyr)
library(stringr)
```

## Getting started

LimpiaR's PoS workflow leans on pre-existing functionality of the {udpipe} package to allow users to import a pre-trained model for parts of speech analysis whilst enabling advanced users to perform 'dependency parsing'.

<details>
<summary> What is dependency parsing?</summary>
"a technique which provides to each word in a sentence the link to another word in the sentence, which is called its syntactical head. This link between each 2 words furthermore has a certain type of relationship giving you further details about it"

see [here](https://www.r-bloggers.com/2019/07/dependency-parsing-with-udpipe/) for more info.

</details>

## Importing a udpipe model

First, we import the model we want to use. UDPipe pre-trained models are built upon Universal Dependencies treebanks and are made available for more than 65 languages based on 101 treebanks. For demonstrative purposes we'll use `language = "english"`.

```{r, eval = FALSE}
model <- limpiar_pos_import_model(language = "english")
```

## Example data

Now that we have our model imported into our session, let's get some data to tag, tokenise, lemmatise, and extract parts of speech. We need a data frame with both a text variable and an ID column which uniquely identifies each text.

```{r}
(
data <- dplyr::tibble(text = tolower(stringr::sentences[1:100]),
                      universal_message_id = paste0("TWITTER", 1:100))
)
```

So now we have some data and we also have our desired model loaded into session, the function we need next is `limpiar_pos_annotate()`. 

<div class="callout">

Before we begin, it's important to note that our output will be sensitive to cleaning steps. For example, it is possible that with the removal of punctuation, PoS Tagging may under-perform. This could also be said for the POS tagging process as nouns(NOUN) and proper-nouns(PROPN) will be harder to differentiate if punctuation is removed as well as all text being lowercase.

So be careful when pre-processing your text variable if you're intending to follow the PoS workflow.

</div>

## Extracting parts of speech

The`limpiar_pos_annotate` function converts the text of each document into its tokens, lemmas, POS tags, and dependency relationships*. The input data frame should have one document per row, the return data frame will have one row per token. This means the return data frame will have many more rows than the input data frame!

<details>
<summary> * </summary>
*the `dependency_parse` argument is set to `FALSE` by default as this step can be costly in time and compute if performed on large data sets. Most users will get by just fine without parsing dependencies, but if you are sure you need them set this argument to `TRUE`. 
</details>

`limpiar_pos_annotate` can be sped up by using `in_parallel = TRUE`, this argument makes the function process multiple rows of the data frame in parallel. 

By default the function will print its progress every 100 rows, this way you know that it's still working and roughly how long it should take to finish running. If you don't want the progress updates to print in your console set `update_progress = 0`. 

```{r, message=FALSE} 
# annotate texts and perform dependency parsing
annotations <- limpiar_pos_annotate(data = data, text_var = text, id_var = universal_message_id, pos_model = model, dependency_parse = TRUE, in_parallel = FALSE, update_progress = 25)
```

Now that we have our texts tokenized, dependencies parsed, and POS annotations are complete, let's take a look at the output. After annotating we have `r nrow(annotations)` rows, which is an `r nrow(annotations)/nrow(data)`x increase!

```{r, message=FALSE, warning=FALSE, width = 120}
annotations %>%
  select(-c(sentence, feats, xpos, doc_id)) %>%
  relocate(universal_message_id)
```

Output has some additional columns, we will pay closest attention to paragraph_id, sentence_id, token, lemma, pos_tag, dependency_tag.

What are these new columns? 

`paragraph_id` and `sentence_id` tell us which of the document's paragraphs, and which sentence the token is in. Because our data set contains only single sentences, all of these values are 1. 

`token` is the word the rest of the columns refer to. 

`lemma` is the [lemmatised](https://en.wikipedia.org/wiki/Lemmatization) version of the token, an example of lemmatisation is 'served' being converted to 'serve'.

`pos_tag` displays PoS labels such as; Noun(NOUN), Proper Noun(PROPN), Pronoun(PRON), Verb(VERB), Adjective(ADJ) etc. For more on the exact tags and what they mean see [PoS Tags](https://universaldependencies.org/u/pos/index.html) or the UPOS tags table at the top of this document. 

`dependency_tag` displays the dependency relation for each token. Dependency relationships are significantly more complicated than parts of speech. see [Dependency Relations](https://universaldependencies.org/u/dep/) for more details.

`head_token_id`, `dependency_tag` and `feats` will only be present if we select `dependency_parse = TRUE`,

Finally, the column you inserted as `id_var = `. This variable will help you to join the results back to your original data frame.

## Manipulating the output

Now that we have adequate context, let's look at what we can do with our output.

Now that we have a bit more context as to what all of this information means, lets dive in and see if we can manipulate this data so we can see what adjectives are most common in their lemmatised form. To do so, we only require some basic {dplyr} functionality and we will use `head()` from {utils} package to select only the top 10 most frequent.

```{r}
annotations %>% 
  dplyr::filter(pos_tag %in% c("ADJ")) %>%  # select which parts of speech we want to inspect
  dplyr::count(.by = lemma, lemma, sort = TRUE) %>% 
  utils::head(n = 10)
```

We can see that 'old' is the most frequent adjective followed by the likes of 'high' and 'fine' along with a few others within our text documents. Now lets look at frequent nouns.

```{r}
annotations %>% 
  dplyr::filter(pos_tag %in% c("NOUN")) %>% # this time selecting nouns
  dplyr::count(.by = lemma, lemma, sort = TRUE) %>% 
  utils::head(n = 10)
```

Looking at frequent nouns shows us that 'man' is the most used noun within the data set. This paired with the adjective counts, can start to tell a story about what's in the data and how these parts of speech are distributed across our documents. 

So far, we've observed 'old' as the most frequent adjective and 'man' as the most used noun, this isn't to say that a majority of posts are consistent of the term 'old man', but if we wanted to investigate adjective and noun relationships we can so do by calculating these co-occurrences using `udpipe::cooccurrence()`.

```{r, message=FALSE, warning=FALSE}
co_occurrences <- udpipe::cooccurrence(x = subset(annotations, pos_tag %in% c("NOUN", "ADJ")),
                                       term = "lemma",
                                       group = c("paragraph_id", "sentence_id", "universal_message_id"))
utils::head(co_occurrences)
```

Once we have the information regarding the co-occurrences of nouns and adjectives within our documents, we are able to evidence what entities appear throughout our data, as well as the type of language used in conjunction with and toward them. We can later add to this, by visualizing the frequency of terms appearing within the same document as well as those that are explicitly used one after another, just like the example above, where we see 'hot sun' appear as most frequent.


