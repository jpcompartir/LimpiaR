---
title: "LimpiaR X Parts of Speech Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{processing_parts_of_speech}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
In this vignette we'll be looking at how to use LimpiaR to process a text variable for its parts of speech via {udpipe}. For a primer on parts of speech see [Parts of Speech Wikipedia](https://en.wikipedia.org/wiki/Part_of_speech)

The main motivation for processing and extraction parts of speech is to home in on the particular aspects of language which are most informative for answering a given research question. For example, if you want to know what people think about something, you might focus on adjectives and nouns. If you want to know how people are using something, you might focus more on verbs/phrasal verbs.

Before we start, we must ensure that we have the necessary libraries installed and loaded, including LimpiaR, and a few others for data wrangling purposes.
```{r}
library(LimpiaR)
library(tibble)
library(dplyr)
library(stringr)
```
LimpiaR's parts of speech(POS) workflow leans on existing functionality of {udpipe} and allow for users to import one of their existing trained models for parts of speech analysis, to later annotate texts by tokenizing(breaking sentences up into words) and also enables users to perform dependency parsing on tokens, 'a technique which provides to each word in a sentence the link to another word in the sentence, which is called its syntactical head. This link between each 2 words furthermore has a certain type of relationship giving you further details about it', see [here](https://www.r-bloggers.com/2019/07/dependency-parsing-with-udpipe/) for more.

First, we must select what model it is that that we want to download. UDPipe pre-trained models are built upon Universal Dependencies treebanks and are made available for more than 65 languages based on 101 treebanks. For the purpose of this documentation, we will select the 'english' model, and we can do so using the `limpiar_pos_import_model` function and supplying its only argument, `language` with the desired model we want.

```{r}
model <- limpiar_pos_import_model(language = "english")
```
 
For the purpose of this demo, lets use the data object derived from the {stringr} package containing sentences ideal for tagging, tokenizing, lemmatizing and parsing for parts of speech analysis.
```{r}
data <- dplyr::tibble(text = tolower(stringr::sentences[1:100]), document = 1:100)
```

Now we have our data and we should have our loaded model, we can begin annotating with the `limpiar_pos_annotate()` function. 

It's worth noting that the output is dependent on cleaning steps performed on texts before annotating. For example, it is possible that with the removal of punctuation the dependency parsing bits of the function may under-perform. This could also be said for the POS tagging process as nouns(PRON) and proper-nouns(PROPN) will be harder to differentiate.

When using LimpiaR's annotate function we tokenize all sentences of our text variable(text_var) within a selected data frame or tibble(data), enabling us to perform Parts of Speech tagging, lemmatization and dependency parsing(parse_text). We can also speed the process up by calling TRUE on the parallel processing argument(in_parallel).

```{r}
annotations <- limpiar_pos_annotate(data = data, text_var = text, pos_model = model, in_parallel = FALSE, parse_text = TRUE, id_var = "document")
```

We can see that there's a doc_id, paragraph_id, sentence_id, the sentence, the token_id,token, lemma, upos, xpos, feats (seems like features?) 
- explain these features
- how to summarise/filter by particular features & use this to inform what other functions should be built 
- then look at feeding the output into some ParseR/topic modelling visualisations to get a feel for the difference + complete the vignette
