---
title: "processing_parts_of_speech"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{processing_parts_of_speech}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(LimpiaR)
```

In this vignette we'll be looking at how to use LimpiaR to process a text variable for its parts of speech via {udpipe}. For a primer on parts of speech see [Parts of Speech Wikipedia](https://en.wikipedia.org/wiki/Part_of_speech)

The main motivation for processing and extraction parts of speech is to home in on the particular aspects of language which are most informative for answering a given research question. For example, if you want to know what people think about something, you might focus on adjectives and nouns. If you want to know how people are using something, you might focus more on verbs/phrasal verbs.

# Extracting PoS

We're going to need a small data frame with some text information, so we'll use the first 100 sentences of {stringr}'s `sentences`

```{r}
library(udpipe)
library(tibble)
library(dplyr)
library(stringr)

data <- tibble(text = tolower(sentences[1:100]), document = 1:100)
```

If you've never used {udpipe} before, you'll need to download a model. In this case we'll just use the default 'english-ewt' for a more comprehensive list visit [udpipe models](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html#pre-trained-models)


When you download the model, make sure to copy and paste the path as you'll need it later. For this vignette we'll use a temporary directory.
```{r, eval = FALSE}
# run this if you don't have the model downloaded already
udpipe_download_model("english-ewt")
?udpipe_load_model()

temp_model <- udpipe_download_model("english-ewt", model_dir = tempdir())
model_path <- temp_model$file_model
english <- udpipe_load_model(model_path)
```

Now that we've got a model we can use it to annotate our text, which will tokenise, tag parts of speech, lemmatise and parse dependencies, unless we ask it not to. To begin with we'll just ask for the parts of speech and lemma.

```{r}
test <- udpipe_annotate(
  english,
  data$text,
  parser = "none",
  trace = 20
)
```
The output is pretty nasty, but we can render it into a data frame/tibble pretty easily:
```{r}
parsed_data <-as.data.frame(test) %>%
  as_tibble()

parsed_data
```

We can see that there's a doc_id, paragraph_id, sentence_id, the sentence, the token_id,token, lemma, upos, xpos, feats (seems like features?) 
- explain these features
- how to summarise/filter by particular features & use this to inform what other functions should be built 
- then look at feeding the output into some ParseR/topic modelling visualisations to get a feel for the difference + complete the vignette
