---
title: "Stopwords"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Stopwords}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r}
library(tidyverse)
library(LimpiaR)
library(tidytext)
library(tm)
```

## Stop Words

Our stop words have been stored in LimpiaR within data frames. We have two separate data frames, one for sentiment stopwords and another for topic stopwords.

The workflow is somewhat complicated, but if you're used to reading data into R, using common Tidyverse/dplyr commands like filter & mutate, you should have no problem in following the workflow; and if you're not, this will show you how.

Later on we will look at editing stopword lists, creating custom lists & lists for different languages.

First we'll look at how our sentiment stop words are stored:

```{r}
data("sentiment_stops")
sentiment_stops

data("topic_stops")
topic_stops
```

We have two similar, but importantly distinct sets of stop words. The first set is for sentiment, in this set we leave negation cues in our text variable (or valence shifters) - words like no, not, never as they alter the sentiment of a post/sentence. Think of 'this is not great' vs 'this is great'. If we had 'not' as a stop word and removed it, we would lose important information for classifying sentiment.

The second list is for topic modelling, in this list we remove the negation cues, as for topic modelling they are generally uninformative. When counting tokens, producing bigram networks, pairwise correlations, or LDA topic models, it is advisable to use the topic modelling stop word list, as they remove all the sentiment stop words + the additional negation cues.

An example workflow would be: remove sentiment stop words > run sentiment algorithm > remove topic stop words > create bigram networks, topic models etc. > remove spam > recreate bigram networks & topic models. 

Ok, so how do we actually do this with R, LimpiaR and the tm (textmining) package?

```{r}
original <- read_csv("~/Google Drive/My Drive/Data Science Project Work/SAMY/eroski/data/merge_from_ofoct (1).csv") %>%
  janitor::clean_names()


sentiment_stopwords <- sentiment_stops$sentiment

#load the tm package or call tm directly with tm::
library(tm)

original %>%
  mutate(mention_content = limpiar_accents(mention_content))%>%
  mutate(mention_content = tm::removeWords(mention_content, sentiment_stopwords))%>%
  limpiar_spaces(mention_content) #Stripping whitespaces after calling the tm::remove family of functions is a good habit to cultivate
```

Let's compare this output with our topic stop words:

```{r}
topic_stopwords <- topic_stops$topics

original %>%
  mutate(mention_content = limpiar_accents(mention_content))%>%
  mutate(mention_content = tm::removeWords(mention_content, topic_stopwords))%>%
  limpiar_spaces(mention_content)

```

## Custom Stop Words

Though fairly extensive, our topic and sentiment stopwords are in Spanish and are not domain specific. In most projects we will discover some domain stopwords that we wish to clean from the text variable. We will look at how to add/remove words to the existing stopwords lists, how to create our own list, and finally which stopwords to use for English analyses.



```{r}
data <- original

data <- data %>% 
  janitor::clean_names() 

#Taking a sample of the data for size/speed
set.seed(1234)
data <- data %>% sample_n(5000)

data <- limpiar_df(data, mention_content)
data <- limpiar_na_cols(data, 0.1)

```

We've got an example data frame about Eroski - a Spanish supermarket. Let's take a look at our  text variable by counting the most common words.

```{r}
data %>%
  tidytext::unnest_tokens(words, mention_content) %>%
  count(words, sort = TRUE)
```

We can see the most common words are 'de', 'la', 'y' etc. in Spanish these are all stopwords. Let's remove stopwords and try again:

```{r}
#we used data("sentiment_stops") earlier to access our stopwords. Now we store them in a variable.
sentiment_stopwords <- sentiment_stops$sentiment_no_accents

data %>%
  mutate(mention_content = tm::removeWords(mention_content, sentiment_stopwords))%>%
  limpiar_spaces() %>% 
  unnest_tokens(words, mention_content)%>%
  count(words, sort = TRUE)

```

We can see that more of our top 10 are words that should reveal something about our topic, see 'ecologica', 'productos', 'sostenibilidad' etc. Let's compare this to when we remove our topic_stopwords, we would expect to see a similar output, but with 'no' removed:

```{r}
topic_stopwords <- topic_stops$topics_no_accents

data %>%
  mutate(mention_content = tm::removeWords(mention_content, topic_stopwords))%>%
  limpiar_spaces()%>%
  tidytext::unnest_tokens(words, mention_content)%>%
  count(words, sort = TRUE)

```

We can see sostenibilidad come to the top of our word counter. Let's create our own stopword list and remove 2021, espana, mayo & anos, as they seem to carry little information if any.

```{r}

eroski_stopwords <- c("espana", "2021", "mayo", "anos")

#We remove our topic stopwords and our eroski stopwords in the same pipe:
data %>%
  mutate(mention_content = tm::removeWords(mention_content, topic_stopwords))%>%
  mutate(mention_content = tm::removeWords(mention_content, eroski_stopwords))%>%
  limpiar_spaces()%>%
  tidytext::unnest_tokens(words, mention_content)%>%
  count(words, sort = TRUE)

```

We can see that the words we included to our eroski stopwords have been removed. The process isn't finished though, we see that 'ano' has now crept into our top 10. We could add ano to our eroski_stopwords like so:

```{r}
eroski_stopwords <- c(eroski_stopwords, "ano")
```

We could then run the same code as before to count our new top 10, and remove more stopwords. 

