---
title: "LimpiaR_user_guide"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{LimpiaR_user_guide}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

First we will load LimpiaR and the tidyverse package (which we use extensively)
```{r setup, echo = FALSE}
library(LimpiaR)
library(tidyverse)
```

LimpiaR is a small package built to speed up the data pre-processing and cleaning workflow. Let's create a very basic example data set to demonstrate a workflow with a mixture of LimpiaR functions and commonly-used pre-processing packages that covered the basic functionalities we were looking for.

```{r}

df <- data.frame(`Mention Content` = cbind(c("Hello, what's your name?",
                                           "RT LINK BIO BUY NOW",
                                           "a mi no me gustaría",
                                           "a mi no me gustaría",
                                           "a mi no  mé   gustaría",
                                           "hello, what's your name?",
                                           "I can't believe this article to be honest https://guardian.com/spampost", "yo no sé que ha pasado, pero no me gusta por nada")),
                 `Mention Url` = cbind(c("www.twitter.com/user/post",
                                       "www.facebook.com/user/post",
                                       "www.youtube.com/user/post",
                                       "www.forocoches.com/forum",
                                       "www.bbc.news.co.uk",
                                       "www.google.co.uk/images/post_url",
                                       "www.guardian.com/spam_post",
                                       "www.twitter.es/spanish_user")),
                 `Super Bad NA Column` = cbind(c(NA, NA, NA, NA, NA, 2, 1, NA)))



```

The first step is to clean up the column names, generally we want our columns to be all lowercase, with spaces marked by '_' (snake case) and no other punctuation. We can dip into the janitor package for a convenient and widely-used function to do this:

```{r}
df <- df %>% 
  janitor::clean_names()

original <- df #We'll copy our data frame for later use

names(df)
```

Cleaning the names like this every time you start a new project will save you a lot of time in the long run, as you won't get confused by capitals, and tab completion is much smoother when column names are in a uniform format.

In the future you may want to perform some actions on the text variable before doing any pre-processing, but for now we are going to go ahead and put our text variable to lowercase, which is something we want to do for virtually every data set that we analyse. We do this so that strings like "HelLo" aren't seen as different to "hello" or "Hello", which helps us when counting words and bigrams, or modelling topics!

```{r, echo = FALSE}
#We use the base R function tolower() on our text variable
df <- df %>%
  mutate(mention_content = tolower(mention_content))

```

Let's take a look at the first LimpiaR function. This function will replace accented characters with non-accented, e.g. é becomes e and ñ becomes n. Converting ñ to n has some slightly unwanted effects in that words like año(s) become ano(s). Later on this may become an issue, but for now the trade off is ok. We call limpiar_accents() on the text variable with the help of mutate()

```{r}
df <- df %>% 
  mutate(mention_content = limpiar_accents(mention_content))
df
```

Next, we'll remove duplicates and "Deleted or protected mentions" from the text variable.

```{r}
df <- df %>%
  limpiar_duplicates(mention_content)
df

```
At first glance, it looks like our function may not be functioning properly, as two seemingly-identical strings remain. However, the tibble output is masking some spaces in one of our 'a mi no me gustaria' strings. How could we see that?

```{r}
df %>%
  pull(mention_content)
```

So, we could use another LimpiaR function to remove the excess whitespaces and try again:

```{r}
df <- df %>%
  limpiar_spaces(mention_content)%>%
  limpiar_duplicates(mention_content)
df
```

Now our function seems to be working as expected. We can see that one of the posts begins with 'rt' which indicates that it is a retweet, oftentimes we will want to remove all posts that are retweets, so let's call anohter LimpiaR function: limpiar_retweets, on our data frame.

```{r}
df <- df %>%
  limpiar_retweets(mention_content)
df
```

Our data frame is getting pretty small, but in our larger data frames we often see the same URLs posted by many accounts or bots. We don't want our bigram networks, token charts or topic models to be filled with URLs. We will use another function: limpiar_url to remove URLs from our text variable:

```{r}
df <- df %>%
  limpiar_url(mention_content)
df
```

Looking at our data set on the qualitative level, we might want to see which article the post is referencing, or closely inspect all of the posts which feature a certain pattern, to then copy the URL and take a screenshot, or examine for traces of spam. The limpiar_inspect function gives us an inspectable data frame with just the text variable and the post url column for ease of inspection:

```{r}
limpiar_inspect(df, "article", mention_content, mention_url)
```

You may have noticed that our data frame has a column which is filled with NA values, when dealing with small data sets, this is no issue. However, in large data sets conserving memory can be helpful. As humans, reducing the dimensions of the data frame can also help us to see more things at once and put less strain on our working memories. 

The next LimpiaR function allows you to reduce the columns of your data frame, by selecting a threshold of non-NA values a column must surpass to be retained. If the threshold is 0.1, a column must have 10% or more values which are not NA and so forth. 

```{r}

df <- limpiar_na_cols(df, 0.3)
df
```

We see that the NA-heavy column is deleted. What happens if we change the threshold to 0.2? Asking R to get rid of all columns that have 80% or more  Would we expect to have 3 columns or 2?

```{r}
limpiar_na_cols(original, 0.2)
```

We've now seen the majority of the LimpiaR first release functions. Often they will need to be used in conjunction, but sometimes we will want to use them together in one big function call. The final function we will look at, limpiar_df, effectively does this, as well as allowing for the removal of punctuation. However, removing punctuation is not a given and should be removed only if there is a valid reason for doing so.

```{r}
limpiar_df(original, mention_content, remove_retweets = FALSE, remove_punctuation = FALSE)
```

We could also decide to remove retweets and punctuation:

```{r}
limpiar_df(original, mention_content, remove_retweets = TRUE, remove_punctuation = TRUE)
```

Using limpiar_df won't always be the correct answer, but once you understand what each function in LimpiaR does, you will be able to make an educated guess and use limpiar_df to drastically speed up your analyses.

Finally, we're going to look at some stopwords, which have been stored in LimpiaR within data frames. We have two separate data frames, one for sentiment stopwords and another for topic stopwords; we'll explain why as we go. 

The workflow is somewhat complicated, but if you're used to reading data into R, using common Tidyverse/dplyr commands like filter & mutate, you should have no problem in following the workflow. First we'll look at how our sentiment stop words are stored:

```{r}
data("sentiment_stops")
sentiment_stops
```
We can see that our data frame has two columns, of 682 observations. Each observation is a different stop words. The 'sentiment' column has accents left in, for occasions that an analyst wishes to remove sentiment stop words without having removed accents. We will look at how to remove sentiment stop words both with and without accents, however, generally wou will want to remove accents, so the sentiment_no_accents column will be what you need.


- show why it's best to tolower, remove accents etc. at time

```{r}
sentiment_stopwords <- sentiment_stops$sentiment

#load the tm package or call tm directly with tm::
library(tm)

original %>%
  mutate(mention_content = tm::removeWords(mention_content, sentiment_stopwords))%>%
  limpiar_spaces(mention_content) #Stripping whitespaces after calling the tm::remove family of functions is a good habit to cultivate
```


 
